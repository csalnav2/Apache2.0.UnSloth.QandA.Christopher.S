{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csalnav2/QdotCS/blob/master/Unsloth_Solutions_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWoTNsza4kRP"
      },
      "source": [
        "# Unsloth (Demo Solutions Notebook)\n",
        "\n",
        "This notebook collects various code snippets that address specific tasks:\n",
        "\n",
        "1. **nF4 → Triton** (Quantized 4-bit kernel demo)\n",
        "2. **QLoRA + `torch.compile`** (Naive QLoRA example, no graph breaks)\n",
        "3. **QLoRA + FSDP** (Fully Sharded Data Parallel + LoRA injection)\n",
        "4. **Memory-Efficient Backprop** (Chunked final linear + cross-entropy)\n",
        "5. **Windows Support** (Python scripts to build/install `unsloth`, plus test code)\n",
        "6. **Flexible Attention** (\"Unsloth\" style chunked attention examples)\n",
        "7. **Sequence Classification Patch** (Inject LoRA into `AutoModelForSequenceClassification`)\n",
        "8. **Refactored Attention** (xformers, SDPA, flash-attn, fallback in one interface)\n",
        "\n",
        "Feel free to skip cells or modify as needed."
      ],
      "id": "cWoTNsza4kRP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHdln20U4kRS"
      },
      "source": [
        "---\n",
        "## 1) **nF4 → Triton**\n",
        "\n",
        "**Goal**: Demonstrate converting 4-bit weights (nF4 style) and using a Triton kernel to do matrix multiplication without fully decompressing everything into float16/float32 first.\n",
        "\n",
        "**Note**: This code is a **minimal skeleton**. Real nF4 implementations might have more complex scaling logic, per-row or per-channel quant parameters, etc."
      ],
      "id": "cHdln20U4kRS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "XdO1UCKK4kRT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "64f59f10-73e4-40bf-e0a9-3425c4620db4"
      },
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "# ------------------------------\n",
        "# 1) Fake nF4 encode + decode\n",
        "# ------------------------------\n",
        "def nf4_encode(weights_fp16: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Naive example: we assume the values are roughly in [-8, +7.75].\n",
        "    1) clamp\n",
        "    2) shift => [0, ~15.75]\n",
        "    3) scale => [0..15]\n",
        "    4) round => integer\n",
        "    5) pack two 4-bit values per byte.\n",
        "    \"\"\"\n",
        "    # clamp + shift\n",
        "    clamped = torch.clamp(weights_fp16, -8.0, 7.75)\n",
        "    shifted = clamped + 8.0\n",
        "\n",
        "    # scale to [0..15], then round\n",
        "    scaled = shifted * (15.0 / 15.75)\n",
        "    quant_4bit = torch.round(scaled).to(torch.uint8)\n",
        "\n",
        "    # pack two 4-bit into one byte: [value0 (lower 4bits), value1 (upper 4bits)]\n",
        "    # shape stays the same in terms of # of elements, but physically we combine pairs.\n",
        "\n",
        "    # We'll flatten first for simplicity\n",
        "    flat = quant_4bit.view(-1)\n",
        "    if len(flat) % 2 != 0:\n",
        "        # pad if odd\n",
        "        flat = torch.cat([flat, flat.new_zeros(1)])\n",
        "\n",
        "    # even indices -> lower 4 bits, odd indices -> upper 4 bits\n",
        "    low_4 = flat[0::2] & 0xF\n",
        "    high_4 = flat[1::2] & 0xF\n",
        "    packed = (high_4 << 4) | low_4\n",
        "    return packed\n",
        "\n",
        "def nf4_unpad_and_reshape(packed: torch.Tensor, shape):\n",
        "    \"\"\"\n",
        "    Helper to handle if we padded an odd element.\n",
        "    \"\"\"\n",
        "    # total n elements for shape must be shape.numel()\n",
        "    # each output element is 4 bits, so we need shape.numel() // 2 bytes if it's even.\n",
        "    n_el = shape.numel()\n",
        "    n_bytes_needed = (n_el + 1)//2  # if odd, we used +1 in the code\n",
        "    # so we slice the 'packed' and ignore extra.\n",
        "    packed = packed[:n_bytes_needed]\n",
        "    return packed\n",
        "\n",
        "def nf4_decode(packed: torch.Tensor, out_shape) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Unpacks the 4-bit values from a [N/2] byte buffer,\n",
        "    re-scales them back to float16 in [-8..7.75], shape=out_shape.\n",
        "    \"\"\"\n",
        "    # each byte has 2 nibbles => 2 values\n",
        "    # lower nibble: x & 0xF\n",
        "    # upper nibble: (x >> 4) & 0xF\n",
        "    flat = packed.view(-1)\n",
        "    n_el = out_shape.numel()\n",
        "\n",
        "    vals_0 = (flat & 0xF).to(torch.float16)\n",
        "    vals_1 = ((flat >> 4) & 0xF).to(torch.float16)\n",
        "\n",
        "    # interleave\n",
        "    decoded = torch.zeros(n_el, dtype=torch.float16, device=flat.device)\n",
        "    decoded[0::2] = vals_0\n",
        "    decoded[1::2] = vals_1\n",
        "\n",
        "    # scale back => float16 in [-8..7.75]\n",
        "    # reverse step: val in [0..15], => shift to [0..15.75], => shift down by 8.\n",
        "    # recall we used: shifted * (15 / 15.75)\n",
        "    # so decode => val*(15.75/15) - 8\n",
        "    rescaled = decoded * (15.75 / 15.0) - 8.0\n",
        "\n",
        "    return rescaled.view(out_shape)\n",
        "\n",
        "# ------------------------------\n",
        "# 2) Minimal Triton Kernel for nF4 MatMul\n",
        "# ------------------------------\n",
        "@triton.jit\n",
        "def matmul_nf4_kernel(\n",
        "    A_ptr,  # int32 ptr\n",
        "    B_ptr,  # int32 ptr\n",
        "    C_ptr,  # float16 ptr\n",
        "    M, N, K,\n",
        "    stride_am, stride_an,\n",
        "    stride_bm, stride_bn,\n",
        "    stride_cm, stride_cn,\n",
        "    BLOCK_M: tl.constexpr,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    BLOCK_K: tl.constexpr\n",
        "):\n",
        "    \"\"\"\n",
        "    A is [M x K] in nF4-packed format, B is [K x N] in nF4-packed,\n",
        "    C is [M x N] in float16.\n",
        "    This kernel decodes partial blocks, accumulates them.\n",
        "\n",
        "    For a more robust solution, you'd handle partial tiles,\n",
        "    dynamic shapes, etc. This is a minimal example.\n",
        "    \"\"\"\n",
        "    # row idx and col idx in the output tile\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "    off_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    off_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "\n",
        "    # We’ll accumulate in float32 for partial sums\n",
        "    accum = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "\n",
        "    # Loop over K in chunks of BLOCK_K\n",
        "    # For each chunk, we decode a tile from A and B, then multiply.\n",
        "    # NOTE: This is simplistic. Real kernels do more advanced blocking.\n",
        "\n",
        "    for k_block_start in range(0, K, BLOCK_K):\n",
        "        # load A block\n",
        "        # we have indices [off_m, k_block_start : k_block_start+BLOCK_K]\n",
        "        # decode nF4 on the fly\n",
        "\n",
        "        # Pseudocode: We can do a direct load from A_ptr.\n",
        "        # But in real code, we must decode from nF4. We'll do it in Python first for simplicity.\n",
        "        # This is where a custom decoding kernel might be used. We'll just do naive loads.\n",
        "        # We'll show the concept, not a fully correct kernel.\n",
        "\n",
        "        # In a real kernel, you'd load the 4-bit data from A_ptr + offsets, decode into float16,\n",
        "        # then cast to float32.\n",
        "        # We’ll do something like:\n",
        "        a_block = tl.zeros((BLOCK_M, BLOCK_K), dtype=tl.float32)\n",
        "        b_block = tl.zeros((BLOCK_K, BLOCK_N), dtype=tl.float32)\n",
        "\n",
        "        # multiply\n",
        "        accum += tl.dot(a_block, b_block)\n",
        "\n",
        "    # write accum to C\n",
        "    # offset in C => c_ptr + row*stride_cm + col*stride_cn\n",
        "    # We'll store only if in-bounds.\n",
        "    # This is incomplete but a minimal shape.\n",
        "    out_f16 = accum.to(tl.float16)\n",
        "    # store the tile\n",
        "    for i in range(BLOCK_M):\n",
        "        for j in range(BLOCK_N):\n",
        "            c_row = off_m[i]\n",
        "            c_col = off_n[j]\n",
        "            if (c_row < M) and (c_col < N):\n",
        "                offset_c = c_row * stride_cm + c_col * stride_cn\n",
        "                tl.store(C_ptr + offset_c, out_f16[i, j])\n",
        "\n",
        "# ------------------------------\n",
        "# 3) Python driver to do nF4 matmul\n",
        "# ------------------------------\n",
        "def matmul_nf4(A_fp16, B_fp16):\n",
        "    \"\"\"\n",
        "    A_fp16: [M, K]\n",
        "    B_fp16: [K, N]\n",
        "    Return: [M, N] float16\n",
        "\n",
        "    *Just a naive demonstration.* We'll do an actual decode on CPU, normal mm.\n",
        "     Then we run a fake Triton kernel that doesn't do the real decode.\n",
        "    \"\"\"\n",
        "    device = A_fp16.device\n",
        "    M, K = A_fp16.shape\n",
        "    K2, N = B_fp16.shape\n",
        "    assert K == K2\n",
        "\n",
        "    # 1) encode A, B to nF4\n",
        "    A_packed = nf4_encode(A_fp16)\n",
        "    B_packed = nf4_encode(B_fp16)\n",
        "\n",
        "    # create output buffer\n",
        "    C = torch.empty((M, N), dtype=torch.float16, device=device)\n",
        "\n",
        "    # for the sake of demonstration, we'll do a single-grid launch (no tiling)\n",
        "    # real code: you tile across M and N.\n",
        "    block_m = 128\n",
        "    block_n = 128\n",
        "    block_k = 32\n",
        "\n",
        "    grid = ( (M + block_m - 1)//block_m, (N + block_n - 1)//block_n )\n",
        "\n",
        "    matmul_nf4_kernel[grid](\n",
        "        A_packed,\n",
        "        B_packed,\n",
        "        C,\n",
        "        M, N, K,\n",
        "        N, 1,  # strides for A ? (placeholder)\n",
        "        N, 1,  # strides for B ? (placeholder)\n",
        "        N, 1,  # strides for C\n",
        "        BLOCK_M=block_m,\n",
        "        BLOCK_N=block_n,\n",
        "        BLOCK_K=block_k\n",
        "    )\n",
        "\n",
        "    return C\n",
        "\n",
        "# 4) Demo usage\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    A_fp16 = torch.randn((64, 64), dtype=torch.float16, device=device)\n",
        "    B_fp16 = torch.randn((64, 32), dtype=torch.float16, device=device)\n",
        "    C_out = matmul_nf4(A_fp16, B_fp16)\n",
        "    print(\"C_out shape:\", C_out.shape)\n",
        "    # In reality, the kernel is incomplete, so results are garbage.\n",
        "    # This is just a skeleton.\n",
        "    print(\"Example done (skeleton).\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "CompilationError",
          "evalue": "at 61:20:\n        # multiply\n        accum += tl.dot(a_block, b_block)\n\n    # write accum to C\n    # offset in C => c_ptr + row*stride_cm + col*stride_cn\n    # We'll store only if in-bounds.\n    # This is incomplete but a minimal shape.\n    out_f16 = accum.to(tl.float16)\n    # store the tile\n    for i in range(BLOCK_M):\n        for j in range(BLOCK_N):\n            c_row = off_m[i]\n                    ^\nValueError('Did you forget to add @triton.jit ? (`_builder` argument must be provided outside of JIT functions.)')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCompilationError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6c4a6629ab2e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mA_fp16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mB_fp16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mC_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatmul_nf4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_fp16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB_fp16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C_out shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;31m# In reality, the kernel is incomplete, so results are garbage.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-6c4a6629ab2e>\u001b[0m in \u001b[0;36mmatmul_nf4\u001b[0;34m(A_fp16, B_fp16)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mblock_m\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mblock_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mblock_n\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mblock_n\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     matmul_nf4_kernel[grid](\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mA_packed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mB_packed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/runtime/jit.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mmemorizes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \"\"\"\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;31m# return cast(T, functools.partial(cast(Callable, self.run), grid=grid))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/runtime/jit.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0;31m# compile the kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mASTSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             kernel = self.compile(\n\u001b[0m\u001b[1;32m    663\u001b[0m                 \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/compiler/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(src, target, options)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mcodegen_fns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_codegen_implementation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodegen_fns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mfilter_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/compiler/compiler.py\u001b[0m in \u001b[0;36mmake_ir\u001b[0;34m(self, options, codegen_fns, context)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodegen_fns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mast_to_ttir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodegen_fns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcodegen_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCompilationError\u001b[0m: at 61:20:\n        # multiply\n        accum += tl.dot(a_block, b_block)\n\n    # write accum to C\n    # offset in C => c_ptr + row*stride_cm + col*stride_cn\n    # We'll store only if in-bounds.\n    # This is incomplete but a minimal shape.\n    out_f16 = accum.to(tl.float16)\n    # store the tile\n    for i in range(BLOCK_M):\n        for j in range(BLOCK_N):\n            c_row = off_m[i]\n                    ^\nValueError('Did you forget to add @triton.jit ? (`_builder` argument must be provided outside of JIT functions.)')"
          ]
        }
      ],
      "id": "XdO1UCKK4kRT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8RjGWg54kRU"
      },
      "source": [
        "---\n",
        "## 2) **QLoRA + `torch.compile`** (Naive Example)\n",
        "\n",
        "This snippet demonstrates a simple QLoRA-like module (4-bit quant + LoRA adapters), then wraps the model in `torch.compile` to ensure we avoid graph breaks."
      ],
      "id": "c8RjGWg54kRU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUdxpsku4kRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a6b7a6-5bec-45d5-8306-833509fc7be2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "############################################\n",
        "# 1) Define a naive QLoRA quant module\n",
        "############################################\n",
        "class QLoRAQuant(nn.Module):\n",
        "    \"\"\"\n",
        "    A toy QLoRA-like module that:\n",
        "      - Maintains a base weight.\n",
        "      - Has low-rank \"LoRA\" factors A & B (rank adaptation).\n",
        "      - Performs a simple 4-bit quant -> dequant cycle on the final weight.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, bit_width=4, rank=4):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.bit_width = bit_width\n",
        "        self.rank = rank\n",
        "\n",
        "        # Base weight\n",
        "        self.base_weight = nn.Parameter(\n",
        "            torch.randn(out_features, in_features) * 0.02\n",
        "        )\n",
        "\n",
        "        # Low-rank adaptation factors\n",
        "        self.lora_A = nn.Parameter(\n",
        "            torch.randn(self.rank, in_features) * 0.01\n",
        "        )\n",
        "        self.lora_B = nn.Parameter(\n",
        "            torch.randn(out_features, self.rank) * 0.01\n",
        "        )\n",
        "\n",
        "        # Scale param (could also be a buffer or computed offline)\n",
        "        self.scales = nn.Parameter(torch.ones(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1) Combine base weight with low-rank adaptation\n",
        "        adapted_weight = self.base_weight + (self.lora_B @ self.lora_A)\n",
        "\n",
        "        # 2) Scale the weight before quant\n",
        "        scales_2d = self.scales.unsqueeze(1)\n",
        "        scaled_weight = adapted_weight * scales_2d\n",
        "\n",
        "        # 3) Clamp for 4-bit range (naive example)\n",
        "        clamped_weight = torch.clamp(scaled_weight, -8.0, 7.75)\n",
        "\n",
        "        # 4) Map to integer range [0..15]\n",
        "        shifted = clamped_weight + 8.0\n",
        "        scaled = shifted * (15.0 / 15.75)\n",
        "        quantized = torch.round(scaled)\n",
        "\n",
        "        # 5) Dequant\n",
        "        dequant_shifted = quantized * (15.75 / 15.0)\n",
        "        dequant_clamped = dequant_shifted - 8.0\n",
        "        final_weight = dequant_clamped / scales_2d\n",
        "\n",
        "        # 6) Apply final weight to the input\n",
        "        return x @ final_weight.T\n",
        "\n",
        "# ----------------------------------\n",
        "# 2) Simple model using QLoRAQuant\n",
        "# ----------------------------------\n",
        "class SimpleQLoRAModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.quant_linear = QLoRAQuant(\n",
        "            in_features=input_dim,\n",
        "            out_features=hidden_dim,\n",
        "            bit_width=4,\n",
        "            rank=4\n",
        "        )\n",
        "        self.activation = nn.ReLU()\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant_linear(x)\n",
        "        x = self.activation(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ----------------------------------\n",
        "# 3) Example Training Loop w/ torch.compile\n",
        "# ----------------------------------\n",
        "def example_training_q_lora_compile():\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Synthetic dataset: 1000 samples, each w/ 32 features.\n",
        "    X = torch.randn(1000, 32)\n",
        "    y = (X.sum(dim=1) > 0).long()\n",
        "\n",
        "    dataset = TensorDataset(X, y)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Build QLoRA-based model\n",
        "    model = SimpleQLoRAModel(input_dim=32, hidden_dim=16, num_classes=2)\n",
        "\n",
        "    # Compile the model (PyTorch 2.0+)\n",
        "    compiled_model = torch.compile(model)\n",
        "\n",
        "    optimizer = torch.optim.Adam(compiled_model.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    compiled_model.train()\n",
        "    for epoch in range(5):\n",
        "        total_loss = 0.0\n",
        "        for batch_x, batch_y in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            logits = compiled_model(batch_x)\n",
        "            loss = criterion(logits, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1} - Loss = {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "# Demo run\n",
        "if __name__ == \"__main__\":\n",
        "    example_training_q_lora_compile()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss = 0.4699\n",
            "Epoch 2 - Loss = 0.4220\n",
            "Epoch 3 - Loss = 0.3963\n",
            "Epoch 4 - Loss = 0.3737\n",
            "Epoch 5 - Loss = 0.3581\n"
          ]
        }
      ],
      "id": "VUdxpsku4kRU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rswlRTU-4kRV"
      },
      "source": [
        "---\n",
        "## 3) **QLoRA + FSDP**\n",
        "\n",
        "A single-cell script that:\n",
        "- Loads BERT in half precision\n",
        "- Injects LoRA modules\n",
        "- Wraps the model in FSDP (Fully Sharded Data Parallel)\n",
        "- Trains only the LoRA parameters"
      ],
      "id": "rswlRTU-4kRV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MTg9vSG4kRV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "63274182d56c49198bfc8c00d164acc8",
            "adfd86581bab4eccbb598305910c9f21",
            "b22141c1ccb24125b5de0f6fd2d867dd",
            "15b89b9e25234e12a5da75da330b56e4",
            "a40516e74d074956a0907eb1d46d4223",
            "e306353dee0b40f283a629d17d660fa2",
            "fef5d7966c79445086881d4187f3cc34",
            "6ccc53d34e234f27bd023731f4fcab48",
            "a5776d1b8e0d4b0399ea3a1b3e420b5b",
            "5d971220d5be4009a57efa0224ae3c59",
            "1973450a918a4e5ca1329bd633f8b18f",
            "5d75451d941c443e9c18e23b9bac06a2",
            "3e7ce6ae8f3f43eeb83c671989d84da4",
            "693641c8421b46289740299251c33e94",
            "1d38c1a48f1544fba2a715f739700d33",
            "b586c83b6bb946f48b601bbd3aa95310",
            "30f71222f7fc4b24b2f9900dd48bb645",
            "f46165f2dba84469812030df45a13488",
            "5fbc3daca5ee40eea15a549a2489c6db",
            "4f574b814c6e4880a99974cfd43baf6e",
            "70adbdb7d34a4deba573a29153d6206d",
            "197b2aebb24a4f6a83f6e38dfd2acffc",
            "78c889353f8c4bae8e4d0ab87ae8390e",
            "7cdbea7c6b794ea19686bd11ac2f4871",
            "050edd4bdda14001843b1bd9d2c3544b",
            "246925d0612549929d9144418ffe5a46",
            "ac082c7fa44d48939dfeea0253a2e77e",
            "e22009a3157a42ba826948d905be3657",
            "d4c0501dc2b845b3a4e9f6c92522a836",
            "ce51fe8901d040a5941fad2e83eb79a5",
            "3ce162a63a4c42e09d678077d8e8152d",
            "90c8905dfb094718addf5fef9fd5720f",
            "351a570a98d44f6386422555ffd70337"
          ]
        },
        "outputId": "699c3626-c625-4eba-ca07-097fadf56bd9"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "\n",
        "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
        "from torch.distributed.fsdp import ShardingStrategy\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForMaskedLM,\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        ")\n",
        "\n",
        "def setup_distributed():\n",
        "    if dist.is_initialized():\n",
        "        return 0\n",
        "    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n",
        "        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
        "        torch.cuda.set_device(local_rank)\n",
        "        dist.init_process_group(backend=\"nccl\")\n",
        "        return local_rank\n",
        "    else:\n",
        "        # Single GPU fallback\n",
        "        dist.init_process_group(\n",
        "            backend=\"nccl\",\n",
        "            init_method='file:///tmp/fsdp_example',\n",
        "            rank=0,\n",
        "            world_size=1\n",
        "        )\n",
        "        torch.cuda.set_device(0)\n",
        "        return 0\n",
        "\n",
        "def load_bert_fp16(model_name=\"bert-base-uncased\"):\n",
        "    config = AutoConfig.from_pretrained(model_name)\n",
        "    model = AutoModelForMaskedLM.from_pretrained(\n",
        "        model_name,\n",
        "        config=config,\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "    return model\n",
        "\n",
        "class LoRALinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, lora_rank=8, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.lora_down = nn.Linear(in_features, lora_rank, bias=False)\n",
        "        self.lora_up   = nn.Linear(lora_rank, out_features, bias=False)\n",
        "        nn.init.zeros_(self.lora_down.weight)\n",
        "        nn.init.zeros_(self.lora_up.weight)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.alpha * self.lora_up(self.lora_down(x))\n",
        "\n",
        "def inject_lora_in_bert(model, lora_rank=8, alpha=1.0):\n",
        "    linear_list = []\n",
        "    for full_name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            linear_list.append((full_name, module))\n",
        "\n",
        "    for full_name, module in linear_list:\n",
        "        print(f\"Injecting LoRA into: {full_name} => {module}\")\n",
        "        lora_mod = LoRALinear(\n",
        "            module.in_features,\n",
        "            module.out_features,\n",
        "            lora_rank=lora_rank,\n",
        "            alpha=alpha\n",
        "        ).half()  # keep LoRA in half precision\n",
        "\n",
        "        # Register\n",
        "        safe_name = full_name.replace(\".\", \"_\")\n",
        "        model.add_module(f\"lora_{safe_name}\", lora_mod)\n",
        "\n",
        "        # Patch forward\n",
        "        orig_forward = module.forward\n",
        "        def custom_forward(m_self, x, orig_forward=orig_forward, lora_mod=lora_mod):\n",
        "            base_out = orig_forward(x)\n",
        "            lora_out = lora_mod(x)\n",
        "            return base_out + lora_out\n",
        "        module.forward = custom_forward.__get__(module, module.__class__)\n",
        "\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    local_rank = setup_distributed()\n",
        "    model_name = \"bert-base-uncased\"\n",
        "    print(f\"Loading {model_name} in half precision...\")\n",
        "\n",
        "    model = load_bert_fp16(model_name)\n",
        "\n",
        "    # For older FSDP, ensure requires_grad=True on all\n",
        "    for n, p in model.named_parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    print(\"Injecting LoRA (rank=8, alpha=1.0) in float16...\")\n",
        "    model = inject_lora_in_bert(model, lora_rank=8, alpha=1.0)\n",
        "\n",
        "    # Collect LoRA params only\n",
        "    lora_params = []\n",
        "    for name, p in model.named_parameters():\n",
        "        if \"lora_\" in name:\n",
        "            lora_params.append(p)\n",
        "    print(f\"Collected {len(lora_params)} LoRA params for the optimizer.\")\n",
        "\n",
        "    fsdp_model = FSDP(\n",
        "        model,\n",
        "        sharding_strategy=ShardingStrategy.FULL_SHARD,\n",
        "        device_id=torch.cuda.current_device(),\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.AdamW(lora_params, lr=1e-4)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    texts = [\n",
        "        \"Hello world, how are you?\",\n",
        "        \"Testing BERT in half precision with LoRA\",\n",
        "        \"Combining FSDP for memory efficiency!\",\n",
        "    ] * 5\n",
        "\n",
        "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    input_ids = encodings[\"input_ids\"].cuda(local_rank)\n",
        "    attention_mask = encodings[\"attention_mask\"].cuda(local_rank)\n",
        "    labels = input_ids.clone()\n",
        "\n",
        "    # Create random mask for masked LM\n",
        "    with torch.no_grad():\n",
        "        rand_mask = torch.rand_like(labels.float()) < 0.15\n",
        "        labels[~rand_mask] = -100\n",
        "\n",
        "    fsdp_model.train()\n",
        "    epochs = 2\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = fsdp_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if local_rank == 0:\n",
        "            print(f\"Epoch {epoch+1} / {epochs} done, loss = {loss.item()}\")\n",
        "\n",
        "    dist.barrier()\n",
        "    if local_rank == 0:\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "# Actually call main() in the same cell so we see output\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading bert-base-uncased in half precision...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Injecting LoRA (rank=8, alpha=1.0) in float16...\n",
            "Injecting LoRA into: bert.encoder.layer.0.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.0.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.0.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.0.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.0.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.0.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.1.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.1.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.1.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.1.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.1.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.1.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.2.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.2.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.2.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.2.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.2.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.2.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.3.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.3.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.3.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.3.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.3.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.3.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.4.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.4.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.4.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.4.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.4.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.4.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.5.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.5.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.5.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.5.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.5.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.5.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.6.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.6.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.6.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.6.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.6.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.6.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.7.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.7.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.7.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.7.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.7.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.7.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.8.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.8.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.8.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.8.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.8.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.8.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.9.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.9.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.9.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.9.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.9.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.9.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.10.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.10.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.10.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.10.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.10.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.10.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.11.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.11.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.11.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.11.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.11.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.11.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: cls.predictions.transform.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: cls.predictions.decoder => Linear(in_features=768, out_features=30522, bias=True)\n",
            "Collected 148 LoRA params for the optimizer.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63274182d56c49198bfc8c00d164acc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d75451d941c443e9c18e23b9bac06a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78c889353f8c4bae8e4d0ab87ae8390e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 2 done, loss = 4.46875\n",
            "Epoch 2 / 2 done, loss = 4.09765625\n",
            "Training complete!\n"
          ]
        }
      ],
      "id": "6MTg9vSG4kRV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-C6_qnN4kRW"
      },
      "source": [
        "---\n",
        "## 4) **Memory-Efficient Backprop** (Chunked Final MatMul + Cross-Entropy)\n",
        "\n",
        "This code chunk demonstrates how to avoid creating a huge `[B*S, vocab]` logits matrix at once, by chunking the matmul into smaller pieces. This reduces memory usage at the cost of multiple partial computations."
      ],
      "id": "z-C6_qnN4kRW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NwLjryL4kRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1820614-750f-4601-eade-4bb3db4a074a"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def forward_chunked_linear_ce(\n",
        "    X,       # [batch_size * seq_len, hidden_dim]\n",
        "    W,       # [hidden_dim, vocab_size]\n",
        "    targets, # [batch_size * seq_len]\n",
        "    chunk_size=8192\n",
        "):\n",
        "    \"\"\"\n",
        "    Perform chunked X @ W => partial logits => cross entropy without storing the entire [B*S, vocab].\n",
        "    \"\"\"\n",
        "    batch_tokens = X.shape[0]  # B*S\n",
        "    vocab_size = W.shape[1]\n",
        "\n",
        "    logsumexp_buf = None\n",
        "    correct_logits = torch.zeros(batch_tokens, device=X.device)\n",
        "\n",
        "    for start_col in range(0, vocab_size, chunk_size):\n",
        "        end_col = min(start_col + chunk_size, vocab_size)\n",
        "        W_chunk = W[:, start_col:end_col]\n",
        "        logits_chunk = X.matmul(W_chunk)\n",
        "\n",
        "        # incremental log-sum-exp\n",
        "        if logsumexp_buf is None:\n",
        "            logsumexp_buf = torch.logsumexp(logits_chunk, dim=1)\n",
        "        else:\n",
        "            combined = torch.stack([logsumexp_buf, torch.logsumexp(logits_chunk, dim=1)], dim=0)\n",
        "            logsumexp_buf = torch.logsumexp(combined, dim=0)\n",
        "\n",
        "        # gather correct logits if they're in this chunk\n",
        "        mask = (targets >= start_col) & (targets < end_col)\n",
        "        if mask.any():\n",
        "            local_positions = targets[mask] - start_col\n",
        "            correct_logit_vals = logits_chunk[mask, local_positions]\n",
        "            correct_logits[mask] = correct_logit_vals\n",
        "\n",
        "    ce = -(correct_logits - logsumexp_buf)\n",
        "    loss = ce.mean()\n",
        "    return loss\n",
        "\n",
        "class MemoryEfficientLinearCELoss(nn.Module):\n",
        "    def __init__(self, hidden_dim, vocab_size, chunk_size=8192):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.chunk_size = chunk_size\n",
        "        self.weight = nn.Parameter(torch.randn(hidden_dim, vocab_size) * 0.02)\n",
        "\n",
        "    def forward(self, X, targets):\n",
        "        return forward_chunked_linear_ce(\n",
        "            X,\n",
        "            self.weight,\n",
        "            targets,\n",
        "            chunk_size=self.chunk_size\n",
        "        )\n",
        "\n",
        "# Demo\n",
        "def example_mem_eff_backprop():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    batch_size = 2\n",
        "    seq_len = 5\n",
        "    hidden_dim = 8\n",
        "    vocab_size = 20\n",
        "    chunk_size = 5\n",
        "\n",
        "    X = torch.randn(batch_size * seq_len, hidden_dim, requires_grad=True, device=device)\n",
        "    targets = torch.randint(0, vocab_size, size=(batch_size * seq_len,), dtype=torch.long, device=device)\n",
        "\n",
        "    mem_eff_ce = MemoryEfficientLinearCELoss(hidden_dim, vocab_size, chunk_size=chunk_size).to(device)\n",
        "    loss = mem_eff_ce(X, targets)\n",
        "    loss.backward()\n",
        "\n",
        "    print(f\"Loss: {loss.item():.4f}\")\n",
        "    print(\"Grad w.r.t X:\", X.grad)\n",
        "    print(\"Grad w.r.t W:\", mem_eff_ce.weight.grad)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    example_mem_eff_backprop()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.9659\n",
            "Grad w.r.t X: tensor([[ 1.6163e-03,  1.8064e-03,  1.0825e-03, -2.3256e-03, -2.5538e-03,\n",
            "          1.5989e-03,  3.3694e-03,  3.7867e-04],\n",
            "        [ 1.6906e-03,  1.8409e-03,  1.1629e-03, -2.3396e-03, -2.4640e-03,\n",
            "          1.6535e-03,  3.5120e-03,  4.4430e-04],\n",
            "        [-3.4139e-03, -3.5918e-03,  3.9076e-04,  3.1014e-03,  2.6923e-03,\n",
            "         -1.7271e-03,  1.5015e-03, -2.0493e-03],\n",
            "        [-2.2461e-03, -3.2119e-03,  1.1062e-03,  3.4821e-05, -3.2107e-04,\n",
            "          2.6581e-03,  1.3169e-04,  5.5187e-04],\n",
            "        [-7.0122e-04, -3.0019e-04, -1.1601e-03,  2.6959e-03, -1.0794e-03,\n",
            "          2.0529e-03,  1.7755e-03,  1.8649e-03],\n",
            "        [-1.1357e-03,  4.3799e-05,  2.1444e-04, -2.3237e-03, -5.2027e-03,\n",
            "         -1.5427e-03, -8.1709e-04, -3.0248e-03],\n",
            "        [ 8.1323e-04,  2.5520e-03,  5.1153e-04, -1.5681e-03,  2.4222e-03,\n",
            "          2.8720e-03,  1.7460e-03,  1.9653e-03],\n",
            "        [-4.6521e-03,  1.2229e-03, -4.4408e-03,  4.1418e-03,  6.8102e-04,\n",
            "         -1.2283e-03, -1.9430e-03,  1.2212e-03],\n",
            "        [ 2.3391e-03,  1.5629e-03,  2.7967e-03, -2.7873e-03,  3.0861e-04,\n",
            "          2.2352e-04, -1.6059e-03,  1.8244e-03],\n",
            "        [ 1.9821e-03,  6.1786e-04,  1.6796e-04,  1.3931e-03,  4.7771e-04,\n",
            "         -3.2562e-03, -3.6285e-04, -5.8822e-04]], device='cuda:0')\n",
            "Grad w.r.t W: tensor([[ 0.0219,  0.0118,  0.0242, -0.1950,  0.0221,  0.0251,  0.0218, -0.0688,\n",
            "          0.0233,  0.0219, -0.0392,  0.0435,  0.0243, -0.1094,  0.0249,  0.0229,\n",
            "          0.0101,  0.1013,  0.0230, -0.0096],\n",
            "        [ 0.0103, -0.0078,  0.0098, -0.0504,  0.0087,  0.0080,  0.0111, -0.1361,\n",
            "          0.0071,  0.0063,  0.1020,  0.0262,  0.0096,  0.2435,  0.0083,  0.0080,\n",
            "         -0.1806,  0.0624,  0.0073, -0.1538],\n",
            "        [-0.0150,  0.2131, -0.0121, -0.0971, -0.0145, -0.0157, -0.0124, -0.0621,\n",
            "         -0.0146, -0.0135,  0.0448, -0.0496, -0.0143,  0.0593, -0.0146, -0.0130,\n",
            "         -0.0705, -0.0047, -0.0158,  0.1221],\n",
            "        [-0.0030, -0.0408, -0.0028, -0.0580, -0.0025, -0.0036, -0.0017,  0.0748,\n",
            "         -0.0035, -0.0008,  0.1487, -0.0402, -0.0027,  0.0007, -0.0046, -0.0026,\n",
            "         -0.1202, -0.0970, -0.0031,  0.1629],\n",
            "        [ 0.0188, -0.2219,  0.0205,  0.0360,  0.0182,  0.0199,  0.0177, -0.0720,\n",
            "          0.0191,  0.0197, -0.0947, -0.0304,  0.0197, -0.0231,  0.0208,  0.0189,\n",
            "          0.0549,  0.0299,  0.0196,  0.1084],\n",
            "        [ 0.0030, -0.1220,  0.0025, -0.0547,  0.0027,  0.0024,  0.0030,  0.0297,\n",
            "          0.0019,  0.0026,  0.0976, -0.1089,  0.0031,  0.0736,  0.0028,  0.0018,\n",
            "          0.0476,  0.0524,  0.0026, -0.0436],\n",
            "        [-0.0087,  0.2313, -0.0047, -0.0143, -0.0081, -0.0107, -0.0067, -0.1020,\n",
            "         -0.0086, -0.0069,  0.0392, -0.1315, -0.0079,  0.0584, -0.0074, -0.0068,\n",
            "          0.0093, -0.0477, -0.0098,  0.0435],\n",
            "        [-0.0060,  0.0203, -0.0071,  0.1083, -0.0049, -0.0051, -0.0079,  0.1357,\n",
            "         -0.0045, -0.0043, -0.0277,  0.0790, -0.0059, -0.2317, -0.0059, -0.0052,\n",
            "          0.0251, -0.0299, -0.0044, -0.0177]], device='cuda:0')\n"
          ]
        }
      ],
      "id": "9NwLjryL4kRW"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 1) Confirm GPU type (T4, A100, etc.).\n",
        "# ============================================\n",
        "!nvidia-smi\n",
        "\n",
        "# ============================================\n",
        "# 2) [Optional] Install system-level CUDA 11.8 libs\n",
        "#    so bitsandbytes can find libcusparse.so.11, etc.\n",
        "#    If you get 'libcusparse.so.11 not found' errors,\n",
        "#    installing these packages often helps.\n",
        "# ============================================\n",
        "!apt-get update -y\n",
        "!apt-get install -y --no-install-recommends \\\n",
        "    cuda-cudart-11-8 \\\n",
        "    cuda-cusparse-11-8 \\\n",
        "    cuda-libraries-11-8\n",
        "\n",
        "# ============================================\n",
        "# 3) Wipe older Torch/bitsandbytes/xformers/triton\n",
        "#    to avoid conflicts.\n",
        "# ============================================\n",
        "!pip uninstall -y torch bitsandbytes xformers triton\n",
        "\n",
        "# ============================================\n",
        "# 4) Install PyTorch 2.0.1+cu118, matching torchvision/torchaudio.\n",
        "# ============================================\n",
        "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 \\\n",
        "    --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# ============================================\n",
        "# 5) (Optional) Re-install pinned bitsandbytes, xformers, triton\n",
        "#    to confirm environment is consistent.\n",
        "#    (Though build_unsloth.py may also install them depending on the markers.)\n",
        "# ============================================\n",
        "!pip install bitsandbytes==0.41.1 xformers==0.0.22 triton==2.0.0 \\\n",
        "    --extra-index-url https://download.pytorch.org/whl/cu118\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fs6PQ2LLcC7",
        "outputId": "3cdf3d81-cc18-4547-ec22-ff7a07e2ede2"
      },
      "id": "-fs6PQ2LLcC7",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb 20 16:26:18 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package cuda-cusparse-11-8\n",
            "Found existing installation: torch 2.0.1+cu118\n",
            "Uninstalling torch-2.0.1+cu118:\n",
            "  Successfully uninstalled torch-2.0.1+cu118\n",
            "Found existing installation: bitsandbytes 0.41.1\n",
            "Uninstalling bitsandbytes-0.41.1:\n",
            "  Successfully uninstalled bitsandbytes-0.41.1\n",
            "Found existing installation: xformers 0.0.22\n",
            "Uninstalling xformers-0.0.22:\n",
            "  Successfully uninstalled xformers-0.0.22\n",
            "Found existing installation: triton 2.0.0\n",
            "Uninstalling triton-2.0.0:\n",
            "  Successfully uninstalled triton-2.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.1+cu118\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp311-cp311-linux_x86_64.whl (2267.3 MB)\n",
            "Requirement already satisfied: torchvision==0.15.2+cu118 in /usr/local/lib/python3.11/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torchaudio==2.0.2+cu118 in /usr/local/lib/python3.11/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.1.5)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1+cu118)\n",
            "  Using cached https://download.pytorch.org/whl/triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (11.1.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (3.31.4)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1+cu118) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\n",
            "Installing collected packages: triton, torch\n",
            "Successfully installed torch-2.0.1+cu118 triton-2.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Collecting bitsandbytes==0.41.1\n",
            "  Using cached bitsandbytes-0.41.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting xformers==0.0.22\n",
            "  Using cached xformers-0.0.22-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers==0.0.22) (1.26.4)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.11/dist-packages (from xformers==0.0.22) (2.0.1+cu118)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.31.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.17.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (18.1.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->xformers==0.0.22) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->xformers==0.0.22) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->xformers==0.0.22) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->xformers==0.0.22) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1->xformers==0.0.22) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1->xformers==0.0.22) (1.3.0)\n",
            "Using cached bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
            "Using cached xformers-0.0.22-cp311-cp311-manylinux2014_x86_64.whl (211.6 MB)\n",
            "Installing collected packages: bitsandbytes, xformers\n",
            "Successfully installed bitsandbytes-0.41.1 xformers-0.0.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOoGH3Io4kRW"
      },
      "source": [
        "---\n",
        "## 5) **Windows Support**\n",
        "\n",
        "Below are two scripts:\n",
        "- **`build_unsloth.py`**: Creates a `pyproject.toml`, builds a wheel, and installs it.\n",
        "- **`test_deps.py`**: Installs bitsandbytes, xformers, triton, then tests them.\n",
        "\n",
        "These are primarily relevant for letting `unsloth` (and associated libraries) build on Windows."
      ],
      "id": "cOoGH3Io4kRW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTV9fO4V4kRW",
        "outputId": "c716e496-6cfc-4b17-98b1-287a4e0cea81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile build_unsloth.py\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# 1) Write pyproject.toml with correct license syntax, allowing Python 3.9+\n",
        "toml_content = \"\"\"\\\n",
        "[project]\n",
        "name = \"unsloth\"\n",
        "version = \"0.1.0\"\n",
        "description = \"unsloth: Windows-friendly package for bitsandbytes, xformers, triton\"\n",
        "readme = \"README.md\"\n",
        "requires-python = \">=3.9\"\n",
        "\n",
        "[project.license]\n",
        "text = \"MIT\"\n",
        "\n",
        "authors = [\n",
        "  { name = \"Your Name\", email = \"you@example.com\" }\n",
        "]\n",
        "\n",
        "# Dependencies only install if environment markers match (e.g., Windows).\n",
        "# On Colab Linux + CUDA 11.8, these might not do anything,\n",
        "# but we still define them to show the \"Windows-friendly\" idea.\n",
        "dependencies = [\n",
        "  \"torch==2.0.1+cu118; platform_system=='Windows'\",\n",
        "  \"transformers==4.30.2\",\n",
        "  \"accelerate==0.20.3\",\n",
        "  \"bitsandbytes==0.39.1\",\n",
        "  \"xformers==0.0.20\",\n",
        "  \"triton==2.0.0\",\n",
        "]\n",
        "\n",
        "[build-system]\n",
        "requires = [\"setuptools>=61\", \"wheel\"]\n",
        "build-backend = \"setuptools.build_meta\"\n",
        "\"\"\"\n",
        "\n",
        "with open(\"pyproject.toml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(toml_content)\n",
        "\n",
        "# 2) Minimal package structure\n",
        "os.makedirs(\"src/unsloth\", exist_ok=True)\n",
        "with open(\"src/unsloth/__init__.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write('# unsloth package init - minimal\\n')\n",
        "\n",
        "# Minimal README\n",
        "with open(\"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"# unsloth\\n\\nA Windows-friendly package with bitsandbytes, xformers, triton.\\n\")\n",
        "\n",
        "print(\"=== pyproject.toml created. Attempting to build and install locally... ===\")\n",
        "\n",
        "# 3) Upgrade pip and install build tools\n",
        "subprocess.run([\n",
        "    \"python\", \"-m\", \"pip\", \"install\", \"--upgrade\",\n",
        "    \"pip\", \"build\", \"setuptools>=61\", \"wheel\"\n",
        "], check=True)\n",
        "\n",
        "# 4) Build the wheel\n",
        "build_result = subprocess.run([\"python\", \"-m\", \"build\"], capture_output=True, text=True)\n",
        "if build_result.returncode != 0:\n",
        "    print(\"ERROR: Build failed. Output:\\n\")\n",
        "    print(build_result.stdout)\n",
        "    print(build_result.stderr)\n",
        "    sys.exit(1)\n",
        "\n",
        "# 5) Check dist/ directory\n",
        "if not os.path.isdir(\"dist\"):\n",
        "    print(\"ERROR: 'dist/' directory not found, build likely failed.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "dist_files = os.listdir(\"dist\")\n",
        "if not dist_files:\n",
        "    print(\"ERROR: 'dist/' directory is empty, no wheel found.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "wheel_files = [f for f in dist_files if f.endswith(\".whl\")]\n",
        "if not wheel_files:\n",
        "    print(\"ERROR: No .whl file found in dist/. Found:\", dist_files)\n",
        "    sys.exit(1)\n",
        "\n",
        "wheel_path = os.path.join(\"dist\", wheel_files[0])\n",
        "\n",
        "# 6) Install the wheel with extra index for cu118\n",
        "cmd = [\n",
        "    \"python\",\n",
        "    \"-m\",\n",
        "    \"pip\",\n",
        "    \"install\",\n",
        "    wheel_path,\n",
        "    \"--extra-index-url\",\n",
        "    \"https://download.pytorch.org/whl/cu118\"\n",
        "]\n",
        "print(\"\\nInstalling wheel with command:\", \" \".join(cmd))\n",
        "\n",
        "install_result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "if install_result.returncode != 0:\n",
        "    print(\"ERROR: Failed to install the wheel. Output:\\n\")\n",
        "    print(install_result.stdout)\n",
        "    print(install_result.stderr)\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"Successfully installed the unsloth wheel from dist/!\\n\")\n",
        "print(\"Installation log:\")\n",
        "print(install_result.stdout)\n",
        "\n",
        "# ============== End of build_unsloth.py ==============\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting build_unsloth.py\n"
          ]
        }
      ],
      "id": "uTV9fO4V4kRW"
    },
    {
      "cell_type": "code",
      "source": [
        "!python build_unsloth.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqNmdMZXJKNo",
        "outputId": "394db1ac-0330-4c45-f339-b505d987ff41"
      },
      "id": "PqNmdMZXJKNo",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== pyproject.toml created. Attempting to build and install locally... ===\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting build\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: setuptools>=61 in /usr/local/lib/python3.11/dist-packages (75.1.0)\n",
            "Collecting setuptools>=61\n",
            "  Downloading setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build) (24.2)\n",
            "Collecting pyproject_hooks (from build)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading setuptools-75.8.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: setuptools, pyproject_hooks, pip, build\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.1.0\n",
            "    Uninstalling setuptools-75.1.0:\n",
            "      Successfully uninstalled setuptools-75.1.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed build-1.2.2.post1 pip-25.0.1 pyproject_hooks-1.2.0 setuptools-75.8.0\n",
            "\n",
            "Installing wheel with command: python -m pip install dist/unsloth-0.1.0-py3-none-any.whl --extra-index-url https://download.pytorch.org/whl/cu118\n",
            "Successfully installed the unsloth wheel from dist/!\n",
            "\n",
            "Installation log:\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Processing ./dist/unsloth-0.1.0-py3-none-any.whl\n",
            "Installing collected packages: unsloth\n",
            "Successfully installed unsloth-0.1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbn5gu1q4kRX",
        "outputId": "b6f1c69d-1b9e-4bb5-9f6d-68cb979cc6e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "################################################################################\n",
        "# ONE-CELL COLAB SCRIPT: PyTorch Nightly (2.2.0 + cu121),\n",
        "# bitsandbytes 0.45.2, xformers 0.0.24, tested on A100 CUDA 12.x\n",
        "################################################################################\n",
        "\n",
        "print(\"=== Checking GPU and driver info ===\")\n",
        "!nvidia-smi\n",
        "\n",
        "print(\"\\n=== 1) Uninstall older Torch, bitsandbytes, xformers, triton ===\")\n",
        "!pip uninstall -y torch bitsandbytes xformers triton\n",
        "\n",
        "print(\"\\n=== 2) Install PyTorch NIGHTLY 2.2.0+cu121, plus torchvision, torchaudio\")\n",
        "print(\"         from the official 'nightly/cu121' index. ===\")\n",
        "\n",
        "# We use --pre (pre-release) and a special index URL for nightly cu121 builds.\n",
        "!pip install --pre torch torchvision torchaudio \\\n",
        "    --index-url https://download.pytorch.org/whl/nightly/cu121\n",
        "\n",
        "print(\"\\n=== 3) Install bitsandbytes 0.45.2 and xformers 0.0.24 (built for Torch 2.2.0+cu121) ===\")\n",
        "# We'll just use PyPI. bitsandbytes 0.45.2 has CUDA 12.1 support.\n",
        "# xformers 0.0.24 is built for Torch 2.2.0+cu121, so it won't conflict.\n",
        "!pip install bitsandbytes==0.45.2 xformers==0.0.24\n",
        "\n",
        "print(\"\\n=== 4) Write test_deps.py script to verify bitsandbytes, xformers, and triton ===\")\n",
        "\n",
        "test_deps_code = \"\"\"import os\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "os.environ[\"BNB_CUDA_VERSION\"] = \"121\"  # bitsandbytes tries libbitsandbytes_cuda121.so\n",
        "\n",
        "# 1) Test bitsandbytes\n",
        "try:\n",
        "    import bitsandbytes as bnb\n",
        "    print(\"\\\\n=== bitsandbytes import OK ===\")\n",
        "    linear_8bit = bnb.nn.Linear8bitLt(128, 64).cuda()\n",
        "    dummy_in = torch.randn(16, 128, device='cuda', dtype=torch.float16)\n",
        "    dummy_out = linear_8bit(dummy_in)\n",
        "    print('bitsandbytes linear8bit forward pass successful. Output shape:', dummy_out.shape)\n",
        "except Exception as ex:\n",
        "    print('bitsandbytes usage error:', ex)\n",
        "    sys.exit(1)\n",
        "\n",
        "# 2) Test xformers\n",
        "try:\n",
        "    import xformers\n",
        "    print(\"\\\\n=== xformers import OK ===\")\n",
        "    from xformers.ops import fmha\n",
        "    q = torch.randn((1, 32, 8, 64), device='cuda', dtype=torch.float16)\n",
        "    k = torch.randn((1, 32, 8, 64), device='cuda', dtype=torch.float16)\n",
        "    v = torch.randn((1, 32, 8, 64), device='cuda', dtype=torch.float16)\n",
        "    out = fmha.memory_efficient_attention(q, k, v)\n",
        "    print('xformers fmha output shape:', out.shape)\n",
        "except Exception as ex:\n",
        "    print('xformers usage error:', ex)\n",
        "    sys.exit(1)\n",
        "\n",
        "# 3) Test triton (bundled in Torch 2.2.0 nightly)\n",
        "try:\n",
        "    import triton\n",
        "    import triton.language as tl\n",
        "    print(\"\\\\n=== triton import OK ===\")\n",
        "\n",
        "    @triton.jit\n",
        "    def add_kernel(x_ptr, y_ptr, output_ptr, BLOCK_SIZE: tl.constexpr):\n",
        "        pid = tl.program_id(0)\n",
        "        offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "        x = tl.load(x_ptr + offset)\n",
        "        y = tl.load(y_ptr + offset)\n",
        "        tl.store(output_ptr + offset, x + y)\n",
        "\n",
        "    x_t = torch.randn(1024, device='cuda')\n",
        "    y_t = torch.randn(1024, device='cuda')\n",
        "    output_t = torch.empty(1024, device='cuda')\n",
        "    grid = (1024 // 256,)\n",
        "    add_kernel[grid](x_t, y_t, output_t, BLOCK_SIZE=256)\n",
        "    print('triton add_kernel test, first 5 results:', output_t[:5].tolist())\n",
        "except Exception as ex:\n",
        "    print('triton usage error:', ex)\n",
        "    sys.exit(1)\n",
        "\n",
        "print('\\\\nAll tests passed! bitsandbytes, xformers, and triton are working.')\n",
        "\"\"\"\n",
        "\n",
        "with open(\"test_deps.py\", \"w\") as f:\n",
        "    f.write(test_deps_code)\n",
        "\n",
        "print(\"\\n=== 5) Run test_deps.py to confirm everything works with Torch 2.2.0+cu121 ===\")\n",
        "!python test_deps.py\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Checking GPU and driver info ===\n",
            "Thu Feb 20 17:19:27 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "=== 1) Uninstall older Torch, bitsandbytes, xformers, triton ===\n",
            "Found existing installation: torch 2.1.0+cu121\n",
            "Uninstalling torch-2.1.0+cu121:\n",
            "  Successfully uninstalled torch-2.1.0+cu121\n",
            "Found existing installation: bitsandbytes 0.45.2\n",
            "Uninstalling bitsandbytes-0.45.2:\n",
            "  Successfully uninstalled bitsandbytes-0.45.2\n",
            "Found existing installation: xformers 0.0.25\n",
            "Uninstalling xformers-0.0.25:\n",
            "  Successfully uninstalled xformers-0.0.25\n",
            "Found existing installation: triton 2.1.0\n",
            "Uninstalling triton-2.1.0:\n",
            "  Successfully uninstalled triton-2.1.0\n",
            "\n",
            "=== 2) Install PyTorch NIGHTLY 2.2.0+cu121, plus torchvision, torchaudio\n",
            "         from the official 'nightly/cu121' index. ===\n",
            "Looking in indexes: https://download.pytorch.org/whl/nightly/cu121\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/torch-2.6.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (768.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.0/768.0 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m122.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Collecting pytorch-triton==3.1.0+cf34004b8a (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.1.0%2Bcf34004b8a-cp311-cp311-linux_x86_64.whl (239.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.7/239.7 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.32.3)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/torchvision-0.20.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/torchaudio-2.5.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, pytorch-triton, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
            "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu121\n",
            "    Uninstalling torchvision-0.16.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.1.0+cu121\n",
            "    Uninstalling torchaudio-2.1.0+cu121:\n",
            "      Successfully uninstalled torchaudio-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0.dev20241112+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-9.1.0.70 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 pytorch-triton-3.1.0+cf34004b8a torch-2.6.0.dev20241112+cu121 torchaudio-2.5.0.dev20241112+cu121 torchvision-0.20.0.dev20241112+cu121\n",
            "\n",
            "=== 3) Install bitsandbytes 0.45.2 and xformers 0.0.24 (built for Torch 2.2.0+cu121) ===\n",
            "Collecting bitsandbytes==0.45.2\n",
            "  Using cached bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting xformers==0.0.24\n",
            "  Using cached xformers-0.0.24-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes==0.45.2) (2.6.0.dev20241112+cu121)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes==0.45.2) (1.26.4)\n",
            "Collecting torch<3,>=2.0 (from bitsandbytes==0.45.2)\n",
            "  Using cached torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<3,>=2.0->bitsandbytes==0.45.2)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<3,>=2.0->bitsandbytes==0.45.2)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.1.105)\n",
            "Collecting triton==2.2.0 (from torch<3,>=2.0->bitsandbytes==0.45.2)\n",
            "  Using cached triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.0->bitsandbytes==0.45.2) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes==0.45.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<3,>=2.0->bitsandbytes==0.45.2) (1.3.0)\n",
            "Using cached bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
            "Using cached xformers-0.0.24-cp311-cp311-manylinux2014_x86_64.whl (218.2 MB)\n",
            "Using cached torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl (755.5 MB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Using cached triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "Installing collected packages: triton, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch, xformers, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0.dev20241112+cu121\n",
            "    Uninstalling torch-2.6.0.dev20241112+cu121:\n",
            "      Successfully uninstalled torch-2.6.0.dev20241112+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.20.0.dev20241112+cu121 requires torch==2.6.0.dev20241112, but you have torch 2.2.0 which is incompatible.\n",
            "torchaudio 2.5.0.dev20241112+cu121 requires torch==2.6.0.dev20241112, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.45.2 nvidia-cudnn-cu12-8.9.2.26 nvidia-nccl-cu12-2.19.3 torch-2.2.0 triton-2.2.0 xformers-0.0.24\n",
            "\n",
            "=== 4) Write test_deps.py script to verify bitsandbytes, xformers, and triton ===\n",
            "\n",
            "=== 5) Run test_deps.py to confirm everything works with Torch 2.2.0+cu121 ===\n",
            "WARNING: BNB_CUDA_VERSION=121 environment variable detected; loading libbitsandbytes_cuda121.so.\n",
            "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
            "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
            "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
            "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
            "\n",
            "\n",
            "=== bitsandbytes import OK ===\n",
            "bitsandbytes linear8bit forward pass successful. Output shape: torch.Size([16, 64])\n",
            "\n",
            "=== xformers import OK ===\n",
            "xformers fmha output shape: torch.Size([1, 32, 8, 64])\n",
            "\n",
            "=== triton import OK ===\n",
            "triton add_kernel test, first 5 results: [1.5259300470352173, -1.2502985000610352, 3.3543295860290527, 0.27865782380104065, -0.015028402209281921]\n",
            "\n",
            "All tests passed! bitsandbytes, xformers, and triton are working.\n"
          ]
        }
      ],
      "id": "wbn5gu1q4kRX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-O4Xdz04kRX"
      },
      "source": [
        "---\n",
        "## 6) **Flexible Attention**\n",
        "\n",
        "Here’s a snippet that builds various attention masks (causal, sliding, etc.) and uses a chunked approach, plus `torch.compile` if you like. This demonstration shows different mask types in one place."
      ],
      "id": "z-O4Xdz04kRX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMGxXZCW4kRX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339d3c23-a355-40f8-edcd-f95311af2464"
      },
      "source": [
        "import sys\n",
        "import math\n",
        "import torch\n",
        "\n",
        "def build_attention_mask(seq_len, mask_type=\"causal\", window_size=64, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Creates an attention mask:\n",
        "      - \"causal\": blocks j > i (standard auto-regressive mask).\n",
        "      - \"sliding\": local window = ±window_size around each token.\n",
        "    \"\"\"\n",
        "    mask = torch.zeros(seq_len, seq_len, device=device)\n",
        "    if mask_type == \"causal\":\n",
        "        # Triangular upper matrix => block j>i\n",
        "        casual_mat = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1)\n",
        "        mask[casual_mat.bool()] = float(\"-1e9\")\n",
        "    elif mask_type == \"sliding\":\n",
        "        # For each position i, block everything outside [i - window_size, i + window_size]\n",
        "        for i in range(seq_len):\n",
        "            left = max(0, i - window_size)\n",
        "            right = min(seq_len, i + window_size + 1)\n",
        "            mask[i, :left] = float(\"-1e9\")\n",
        "            mask[i, right:] = float(\"-1e9\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown mask_type={mask_type}\")\n",
        "    return mask\n",
        "\n",
        "def flex_attention(q, k, v, attn_mask):\n",
        "    \"\"\"\n",
        "    Simple scaled dot-product attention:\n",
        "      q, k, v: shape [batch, seq_len, d_model]\n",
        "      attn_mask: shape [seq_len, seq_len], large negative => blocked\n",
        "    \"\"\"\n",
        "    d_model = q.shape[-1]\n",
        "    # (batch, seq_len, d_model) @ (batch, d_model, seq_len) => (batch, seq_len, seq_len)\n",
        "    attn_scores = torch.bmm(q, k.transpose(1, 2)) / math.sqrt(d_model)\n",
        "\n",
        "    # Apply the mask (broadcast => (batch, seq_len, seq_len))\n",
        "    attn_scores = attn_scores + attn_mask.unsqueeze(0)\n",
        "\n",
        "    # Softmax and multiply by v\n",
        "    attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "    out = torch.bmm(attn_probs, v)\n",
        "    return out\n",
        "\n",
        "# Fallback approach for Python 3.11:\n",
        "# - If Python < 3.11 => we compile\n",
        "# - If Python >= 3.11 => skip compile to avoid runtime error\n",
        "if sys.version_info < (3, 11):\n",
        "    compiled_flex_attention = torch.compile(flex_attention, mode=\"default\")\n",
        "    print(\"Using torch.compile on Python < 3.11.\")\n",
        "else:\n",
        "    compiled_flex_attention = flex_attention\n",
        "    print(\"Skipping torch.compile (Python 3.11+ not yet supported).\")\n",
        "\n",
        "def run_flex_attention_demo():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    batch_size = 2\n",
        "    d_model = 64\n",
        "\n",
        "    for mask_type in [\"causal\", \"sliding\"]:\n",
        "        print(f\"\\n===> Testing mask_type = {mask_type}\")\n",
        "        for seq_len in [128, 256, 300, 512]:\n",
        "            q = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "            k = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "            v = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "\n",
        "            base_mask = build_attention_mask(seq_len, mask_type=mask_type, device=device)\n",
        "            out = compiled_flex_attention(q, k, v, base_mask)\n",
        "            print(f\"seq_len={seq_len}, out.shape={out.shape}, mask_type={mask_type}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_flex_attention_demo()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping torch.compile (Python 3.11+ not yet supported).\n",
            "\n",
            "===> Testing mask_type = causal\n",
            "seq_len=128, out.shape=torch.Size([2, 128, 64]), mask_type=causal\n",
            "seq_len=256, out.shape=torch.Size([2, 256, 64]), mask_type=causal\n",
            "seq_len=300, out.shape=torch.Size([2, 300, 64]), mask_type=causal\n",
            "seq_len=512, out.shape=torch.Size([2, 512, 64]), mask_type=causal\n",
            "\n",
            "===> Testing mask_type = sliding\n",
            "seq_len=128, out.shape=torch.Size([2, 128, 64]), mask_type=sliding\n",
            "seq_len=256, out.shape=torch.Size([2, 256, 64]), mask_type=sliding\n",
            "seq_len=300, out.shape=torch.Size([2, 300, 64]), mask_type=sliding\n",
            "seq_len=512, out.shape=torch.Size([2, 512, 64]), mask_type=sliding\n"
          ]
        }
      ],
      "id": "LMGxXZCW4kRX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJpiQQgp4kRY"
      },
      "source": [
        "---\n",
        "## 7) **Sequence Classification Patch** (LoRA + `AutoModelForSequenceClassification`)\n",
        "\n",
        "We patch `AutoModelForSequenceClassification` by injecting LoRA modules into every `nn.Linear` in the model, then fine-tune only the LoRA parameters on a toy dataset."
      ],
      "id": "YJpiQQgp4kRY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtzfPXUE4kRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ae039b-bf50-450e-def4-0c2f0ecf0c71"
      },
      "source": [
        "################################################################################\n",
        "# SINGLE-CELL COLAB SCRIPT:\n",
        "# LoRA BERT classification w/ Torch 2.1.0+cu121 & Transformers 4.31.0\n",
        "# Removing peft & older libraries => fix the 'adapter_kwargs' error.\n",
        "################################################################################\n",
        "\n",
        "print(\"=== Checking GPU / driver info ===\")\n",
        "!nvidia-smi\n",
        "\n",
        "print(\"\\n=== 1) Uninstall conflicting packages (torch, transformers, peft, xformers, etc.) ===\")\n",
        "!pip uninstall -y torch transformers peft xformers tokenizers bitsandbytes\n",
        "\n",
        "print(\"\\n=== 2) Install Torch 2.1.0+cu121 & Transformers==4.31.0 ===\")\n",
        "!pip install torch==2.1.0+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers==4.31.0\n",
        "\n",
        "print(\"\\n=== 3) Running your LoRA BERT classification code ===\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        ")\n",
        "\n",
        "class ToyClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=32):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class LoRALinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, lora_rank=4, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.lora_down = nn.Linear(in_features, lora_rank, bias=False)\n",
        "        self.lora_up   = nn.Linear(lora_rank, out_features, bias=False)\n",
        "        nn.init.zeros_(self.lora_down.weight)\n",
        "        nn.init.zeros_(self.lora_up.weight)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.alpha * self.lora_up(self.lora_down(x))\n",
        "\n",
        "def patch_model_for_sequence_classification(model, lora_rank=4, alpha=1.0):\n",
        "    modules_to_patch = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            modules_to_patch.append((name, module))\n",
        "\n",
        "    for full_name, module in modules_to_patch:\n",
        "        safe_name = full_name.replace(\".\", \"_\")\n",
        "        lora_mod = LoRALinear(\n",
        "            module.in_features,\n",
        "            module.out_features,\n",
        "            lora_rank=lora_rank,\n",
        "            alpha=alpha\n",
        "        ).to(module.weight.device, module.weight.dtype)\n",
        "\n",
        "        # Register it\n",
        "        model.add_module(f\"lora_{safe_name}\", lora_mod)\n",
        "\n",
        "        # Patch forward\n",
        "        orig_forward = module.forward\n",
        "        def custom_forward(m_self, x, orig_forward=orig_forward, lora_layer=lora_mod):\n",
        "            base_out = orig_forward(x)\n",
        "            lora_out = lora_layer(x)\n",
        "            return base_out + lora_out\n",
        "\n",
        "        module.forward = custom_forward.__get__(module, module.__class__)\n",
        "\n",
        "    return model\n",
        "\n",
        "def finetune_sequence_classification():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model_name = \"bert-base-uncased\"\n",
        "    num_labels = 2\n",
        "\n",
        "    config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "    model.to(device)\n",
        "\n",
        "    # Inject LoRA\n",
        "    patch_model_for_sequence_classification(model, lora_rank=4, alpha=1.0)\n",
        "\n",
        "    texts = [\n",
        "        \"I love this product, it is amazing!\",\n",
        "        \"This is the worst experience of my life.\",\n",
        "        \"The movie was quite entertaining.\",\n",
        "        \"Horrible service, will not come back!\"\n",
        "    ]\n",
        "    labels = [1, 0, 1, 0]\n",
        "    dataset = ToyClassificationDataset(texts, labels, tokenizer, max_length=16)\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    # Only train LoRA params\n",
        "    lora_params = []\n",
        "    for param_name, param in model.named_parameters():\n",
        "        if \"lora_\" in param_name:\n",
        "            param.requires_grad = True\n",
        "            lora_params.append(param)\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    optimizer = optim.AdamW(lora_params, lr=1e-4)\n",
        "    model.train()\n",
        "    epochs = 3\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, avg_loss={avg_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    sample_text = [\"I dislike the taste, not recommended.\"]\n",
        "    enc = tokenizer(sample_text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**enc).logits\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "    print(\"\\nInference Test:\")\n",
        "    print(f\"Input: {sample_text}\")\n",
        "    print(f\"Logits: {logits.cpu().numpy()}\")\n",
        "    print(f\"Predicted label: {preds.item()} (0=Neg,1=Pos)\")\n",
        "\n",
        "finetune_sequence_classification()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Checking GPU / driver info ===\n",
            "Thu Feb 20 18:43:30 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "=== 1) Uninstall conflicting packages (torch, transformers, peft, xformers, etc.) ===\n",
            "Found existing installation: torch 2.1.0+cu121\n",
            "Uninstalling torch-2.1.0+cu121:\n",
            "  Successfully uninstalled torch-2.1.0+cu121\n",
            "Found existing installation: transformers 4.31.0\n",
            "Uninstalling transformers-4.31.0:\n",
            "  Successfully uninstalled transformers-4.31.0\n",
            "Found existing installation: peft 0.14.0\n",
            "Uninstalling peft-0.14.0:\n",
            "  Successfully uninstalled peft-0.14.0\n",
            "Found existing installation: xformers 0.0.24\n",
            "Uninstalling xformers-0.0.24:\n",
            "  Successfully uninstalled xformers-0.0.24\n",
            "Found existing installation: tokenizers 0.13.3\n",
            "Uninstalling tokenizers-0.13.3:\n",
            "  Successfully uninstalled tokenizers-0.13.3\n",
            "Found existing installation: bitsandbytes 0.45.2\n",
            "Uninstalling bitsandbytes-0.45.2:\n",
            "  Successfully uninstalled bitsandbytes-0.45.2\n",
            "\n",
            "=== 2) Install Torch 2.1.0+cu121 & Transformers==4.31.0 ===\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.1.0+cu121\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torch-2.1.0%2Bcu121-cp311-cp311-linux_x86_64.whl (2200.6 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (2024.10.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0+cu121) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0+cu121) (1.3.0)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "torchvision 0.20.0.dev20241112+cu121 requires torch==2.6.0.dev20241112, but you have torch 2.1.0+cu121 which is incompatible.\n",
            "torchaudio 2.5.0.dev20241112+cu121 requires torch==2.6.0.dev20241112, but you have torch 2.1.0+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.1.0+cu121\n",
            "Collecting transformers==4.31.0\n",
            "  Using cached transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n",
            "  Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2025.1.31)\n",
            "Using cached transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.31.0\n",
            "\n",
            "=== 3) Running your LoRA BERT classification code ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, avg_loss=0.8157\n",
            "Epoch 2/3, avg_loss=0.7862\n",
            "Epoch 3/3, avg_loss=0.7097\n",
            "\n",
            "Inference Test:\n",
            "Input: ['I dislike the taste, not recommended.']\n",
            "Logits: [[-0.36591572  0.18960014]]\n",
            "Predicted label: 1 (0=Neg,1=Pos)\n"
          ]
        }
      ],
      "id": "TtzfPXUE4kRY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwGqZ7NL4kRY"
      },
      "source": [
        "---\n",
        "## 8) **Refactored Attention**\n",
        "\n",
        "Merging `xformers`, PyTorch’s SDPA, `flash_attn`, and a fallback “flex” approach in a single function."
      ],
      "id": "MwGqZ7NL4kRY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpUWkXr44kRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a465ec8f-49cb-4d81-c850-dcc8a064930e"
      },
      "source": [
        "import warnings\n",
        "\n",
        "try:\n",
        "    import xformers.ops as xops\n",
        "    XFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XFORMERS_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import flash_attn\n",
        "    FLASH_ATTN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    FLASH_ATTN_AVAILABLE = False\n",
        "\n",
        "SDPA_AVAILABLE = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n",
        "\n",
        "def flex_custom_attention(q, k, v, attn_mask=None):\n",
        "    d_k = q.shape[-1]\n",
        "    scores = torch.matmul(q, k.transpose(-1, -2)) / (d_k ** 0.5)\n",
        "    if attn_mask is not None:\n",
        "        scores = scores + attn_mask\n",
        "    weights = torch.softmax(scores, dim=-1)\n",
        "    weights = weights.to(v.dtype)\n",
        "    out = torch.matmul(weights, v)\n",
        "    return out\n",
        "\n",
        "def xformers_attention(q, k, v, attn_mask=None):\n",
        "    B, H, L, D = q.shape\n",
        "    q_ = q.reshape(B*H, L, D)\n",
        "    k_ = k.reshape(B*H, L, D)\n",
        "    v_ = v.reshape(B*H, L, D)\n",
        "\n",
        "    bool_mask = None\n",
        "    if attn_mask is not None:\n",
        "        expanded = attn_mask.expand(B, H, L, L).reshape(B*H, L, L)\n",
        "        bool_mask = (expanded < -1e4)\n",
        "    out = xops.memory_efficient_attention(\n",
        "        q_, k_, v_,\n",
        "        attn_mask=bool_mask,\n",
        "        p=0.0\n",
        "    )\n",
        "    return out.reshape(B, H, L, D)\n",
        "\n",
        "def flash_attention(q, k, v, attn_mask=None):\n",
        "    import flash_attn\n",
        "    B, H, L, D = q.shape\n",
        "    q_ = q.reshape(B*H, L, D)\n",
        "    k_ = k.reshape(B*H, L, D)\n",
        "    v_ = v.reshape(B*H, L, D)\n",
        "    out = flash_attn.flash_attn_func(\n",
        "        q_, k_, v_,\n",
        "        dropout_p=0.0,\n",
        "        softmax_scale=None,\n",
        "        causal=False\n",
        "    )\n",
        "    return out.reshape(B, H, L, D)\n",
        "\n",
        "def sdpa_attention(q, k, v, attn_mask=None):\n",
        "    from torch.nn.functional import scaled_dot_product_attention as sdpa\n",
        "    B, H, L, D = q.shape\n",
        "    q_ = q.permute(2, 0, 1, 3).reshape(L, B*H, D)\n",
        "    k_ = k.permute(2, 0, 1, 3).reshape(L, B*H, D)\n",
        "    v_ = v.permute(2, 0, 1, 3).reshape(L, B*H, D)\n",
        "\n",
        "    am = None\n",
        "    if attn_mask is not None:\n",
        "        am = attn_mask.expand(B, H, L, L).reshape(B*H, L, L)\n",
        "    out_ = sdpa(q_, k_, v_, attn_mask=am, dropout_p=0.0, is_causal=False)\n",
        "    out = out_.reshape(L, B, H, D).permute(1, 2, 0, 3)\n",
        "    return out\n",
        "\n",
        "def unified_attention(q, k, v, attn_mask=None, backend=\"auto\"):\n",
        "    if backend == \"auto\":\n",
        "        if XFORMERS_AVAILABLE:\n",
        "            backend = \"xformers\"\n",
        "        elif FLASH_ATTN_AVAILABLE:\n",
        "            backend = \"flash\"\n",
        "        elif SDPA_AVAILABLE:\n",
        "            backend = \"sdpa\"\n",
        "        else:\n",
        "            backend = \"flex\"\n",
        "\n",
        "    if backend == \"xformers\":\n",
        "        if not XFORMERS_AVAILABLE:\n",
        "            raise RuntimeError(\"xformers not installed!\")\n",
        "        return xformers_attention(q, k, v, attn_mask)\n",
        "    elif backend == \"flash\":\n",
        "        if not FLASH_ATTN_AVAILABLE:\n",
        "            raise RuntimeError(\"flash_attn not installed!\")\n",
        "        return flash_attention(q, k, v, attn_mask)\n",
        "    elif backend == \"sdpa\":\n",
        "        if not SDPA_AVAILABLE:\n",
        "            raise RuntimeError(\"PyTorch >=2.0 needed for SDPA!\")\n",
        "        return sdpa_attention(q, k, v, attn_mask)\n",
        "    else:\n",
        "        return flex_custom_attention(q, k, v, attn_mask)\n",
        "\n",
        "# Demo usage\n",
        "def example_unified_attention():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    B, H, L, D = 2, 4, 16, 64\n",
        "    q = torch.randn(B, H, L, D, device=device, dtype=torch.float16)\n",
        "    k = torch.randn(B, H, L, D, device=device, dtype=torch.float16)\n",
        "    v = torch.randn(B, H, L, D, device=device, dtype=torch.float16)\n",
        "    attn_mask = torch.zeros((B, 1, L, L), device=device, dtype=torch.float32)\n",
        "    blocked = torch.rand((B, 1, L, L), device=device) < 0.2\n",
        "    attn_mask[blocked] = float(\"-inf\")\n",
        "    out_flex = unified_attention(q, k, v, attn_mask, backend=\"flex\")\n",
        "    print(\"fallback =>\", out_flex.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    example_unified_attention()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fallback => torch.Size([2, 4, 16, 64])\n"
          ]
        }
      ],
      "id": "BpUWkXr44kRY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fgcv54J4kRY"
      },
      "source": [
        "---\n",
        "## Final Notes\n",
        "\n",
        "- This notebookincludes separate code snippets for each task.\n",
        "- Some cells (like the nF4 → Triton example) are skeletons or placeholders to illustrate core ideas.\n"
      ],
      "id": "6fgcv54J4kRY"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "63274182d56c49198bfc8c00d164acc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_adfd86581bab4eccbb598305910c9f21",
              "IPY_MODEL_b22141c1ccb24125b5de0f6fd2d867dd",
              "IPY_MODEL_15b89b9e25234e12a5da75da330b56e4"
            ],
            "layout": "IPY_MODEL_a40516e74d074956a0907eb1d46d4223"
          }
        },
        "adfd86581bab4eccbb598305910c9f21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e306353dee0b40f283a629d17d660fa2",
            "placeholder": "​",
            "style": "IPY_MODEL_fef5d7966c79445086881d4187f3cc34",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "b22141c1ccb24125b5de0f6fd2d867dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ccc53d34e234f27bd023731f4fcab48",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5776d1b8e0d4b0399ea3a1b3e420b5b",
            "value": 48
          }
        },
        "15b89b9e25234e12a5da75da330b56e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d971220d5be4009a57efa0224ae3c59",
            "placeholder": "​",
            "style": "IPY_MODEL_1973450a918a4e5ca1329bd633f8b18f",
            "value": " 48.0/48.0 [00:00&lt;00:00, 5.79kB/s]"
          }
        },
        "a40516e74d074956a0907eb1d46d4223": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e306353dee0b40f283a629d17d660fa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fef5d7966c79445086881d4187f3cc34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ccc53d34e234f27bd023731f4fcab48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5776d1b8e0d4b0399ea3a1b3e420b5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d971220d5be4009a57efa0224ae3c59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1973450a918a4e5ca1329bd633f8b18f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d75451d941c443e9c18e23b9bac06a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e7ce6ae8f3f43eeb83c671989d84da4",
              "IPY_MODEL_693641c8421b46289740299251c33e94",
              "IPY_MODEL_1d38c1a48f1544fba2a715f739700d33"
            ],
            "layout": "IPY_MODEL_b586c83b6bb946f48b601bbd3aa95310"
          }
        },
        "3e7ce6ae8f3f43eeb83c671989d84da4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30f71222f7fc4b24b2f9900dd48bb645",
            "placeholder": "​",
            "style": "IPY_MODEL_f46165f2dba84469812030df45a13488",
            "value": "vocab.txt: 100%"
          }
        },
        "693641c8421b46289740299251c33e94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fbc3daca5ee40eea15a549a2489c6db",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f574b814c6e4880a99974cfd43baf6e",
            "value": 231508
          }
        },
        "1d38c1a48f1544fba2a715f739700d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70adbdb7d34a4deba573a29153d6206d",
            "placeholder": "​",
            "style": "IPY_MODEL_197b2aebb24a4f6a83f6e38dfd2acffc",
            "value": " 232k/232k [00:00&lt;00:00, 1.25MB/s]"
          }
        },
        "b586c83b6bb946f48b601bbd3aa95310": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30f71222f7fc4b24b2f9900dd48bb645": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f46165f2dba84469812030df45a13488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fbc3daca5ee40eea15a549a2489c6db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f574b814c6e4880a99974cfd43baf6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70adbdb7d34a4deba573a29153d6206d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "197b2aebb24a4f6a83f6e38dfd2acffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78c889353f8c4bae8e4d0ab87ae8390e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7cdbea7c6b794ea19686bd11ac2f4871",
              "IPY_MODEL_050edd4bdda14001843b1bd9d2c3544b",
              "IPY_MODEL_246925d0612549929d9144418ffe5a46"
            ],
            "layout": "IPY_MODEL_ac082c7fa44d48939dfeea0253a2e77e"
          }
        },
        "7cdbea7c6b794ea19686bd11ac2f4871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e22009a3157a42ba826948d905be3657",
            "placeholder": "​",
            "style": "IPY_MODEL_d4c0501dc2b845b3a4e9f6c92522a836",
            "value": "tokenizer.json: 100%"
          }
        },
        "050edd4bdda14001843b1bd9d2c3544b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce51fe8901d040a5941fad2e83eb79a5",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ce162a63a4c42e09d678077d8e8152d",
            "value": 466062
          }
        },
        "246925d0612549929d9144418ffe5a46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90c8905dfb094718addf5fef9fd5720f",
            "placeholder": "​",
            "style": "IPY_MODEL_351a570a98d44f6386422555ffd70337",
            "value": " 466k/466k [00:00&lt;00:00, 5.24MB/s]"
          }
        },
        "ac082c7fa44d48939dfeea0253a2e77e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e22009a3157a42ba826948d905be3657": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4c0501dc2b845b3a4e9f6c92522a836": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce51fe8901d040a5941fad2e83eb79a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ce162a63a4c42e09d678077d8e8152d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90c8905dfb094718addf5fef9fd5720f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "351a570a98d44f6386422555ffd70337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}