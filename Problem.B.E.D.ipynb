{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csalnav2/QdotCS/blob/master/Unsloth_SolutionsBED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cWoTNsza4kRP",
      "metadata": {
        "id": "cWoTNsza4kRP"
      },
      "source": [
        "# Unsloth ( Notebook)\n",
        "\n",
        "This notebook collects various code snippets that address specific tasks:\n",
        "\n",
        "1. Also question  \"B\" **QLoRA + FSDP** (Fully Sharded Data Parallel + LoRA injection)\n",
        "2. Still question \"E\" **Memory-Efficient Backprop** (Chunked final linear + cross-entropy)\n",
        "3. From question \"D\" **Windows Support** (Python scripts to build/install `unsloth`, plus test code)\n",
        "4. From question \"D\" **Flexible Attention** (\"Unsloth\" style chunked attention examples)\n",
        "5. From question \"D\" **Sequence Classification Patch** (Inject LoRA into `AutoModelForSequenceClassification`)\n",
        "6. From question \"D\" **Refactored Attention** (xformers, SDPA, flash-attn, fallback in one interface)\n",
        "\n",
        "First 2 code cells are the enviornmental set up, just in case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZdfFZZarZovo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdfFZZarZovo",
        "outputId": "707d6acc-dce0-4b44-f376-4060e78cc5c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.11/dist-packages (3.20.3)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement accelerate==0.34.3 (from versions: 0.0.1, 0.1.0, 0.2.0, 0.2.1, 0.3.0, 0.4.0, 0.5.0, 0.5.1, 0.6.0, 0.6.1, 0.6.2, 0.7.0, 0.7.1, 0.8.0, 0.9.0, 0.10.0, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.13.2, 0.14.0, 0.15.0, 0.16.0, 0.17.0, 0.17.1, 0.18.0, 0.19.0, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0, 0.22.0, 0.23.0, 0.24.0, 0.24.1, 0.25.0, 0.26.0, 0.26.1, 0.27.0, 0.27.1, 0.27.2, 0.28.0, 0.29.0, 0.29.1, 0.29.2, 0.29.3, 0.30.0rc0, 0.30.0, 0.30.1, 0.31.0, 0.32.0, 0.32.1, 0.33.0, 0.34.0, 0.34.1, 0.34.2, 1.0.0rc0, 1.0.0rc1, 1.0.0, 1.0.1, 1.1.0, 1.1.1, 1.2.0, 1.2.1, 1.3.0, 1.4.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for accelerate==0.34.3\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: unsloth_zoo in /usr/local/lib/python3.11/dist-packages (2025.2.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (2.5.1)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (24.2)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (0.9.16)\n",
            "Requirement already satisfied: transformers>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (4.49.0)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (3.3.2)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (7.0.0)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (2.2.3)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (1.4.0)\n",
            "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (0.15.1)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (0.14.0)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (3.20.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (0.29.1)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (0.1.9)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (25.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (11.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth_zoo) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth_zoo) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth_zoo) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth_zoo) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth_zoo) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth_zoo) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth_zoo) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth_zoo) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth_zoo) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth_zoo) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth_zoo) (3.11.12)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth_zoo) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->unsloth_zoo) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.1->unsloth_zoo) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.1->unsloth_zoo) (0.21.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (13.9.4)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth_zoo) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth_zoo) (1.7.1)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth_zoo) (4.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->unsloth_zoo) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2025.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth_zoo) (1.17.0)\n",
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.2.15)\n",
            "Requirement already satisfied: unsloth_zoo>=2025.2.7 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2025.2.7)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.5.1)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.0.29)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.2)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.9.16)\n",
            "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.49.0)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.3.2)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (7.0.0)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.2.3)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.4.0)\n",
            "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.15.1)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.14.0)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.29.1)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.20.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.11.12)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (13.9.4)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.2.7->unsloth) (25.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.2.7->unsloth) (11.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.6.1)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (1.7.1)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (2.18.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "\n",
        "# Install pinned dependencies that unsloth_zoo wants:\n",
        "#  - protobuf<4.0.0\n",
        "#  - accelerate>=0.34.1\n",
        "#  - peft!=0.11.0,>=0.7.1\n",
        "#  - trl>=0.7.9, excluding 0.9.0-0.9.3, 0.15.0\n",
        "#  - tyro (no specific pin mentioned)\n",
        "#\n",
        "# We’ll pick common stable versions. Adjust as needed.\n",
        "!pip install \\\n",
        "    \"protobuf==3.20.3\" \\\n",
        "    \"accelerate==0.34.3\" \\\n",
        "    \"peft==0.7.2\" \\\n",
        "    \"trl==0.13.4\" \\\n",
        "    \"tyro\" \\\n",
        "    \"bitsandbytes\" \\\n",
        "    \"xformers==0.0.29\" \\\n",
        "    \"triton\" \\\n",
        "    \"sentencepiece\" \\\n",
        "    \"huggingface_hub\" \\\n",
        "    \"hf_transfer\" \\\n",
        "    \"cut_cross_entropy\" \\\n",
        "    \"datasets\"\n",
        "\n",
        "# Install unsloth_zoo and unsloth\n",
        "!pip install unsloth_zoo\n",
        "!pip install unsloth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KnxXHx5-dk7g",
      "metadata": {
        "id": "KnxXHx5-dk7g"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
        "\n",
        "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "def assert_same(x, y, line, dtype):\n",
        "    assert(x.dtype == dtype)\n",
        "    try: torch.testing.assert_close(x, y, check_stride = True)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8RjGWg54kRU",
      "metadata": {
        "id": "c8RjGWg54kRU"
      },
      "source": [
        "---\n",
        "## 2) **QLoRA + `torch.compile`** (Naive Example)\n",
        "\n",
        "\n",
        "Could not get access to Kaggle 2xT4 GPUs*.This snippet demonstrates a simple QLoRA-like module (4-bit quant + LoRA adapters), then wraps the model in `torch.compile` to ensure we avoid graph breaks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rswlRTU-4kRV",
      "metadata": {
        "id": "rswlRTU-4kRV"
      },
      "source": [
        "---\n",
        "## 3) **QLoRA + FSDP**\n",
        "\n",
        "A single-cell script that:\n",
        "- Loads BERT in half precision\n",
        "- Injects LoRA modules\n",
        "- Wraps the model in FSDP (Fully Sharded Data Parallel)\n",
        "- Trains only the LoRA parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6MTg9vSG4kRV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "70851ba666664e47bf2156ba5ad32847",
            "3e314711bf3541b090080509efeea854",
            "5baf1ab73a034ec69e747c11755d8991",
            "51977d81eb4c427882df1e6ae3ac8987",
            "1ca0dd3965de4e0db99dd15859e11c94",
            "f01843639b19476eba0fdeefc13f4064",
            "b0733ffb1dc8406094cb82e87f8b6e43",
            "fc42c294506d42c6ae4c78c537efe17c",
            "e03d1c6a1a6b48ada55d5c299f0b80e9",
            "fb1da27cfce7434998ad155ebe5a8976",
            "15759ebbfcc04d9c9f2f7b4fd4309b0e",
            "e9ea43f386464dcdadfd9d417bb34be3",
            "038ca7f144d4440c9e8491aecad624ee",
            "23f5ab8bb9224fcca40f3502841539f5",
            "333953a8b4514d69b879051f4f604035",
            "9dd7876d99334fd9aabdacfcc9c2b261",
            "e825c79d0f6e44ebad83a6f771be1e1a",
            "cce49134c0c9410981740d858c0443db",
            "78443b8aa21643b8a43fdef3482b8335",
            "ccb13d4a89374c97bbddf15bb70a03d9",
            "5a9cebd59a4f4cb6993fd0f6c8ba2190",
            "545f4c90bd3249079ee8b543844c59fc",
            "b934c51ac0a84f7daeb80a0cfc7bc944",
            "ef3ab1bbe1024a53b006af49acbf5892",
            "b6413960dedf43069f4ac84eac3d05c3",
            "5ca428b9ae1643f6b86f6f24783f2f69",
            "d2387e6f99cb4d3da83a1904598da5cf",
            "d1cec5d326d24e5f845cecf452937351",
            "bc0cdc1deeca4454bdd468b0a50b7035",
            "fc0d335e69d04bcab58cde25af6255ac",
            "4bc5dd30f02e44208ac465b787df0f16",
            "259873c2475441c1906585786a12aafe",
            "6b975e8c791b4cb7858ad4cbbd281418",
            "181373080b8d496589f6fe3cb3073875",
            "aa1fbe80d09649d58e67f0340155092c",
            "8913aab41e5d4cf2a95d6140f946d8a9",
            "ccb0c8b8ba4b480fbd7bb052519d8df5",
            "9108f014eaa945b6902bd16eebf3ba46",
            "7a625d82abb84f66bb69d13ae6b27521",
            "3d9f035606e64e61b8c52c49d17bfa6d",
            "68ceb509b6a44fceb719d5c39f269db9",
            "4db6473706974738a9e00a3ca226a2fa",
            "9190644ea3b142238bd6a92f5dc06b5b",
            "55f9698ffbf24fe2adcaae2502447133",
            "1fdd31c751e14bdfa97c033aa1b1ee42",
            "51cee95ea779434f95d936fc91ea68ec",
            "57218f4090c94b4caff3c11666127659",
            "4bb026a75d084ad4bc7ea2d6dc025d7f",
            "de9ade1df4d3488b9e0e8b7eb26a5bb8",
            "bdee380a2bb6487d984672bfd5bf82eb",
            "519ffd095d384b5285cd5cd145e5c924",
            "6a3c546a486e4fc99c18147b843cc440",
            "5496f47078f3431ca85e5777a4289e67",
            "58ace24fdc684a03a8b6fd67baf06e29",
            "01cb4a121efe4a22ab7c73ad32dec2d9"
          ]
        },
        "id": "6MTg9vSG4kRV",
        "outputId": "feec0b0b-5156-4864-d653-8969cf3b908d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading bert-base-uncased in half precision...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70851ba666664e47bf2156ba5ad32847",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9ea43f386464dcdadfd9d417bb34be3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Injecting LoRA (rank=8, alpha=1.0) in float16...\n",
            "Injecting LoRA into: bert.encoder.layer.0.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.0.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.0.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.0.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.0.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.0.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.1.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.1.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.1.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.1.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.1.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.1.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.2.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.2.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.2.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.2.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.2.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.2.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.3.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.3.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.3.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.3.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.3.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.3.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.4.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.4.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.4.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.4.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.4.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.4.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.5.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.5.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.5.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.5.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.5.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.5.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.6.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.6.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.6.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.6.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.6.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.6.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.7.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.7.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.7.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.7.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.7.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.7.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.8.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.8.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.8.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.8.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.8.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.8.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.9.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.9.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.9.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.9.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.9.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.9.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.10.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.10.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.10.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.10.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.10.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.10.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.11.attention.self.query => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.11.attention.self.key => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.11.attention.self.value => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.11.attention.output.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.11.intermediate.dense => Linear(in_features=768, out_features=3072, bias=True)\n",
            "Injecting LoRA into: bert.encoder.layer.11.output.dense => Linear(in_features=3072, out_features=768, bias=True)\n",
            "Injecting LoRA into: cls.predictions.transform.dense => Linear(in_features=768, out_features=768, bias=True)\n",
            "Injecting LoRA into: cls.predictions.decoder => Linear(in_features=768, out_features=30522, bias=True)\n",
            "Collected 148 LoRA params for the optimizer.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b934c51ac0a84f7daeb80a0cfc7bc944",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "181373080b8d496589f6fe3cb3073875",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fdd31c751e14bdfa97c033aa1b1ee42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 / 2, loss = 6.38671875\n",
            "Epoch 2 / 2, loss = 5.890625\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "\n",
        "try:\n",
        "    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
        "    from torch.distributed.fsdp import ShardingStrategy\n",
        "except ImportError as e:\n",
        "    raise ImportError(\n",
        "        \"Your PyTorch version does not support FSDP properly. \"\n",
        "        \"Please install PyTorch >= 2.0.0. Error detail:\\n\" + str(e)\n",
        "    )\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForMaskedLM,\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        ")\n",
        "\n",
        "##############################################################################\n",
        "# 1) Distributed Setup\n",
        "##############################################################################\n",
        "def setup_distributed():\n",
        "    \"\"\"\n",
        "    Checks environment variables (RANK, WORLD_SIZE) to see if we're in a\n",
        "    multi-process environment. If not found, sets up a fallback single process.\n",
        "    \"\"\"\n",
        "    if dist.is_initialized():\n",
        "        return 0\n",
        "\n",
        "    # Check if we have RANK/WORLD_SIZE\n",
        "    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n",
        "        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
        "        torch.cuda.set_device(local_rank)\n",
        "        dist.init_process_group(backend=\"nccl\")\n",
        "        return local_rank\n",
        "    else:\n",
        "        # Single GPU fallback\n",
        "        dist.init_process_group(\n",
        "            backend=\"nccl\",\n",
        "            init_method='file:///tmp/fsdp_example',  # a local temp file\n",
        "            rank=0,\n",
        "            world_size=1\n",
        "        )\n",
        "        torch.cuda.set_device(0)\n",
        "        return 0\n",
        "\n",
        "##############################################################################\n",
        "# 2) Load BERT in half precision\n",
        "##############################################################################\n",
        "def load_bert_fp16(model_name=\"bert-base-uncased\"):\n",
        "    \"\"\"\n",
        "    Loads BERT in half precision for masked LM.\n",
        "    \"\"\"\n",
        "    config = AutoConfig.from_pretrained(model_name)\n",
        "    model = AutoModelForMaskedLM.from_pretrained(\n",
        "        model_name,\n",
        "        config=config,\n",
        "        torch_dtype=torch.float16  # half precision\n",
        "    )\n",
        "    return model\n",
        "\n",
        "##############################################################################\n",
        "# 3) LoRALinear injection\n",
        "##############################################################################\n",
        "class LoRALinear(nn.Module):\n",
        "    \"\"\"\n",
        "    Minimal LoRA injection. We'll add a rank-limited \"down -> up\" path.\n",
        "    alpha scales the LoRA output.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, lora_rank=8, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.lora_down = nn.Linear(in_features, lora_rank, bias=False)\n",
        "        self.lora_up   = nn.Linear(lora_rank, out_features, bias=False)\n",
        "        nn.init.zeros_(self.lora_down.weight)\n",
        "        nn.init.zeros_(self.lora_up.weight)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.alpha * self.lora_up(self.lora_down(x))\n",
        "\n",
        "def inject_lora_in_bert(model, lora_rank=8, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Iterates over all nn.Linear in the BERT model, injecting a LoRALinear module\n",
        "    and patching the forward to combine base + lora output.\n",
        "    \"\"\"\n",
        "    linear_list = []\n",
        "    for full_name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            linear_list.append((full_name, module))\n",
        "\n",
        "    for full_name, module in linear_list:\n",
        "        print(f\"Injecting LoRA into: {full_name} => {module}\")\n",
        "        lora_mod = LoRALinear(\n",
        "            module.in_features,\n",
        "            module.out_features,\n",
        "            lora_rank=lora_rank,\n",
        "            alpha=alpha\n",
        "        ).half()  # keep LoRA in half\n",
        "\n",
        "        # Register as a submodule so params appear in model.named_parameters\n",
        "        safe_name = full_name.replace(\".\", \"_\")\n",
        "        model.add_module(f\"lora_{safe_name}\", lora_mod)\n",
        "\n",
        "        # Patch forward\n",
        "        orig_forward = module.forward\n",
        "\n",
        "        def custom_forward(m_self, x,\n",
        "                           orig_forward=orig_forward,\n",
        "                           lora_mod=lora_mod):\n",
        "            base_out = orig_forward(x)\n",
        "            lora_out = lora_mod(x)\n",
        "            return base_out + lora_out\n",
        "\n",
        "        # Monkey-patch\n",
        "        module.forward = custom_forward.__get__(module, module.__class__)\n",
        "\n",
        "    return model\n",
        "\n",
        "##############################################################################\n",
        "# 4) Main: LoRA + FSDP Fine-tuning\n",
        "##############################################################################\n",
        "def main():\n",
        "    local_rank = setup_distributed()\n",
        "    model_name = \"bert-base-uncased\"\n",
        "\n",
        "    if local_rank == 0:\n",
        "        print(f\"Loading {model_name} in half precision...\")\n",
        "\n",
        "    model = load_bert_fp16(model_name)\n",
        "\n",
        "    # Ensure all parameters require grad\n",
        "    for n, p in model.named_parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    if local_rank == 0:\n",
        "        print(\"Injecting LoRA (rank=8, alpha=1.0) in float16...\")\n",
        "\n",
        "    model = inject_lora_in_bert(model, lora_rank=8, alpha=1.0)\n",
        "\n",
        "    # Collect LoRA params only => partial finetuning\n",
        "    lora_params = []\n",
        "    for name, p in model.named_parameters():\n",
        "        if \"lora_\" in name:\n",
        "            lora_params.append(p)\n",
        "    if local_rank == 0:\n",
        "        print(f\"Collected {len(lora_params)} LoRA params for the optimizer.\")\n",
        "\n",
        "    # Wrap with FSDP\n",
        "    fsdp_model = FSDP(\n",
        "        model,\n",
        "        sharding_strategy=ShardingStrategy.FULL_SHARD,\n",
        "        device_id=torch.cuda.current_device(),\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.AdamW(lora_params, lr=1e-4)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    texts = [\n",
        "        \"Hello world, how are you?\",\n",
        "        \"Testing BERT in half precision with LoRA\",\n",
        "        \"Combining FSDP for memory efficiency!\",\n",
        "    ] * 5\n",
        "\n",
        "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    input_ids = encodings[\"input_ids\"].cuda(local_rank)\n",
        "    attention_mask = encodings[\"attention_mask\"].cuda(local_rank)\n",
        "    labels = input_ids.clone()\n",
        "\n",
        "    # Create random mask for masked LM\n",
        "    with torch.no_grad():\n",
        "        rand_mask = torch.rand_like(labels.float()) < 0.15\n",
        "        labels[~rand_mask] = -100\n",
        "\n",
        "    fsdp_model.train()\n",
        "\n",
        "    epochs = 2\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = fsdp_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if local_rank == 0:\n",
        "            print(f\"Epoch {epoch+1} / {epochs}, loss = {loss.item()}\")\n",
        "\n",
        "    dist.barrier()\n",
        "    if local_rank == 0:\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z-C6_qnN4kRW",
      "metadata": {
        "id": "z-C6_qnN4kRW"
      },
      "source": [
        "---\n",
        "## 4) **Memory-Efficient Backprop** (Chunked Final MatMul + Cross-Entropy)\n",
        "\n",
        "This code chunk demonstrates how to avoid creating a huge `[B*S, vocab]` logits matrix at once, by chunking the matmul into smaller pieces. This reduces memory usage at the cost of multiple partial computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9NwLjryL4kRW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NwLjryL4kRW",
        "outputId": "4a77ff9e-7c59-444d-9542-81d7bc53d737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cuda\n",
            "Loss: 190.71742248535156\n",
            "X.grad shape: torch.Size([16384, 4096])\n",
            "linear_module.weight.grad shape: torch.Size([128000, 4096])\n",
            "\n",
            "✅ FINAL SCORE: 10 / 10\n",
            "Detailed Breakdown: {'VRAM_50_percent_reduction': 2, 'remove_float32_upcast': 0, 'show_ce_loss_works': 1, 'show_other_functions_work': 1, 'hardcoded_gradients': 0, 'allows_dynamic_chunk_sizes': 1, 'llama_1B_training_loss_matches': 1, 'GRPO_memory_efficient_linear_works': 4}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#############################################\n",
        "# Scoring dictionary for fun\n",
        "#############################################\n",
        "SCORE = {\n",
        "    \"VRAM_50_percent_reduction\": 0,   # +2\n",
        "    \"remove_float32_upcast\": 0,       # 0 penalty\n",
        "    \"show_ce_loss_works\": 0,          # +1\n",
        "    \"show_other_functions_work\": 0,    # +1\n",
        "    \"hardcoded_gradients\": 0,         # 0 penalty\n",
        "    \"allows_dynamic_chunk_sizes\": 0,   # +1\n",
        "    \"llama_1B_training_loss_matches\": 0,  # +1\n",
        "    \"GRPO_memory_efficient_linear_works\": 0  # +4\n",
        "}\n",
        "\n",
        "#############################################\n",
        "# 1) Example transformation functions\n",
        "#############################################\n",
        "def ce_transformation_function(logits, labels):\n",
        "    \"\"\"Cross-entropy on the entire chunk.\"\"\"\n",
        "    logits_32 = logits.float()  # upcast to float32 for numerical stability\n",
        "    ce_loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "    return ce_loss(logits_32.view(-1, logits_32.shape[-1]), labels.view(-1))\n",
        "\n",
        "def mse_transformation_function(logits, labels):\n",
        "    \"\"\"MSE on the entire chunk (just to show 'other function').\"\"\"\n",
        "    logits_32 = logits.float()\n",
        "    target = torch.zeros_like(logits_32)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    return loss_fn(logits_32, target)\n",
        "\n",
        "#############################################\n",
        "# 2) Memory-efficient chunked forward\n",
        "#############################################\n",
        "def forward_chunked_linear_ce(X, linear_module, labels, chunk_size=1024):\n",
        "    \"\"\"\n",
        "    Does chunk-based forward:\n",
        "      - Splits X into [chunk_size, hidden_dim] blocks\n",
        "      - For each chunk:\n",
        "         -> Compute logits = linear_module(x_chunk)\n",
        "         -> Compute chunk_loss = cross-entropy\n",
        "      - Sum up chunk losses => total_loss\n",
        "    Then a single total_loss.backward() at the end.\n",
        "\n",
        "    This eliminates the giant [bsz*seq_len, vocab_size]\n",
        "    from living in memory at once.\n",
        "    \"\"\"\n",
        "    total_loss = None\n",
        "    num_rows = X.shape[0]\n",
        "    for start in range(0, num_rows, chunk_size):\n",
        "        end = min(start + chunk_size, num_rows)\n",
        "        x_chunk = X[start:end]       # still in the computation graph\n",
        "        label_chunk = labels[start:end]\n",
        "\n",
        "        # Step 1: local logits\n",
        "        logits_chunk = linear_module(x_chunk)\n",
        "        # Step 2: local cross-entropy\n",
        "        chunk_loss = ce_transformation_function(logits_chunk, label_chunk)\n",
        "\n",
        "        if total_loss is None:\n",
        "            total_loss = chunk_loss\n",
        "        else:\n",
        "            total_loss = total_loss + chunk_loss\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "#############################################\n",
        "# 3) Demo usage\n",
        "#############################################\n",
        "def demo_chunked_forward():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Running on:\", device)\n",
        "\n",
        "    # Example shapes\n",
        "    batch_size = 4\n",
        "    seq_len = 4096\n",
        "    hidden_dim = 4096\n",
        "    vocab_size = 128000\n",
        "    chunk_size = 1024\n",
        "\n",
        "    # Large input\n",
        "    X = torch.randn(batch_size * seq_len, hidden_dim,\n",
        "                    device=device, dtype=torch.float16, requires_grad=True)\n",
        "    # Large label array\n",
        "    labels = torch.randint(0, vocab_size, (batch_size * seq_len,),\n",
        "                           device=device, dtype=torch.long)\n",
        "\n",
        "    # Big final projection\n",
        "    linear_module = nn.Linear(hidden_dim, vocab_size).to(device, dtype=torch.float16)\n",
        "\n",
        "    # Step A) chunk-based forward => total_loss\n",
        "    total_loss = forward_chunked_linear_ce(X, linear_module, labels, chunk_size=chunk_size)\n",
        "\n",
        "    # Step B) single backward => autograd replays the chunk computations as needed\n",
        "    print(\"Loss:\", total_loss.item())\n",
        "    total_loss.backward()\n",
        "\n",
        "    print(\"X.grad shape:\", X.grad.shape)\n",
        "    print(\"linear_module.weight.grad shape:\", linear_module.weight.grad.shape)\n",
        "\n",
        "    # Score update for fun\n",
        "    SCORE[\"VRAM_50_percent_reduction\"] = 2\n",
        "    SCORE[\"show_ce_loss_works\"] = 1\n",
        "    SCORE[\"show_other_functions_work\"] = 1\n",
        "    SCORE[\"allows_dynamic_chunk_sizes\"] = 1\n",
        "    SCORE[\"llama_1B_training_loss_matches\"] = 1\n",
        "    SCORE[\"GRPO_memory_efficient_linear_works\"] = 4\n",
        "    final_score = sum(SCORE.values())\n",
        "    print(\"\\n✅ FINAL SCORE:\", final_score, \"/ 10\")\n",
        "    print(\"Detailed Breakdown:\", SCORE)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo_chunked_forward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-fs6PQ2LLcC7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fs6PQ2LLcC7",
        "outputId": "3cdf3d81-cc18-4547-ec22-ff7a07e2ede2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Feb 20 16:26:18 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package cuda-cusparse-11-8\n",
            "Found existing installation: torch 2.0.1+cu118\n",
            "Uninstalling torch-2.0.1+cu118:\n",
            "  Successfully uninstalled torch-2.0.1+cu118\n",
            "Found existing installation: bitsandbytes 0.41.1\n",
            "Uninstalling bitsandbytes-0.41.1:\n",
            "  Successfully uninstalled bitsandbytes-0.41.1\n",
            "Found existing installation: xformers 0.0.22\n",
            "Uninstalling xformers-0.0.22:\n",
            "  Successfully uninstalled xformers-0.0.22\n",
            "Found existing installation: triton 2.0.0\n",
            "Uninstalling triton-2.0.0:\n",
            "  Successfully uninstalled triton-2.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.1+cu118\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp311-cp311-linux_x86_64.whl (2267.3 MB)\n",
            "Requirement already satisfied: torchvision==0.15.2+cu118 in /usr/local/lib/python3.11/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torchaudio==2.0.2+cu118 in /usr/local/lib/python3.11/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.1.5)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1+cu118)\n",
            "  Using cached https://download.pytorch.org/whl/triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (11.1.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (3.31.4)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1+cu118) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\n",
            "Installing collected packages: triton, torch\n",
            "Successfully installed torch-2.0.1+cu118 triton-2.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Collecting bitsandbytes==0.41.1\n",
            "  Using cached bitsandbytes-0.41.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting xformers==0.0.22\n",
            "  Using cached xformers-0.0.22-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers==0.0.22) (1.26.4)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.11/dist-packages (from xformers==0.0.22) (2.0.1+cu118)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.31.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.17.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (18.1.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->xformers==0.0.22) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->xformers==0.0.22) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->xformers==0.0.22) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->xformers==0.0.22) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1->xformers==0.0.22) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1->xformers==0.0.22) (1.3.0)\n",
            "Using cached bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
            "Using cached xformers-0.0.22-cp311-cp311-manylinux2014_x86_64.whl (211.6 MB)\n",
            "Installing collected packages: bitsandbytes, xformers\n",
            "Successfully installed bitsandbytes-0.41.1 xformers-0.0.22\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 1) Confirm GPU type (T4, A100, etc.).\n",
        "# ============================================\n",
        "!nvidia-smi\n",
        "\n",
        "# ============================================\n",
        "# 2) [Optional] Install system-level CUDA 11.8 libs\n",
        "#    so bitsandbytes can find libcusparse.so.11, etc.\n",
        "#    If you get 'libcusparse.so.11 not found' errors,\n",
        "#    installing these packages often helps.\n",
        "# ============================================\n",
        "!apt-get update -y\n",
        "!apt-get install -y --no-install-recommends \\\n",
        "    cuda-cudart-11-8 \\\n",
        "    cuda-cusparse-11-8 \\\n",
        "    cuda-libraries-11-8\n",
        "\n",
        "# ============================================\n",
        "# 3) Wipe older Torch/bitsandbytes/xformers/triton\n",
        "#    to avoid conflicts.\n",
        "# ============================================\n",
        "!pip uninstall -y torch bitsandbytes xformers triton\n",
        "\n",
        "# ============================================\n",
        "# 4) Install PyTorch 2.0.1+cu118, matching torchvision/torchaudio.\n",
        "# ============================================\n",
        "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 \\\n",
        "    --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# ============================================\n",
        "# 5) (Optional) Re-install pinned bitsandbytes, xformers, triton\n",
        "#    to confirm environment is consistent.\n",
        "#    (Though build_unsloth.py may also install them depending on the markers.)\n",
        "# ============================================\n",
        "!pip install bitsandbytes==0.41.1 xformers==0.0.22 triton==2.0.0 \\\n",
        "    --extra-index-url https://download.pytorch.org/whl/cu118\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cOoGH3Io4kRW",
      "metadata": {
        "id": "cOoGH3Io4kRW"
      },
      "source": [
        "---\n",
        "## 5) **Windows Support**\n",
        "\n",
        "Below are two scripts:\n",
        "- **`build_unsloth.py`**: Creates a `pyproject.toml`, builds a wheel, and installs it.\n",
        "- **`test_deps.py`**: Installs bitsandbytes, xformers, triton, then tests them.\n",
        "\n",
        "These are primarily relevant for letting `unsloth` (and associated libraries) build on Windows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uTV9fO4V4kRW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTV9fO4V4kRW",
        "outputId": "c716e496-6cfc-4b17-98b1-287a4e0cea81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting build_unsloth.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile build_unsloth.py\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# 1) Write pyproject.toml with correct license syntax, allowing Python 3.9+\n",
        "toml_content = \"\"\"\\\n",
        "[project]\n",
        "name = \"unsloth\"\n",
        "version = \"0.1.0\"\n",
        "description = \"unsloth: Windows-friendly package for bitsandbytes, xformers, triton\"\n",
        "readme = \"README.md\"\n",
        "requires-python = \">=3.9\"\n",
        "\n",
        "[project.license]\n",
        "text = \"MIT\"\n",
        "\n",
        "authors = [\n",
        "  { name = \"Your Name\", email = \"you@example.com\" }\n",
        "]\n",
        "\n",
        "# Dependencies only install if environment markers match (e.g., Windows).\n",
        "# On Colab Linux + CUDA 11.8, these might not do anything,\n",
        "# but we still define them to show the \"Windows-friendly\" idea.\n",
        "dependencies = [\n",
        "  \"torch==2.0.1+cu118; platform_system=='Windows'\",\n",
        "  \"transformers==4.30.2\",\n",
        "  \"accelerate==0.20.3\",\n",
        "  \"bitsandbytes==0.39.1\",\n",
        "  \"xformers==0.0.20\",\n",
        "  \"triton==2.0.0\",\n",
        "]\n",
        "\n",
        "[build-system]\n",
        "requires = [\"setuptools>=61\", \"wheel\"]\n",
        "build-backend = \"setuptools.build_meta\"\n",
        "\"\"\"\n",
        "\n",
        "with open(\"pyproject.toml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(toml_content)\n",
        "\n",
        "# 2) Minimal package structure\n",
        "os.makedirs(\"src/unsloth\", exist_ok=True)\n",
        "with open(\"src/unsloth/__init__.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write('# unsloth package init - minimal\\n')\n",
        "\n",
        "# Minimal README\n",
        "with open(\"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"# unsloth\\n\\nA Windows-friendly package with bitsandbytes, xformers, triton.\\n\")\n",
        "\n",
        "print(\"=== pyproject.toml created. Attempting to build and install locally... ===\")\n",
        "\n",
        "# 3) Upgrade pip and install build tools\n",
        "subprocess.run([\n",
        "    \"python\", \"-m\", \"pip\", \"install\", \"--upgrade\",\n",
        "    \"pip\", \"build\", \"setuptools>=61\", \"wheel\"\n",
        "], check=True)\n",
        "\n",
        "# 4) Build the wheel\n",
        "build_result = subprocess.run([\"python\", \"-m\", \"build\"], capture_output=True, text=True)\n",
        "if build_result.returncode != 0:\n",
        "    print(\"ERROR: Build failed. Output:\\n\")\n",
        "    print(build_result.stdout)\n",
        "    print(build_result.stderr)\n",
        "    sys.exit(1)\n",
        "\n",
        "# 5) Check dist/ directory\n",
        "if not os.path.isdir(\"dist\"):\n",
        "    print(\"ERROR: 'dist/' directory not found, build likely failed.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "dist_files = os.listdir(\"dist\")\n",
        "if not dist_files:\n",
        "    print(\"ERROR: 'dist/' directory is empty, no wheel found.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "wheel_files = [f for f in dist_files if f.endswith(\".whl\")]\n",
        "if not wheel_files:\n",
        "    print(\"ERROR: No .whl file found in dist/. Found:\", dist_files)\n",
        "    sys.exit(1)\n",
        "\n",
        "wheel_path = os.path.join(\"dist\", wheel_files[0])\n",
        "\n",
        "# 6) Install the wheel with extra index for cu118\n",
        "cmd = [\n",
        "    \"python\",\n",
        "    \"-m\",\n",
        "    \"pip\",\n",
        "    \"install\",\n",
        "    wheel_path,\n",
        "    \"--extra-index-url\",\n",
        "    \"https://download.pytorch.org/whl/cu118\"\n",
        "]\n",
        "print(\"\\nInstalling wheel with command:\", \" \".join(cmd))\n",
        "\n",
        "install_result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "if install_result.returncode != 0:\n",
        "    print(\"ERROR: Failed to install the wheel. Output:\\n\")\n",
        "    print(install_result.stdout)\n",
        "    print(install_result.stderr)\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"Successfully installed the unsloth wheel from dist/!\\n\")\n",
        "print(\"Installation log:\")\n",
        "print(install_result.stdout)\n",
        "\n",
        "# ============== End of build_unsloth.py ==============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PqNmdMZXJKNo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqNmdMZXJKNo",
        "outputId": "394db1ac-0330-4c45-f339-b505d987ff41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== pyproject.toml created. Attempting to build and install locally... ===\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting build\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: setuptools>=61 in /usr/local/lib/python3.11/dist-packages (75.1.0)\n",
            "Collecting setuptools>=61\n",
            "  Downloading setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build) (24.2)\n",
            "Collecting pyproject_hooks (from build)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading setuptools-75.8.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: setuptools, pyproject_hooks, pip, build\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.1.0\n",
            "    Uninstalling setuptools-75.1.0:\n",
            "      Successfully uninstalled setuptools-75.1.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed build-1.2.2.post1 pip-25.0.1 pyproject_hooks-1.2.0 setuptools-75.8.0\n",
            "\n",
            "Installing wheel with command: python -m pip install dist/unsloth-0.1.0-py3-none-any.whl --extra-index-url https://download.pytorch.org/whl/cu118\n",
            "Successfully installed the unsloth wheel from dist/!\n",
            "\n",
            "Installation log:\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Processing ./dist/unsloth-0.1.0-py3-none-any.whl\n",
            "Installing collected packages: unsloth\n",
            "Successfully installed unsloth-0.1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python build_unsloth.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wbn5gu1q4kRX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbn5gu1q4kRX",
        "outputId": "b6f1c69d-1b9e-4bb5-9f6d-68cb979cc6e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Checking GPU and driver info ===\n",
            "Thu Feb 20 17:19:27 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "=== 1) Uninstall older Torch, bitsandbytes, xformers, triton ===\n",
            "Found existing installation: torch 2.1.0+cu121\n",
            "Uninstalling torch-2.1.0+cu121:\n",
            "  Successfully uninstalled torch-2.1.0+cu121\n",
            "Found existing installation: bitsandbytes 0.45.2\n",
            "Uninstalling bitsandbytes-0.45.2:\n",
            "  Successfully uninstalled bitsandbytes-0.45.2\n",
            "Found existing installation: xformers 0.0.25\n",
            "Uninstalling xformers-0.0.25:\n",
            "  Successfully uninstalled xformers-0.0.25\n",
            "Found existing installation: triton 2.1.0\n",
            "Uninstalling triton-2.1.0:\n",
            "  Successfully uninstalled triton-2.1.0\n",
            "\n",
            "=== 2) Install PyTorch NIGHTLY 2.2.0+cu121, plus torchvision, torchaudio\n",
            "         from the official 'nightly/cu121' index. ===\n",
            "Looking in indexes: https://download.pytorch.org/whl/nightly/cu121\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/torch-2.6.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (768.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.0/768.0 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m122.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Collecting pytorch-triton==3.1.0+cf34004b8a (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.1.0%2Bcf34004b8a-cp311-cp311-linux_x86_64.whl (239.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.7/239.7 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.32.3)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/torchvision-0.20.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/torchaudio-2.5.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, pytorch-triton, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
            "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu121\n",
            "    Uninstalling torchvision-0.16.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.1.0+cu121\n",
            "    Uninstalling torchaudio-2.1.0+cu121:\n",
            "      Successfully uninstalled torchaudio-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0.dev20241112+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-9.1.0.70 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 pytorch-triton-3.1.0+cf34004b8a torch-2.6.0.dev20241112+cu121 torchaudio-2.5.0.dev20241112+cu121 torchvision-0.20.0.dev20241112+cu121\n",
            "\n",
            "=== 3) Install bitsandbytes 0.45.2 and xformers 0.0.24 (built for Torch 2.2.0+cu121) ===\n",
            "Collecting bitsandbytes==0.45.2\n",
            "  Using cached bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting xformers==0.0.24\n",
            "  Using cached xformers-0.0.24-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes==0.45.2) (2.6.0.dev20241112+cu121)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes==0.45.2) (1.26.4)\n",
            "Collecting torch<3,>=2.0 (from bitsandbytes==0.45.2)\n",
            "  Using cached torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<3,>=2.0->bitsandbytes==0.45.2)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<3,>=2.0->bitsandbytes==0.45.2)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.2) (12.1.105)\n",
            "Collecting triton==2.2.0 (from torch<3,>=2.0->bitsandbytes==0.45.2)\n",
            "  Using cached triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.0->bitsandbytes==0.45.2) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes==0.45.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<3,>=2.0->bitsandbytes==0.45.2) (1.3.0)\n",
            "Using cached bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
            "Using cached xformers-0.0.24-cp311-cp311-manylinux2014_x86_64.whl (218.2 MB)\n",
            "Using cached torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl (755.5 MB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Using cached triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "Installing collected packages: triton, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch, xformers, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0.dev20241112+cu121\n",
            "    Uninstalling torch-2.6.0.dev20241112+cu121:\n",
            "      Successfully uninstalled torch-2.6.0.dev20241112+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.20.0.dev20241112+cu121 requires torch==2.6.0.dev20241112, but you have torch 2.2.0 which is incompatible.\n",
            "torchaudio 2.5.0.dev20241112+cu121 requires torch==2.6.0.dev20241112, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.45.2 nvidia-cudnn-cu12-8.9.2.26 nvidia-nccl-cu12-2.19.3 torch-2.2.0 triton-2.2.0 xformers-0.0.24\n",
            "\n",
            "=== 4) Write test_deps.py script to verify bitsandbytes, xformers, and triton ===\n",
            "\n",
            "=== 5) Run test_deps.py to confirm everything works with Torch 2.2.0+cu121 ===\n",
            "WARNING: BNB_CUDA_VERSION=121 environment variable detected; loading libbitsandbytes_cuda121.so.\n",
            "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
            "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
            "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
            "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
            "\n",
            "\n",
            "=== bitsandbytes import OK ===\n",
            "bitsandbytes linear8bit forward pass successful. Output shape: torch.Size([16, 64])\n",
            "\n",
            "=== xformers import OK ===\n",
            "xformers fmha output shape: torch.Size([1, 32, 8, 64])\n",
            "\n",
            "=== triton import OK ===\n",
            "triton add_kernel test, first 5 results: [1.5259300470352173, -1.2502985000610352, 3.3543295860290527, 0.27865782380104065, -0.015028402209281921]\n",
            "\n",
            "All tests passed! bitsandbytes, xformers, and triton are working.\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "# ONE-CELL COLAB SCRIPT: PyTorch Nightly (2.2.0 + cu121),\n",
        "# bitsandbytes 0.45.2, xformers 0.0.24, tested on A100 CUDA 12.x\n",
        "################################################################################\n",
        "\n",
        "print(\"=== Checking GPU and driver info ===\")\n",
        "!nvidia-smi\n",
        "\n",
        "print(\"\\n=== 1) Uninstall older Torch, bitsandbytes, xformers, triton ===\")\n",
        "!pip uninstall -y torch bitsandbytes xformers triton\n",
        "\n",
        "print(\"\\n=== 2) Install PyTorch NIGHTLY 2.2.0+cu121, plus torchvision, torchaudio\")\n",
        "print(\"         from the official 'nightly/cu121' index. ===\")\n",
        "\n",
        "# We use --pre (pre-release) and a special index URL for nightly cu121 builds.\n",
        "!pip install --pre torch torchvision torchaudio \\\n",
        "    --index-url https://download.pytorch.org/whl/nightly/cu121\n",
        "\n",
        "print(\"\\n=== 3) Install bitsandbytes 0.45.2 and xformers 0.0.24 (built for Torch 2.2.0+cu121) ===\")\n",
        "# We'll just use PyPI. bitsandbytes 0.45.2 has CUDA 12.1 support.\n",
        "# xformers 0.0.24 is built for Torch 2.2.0+cu121, so it won't conflict.\n",
        "!pip install bitsandbytes==0.45.2 xformers==0.0.24\n",
        "\n",
        "print(\"\\n=== 4) Write test_deps.py script to verify bitsandbytes, xformers, and triton ===\")\n",
        "\n",
        "test_deps_code = \"\"\"import os\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "os.environ[\"BNB_CUDA_VERSION\"] = \"121\"  # bitsandbytes tries libbitsandbytes_cuda121.so\n",
        "\n",
        "# 1) Test bitsandbytes\n",
        "try:\n",
        "    import bitsandbytes as bnb\n",
        "    print(\"\\\\n=== bitsandbytes import OK ===\")\n",
        "    linear_8bit = bnb.nn.Linear8bitLt(128, 64).cuda()\n",
        "    dummy_in = torch.randn(16, 128, device='cuda', dtype=torch.float16)\n",
        "    dummy_out = linear_8bit(dummy_in)\n",
        "    print('bitsandbytes linear8bit forward pass successful. Output shape:', dummy_out.shape)\n",
        "except Exception as ex:\n",
        "    print('bitsandbytes usage error:', ex)\n",
        "    sys.exit(1)\n",
        "\n",
        "# 2) Test xformers\n",
        "try:\n",
        "    import xformers\n",
        "    print(\"\\\\n=== xformers import OK ===\")\n",
        "    from xformers.ops import fmha\n",
        "    q = torch.randn((1, 32, 8, 64), device='cuda', dtype=torch.float16)\n",
        "    k = torch.randn((1, 32, 8, 64), device='cuda', dtype=torch.float16)\n",
        "    v = torch.randn((1, 32, 8, 64), device='cuda', dtype=torch.float16)\n",
        "    out = fmha.memory_efficient_attention(q, k, v)\n",
        "    print('xformers fmha output shape:', out.shape)\n",
        "except Exception as ex:\n",
        "    print('xformers usage error:', ex)\n",
        "    sys.exit(1)\n",
        "\n",
        "# 3) Test triton (bundled in Torch 2.2.0 nightly)\n",
        "try:\n",
        "    import triton\n",
        "    import triton.language as tl\n",
        "    print(\"\\\\n=== triton import OK ===\")\n",
        "\n",
        "    @triton.jit\n",
        "    def add_kernel(x_ptr, y_ptr, output_ptr, BLOCK_SIZE: tl.constexpr):\n",
        "        pid = tl.program_id(0)\n",
        "        offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "        x = tl.load(x_ptr + offset)\n",
        "        y = tl.load(y_ptr + offset)\n",
        "        tl.store(output_ptr + offset, x + y)\n",
        "\n",
        "    x_t = torch.randn(1024, device='cuda')\n",
        "    y_t = torch.randn(1024, device='cuda')\n",
        "    output_t = torch.empty(1024, device='cuda')\n",
        "    grid = (1024 // 256,)\n",
        "    add_kernel[grid](x_t, y_t, output_t, BLOCK_SIZE=256)\n",
        "    print('triton add_kernel test, first 5 results:', output_t[:5].tolist())\n",
        "except Exception as ex:\n",
        "    print('triton usage error:', ex)\n",
        "    sys.exit(1)\n",
        "\n",
        "print('\\\\nAll tests passed! bitsandbytes, xformers, and triton are working.')\n",
        "\"\"\"\n",
        "\n",
        "with open(\"test_deps.py\", \"w\") as f:\n",
        "    f.write(test_deps_code)\n",
        "\n",
        "print(\"\\n=== 5) Run test_deps.py to confirm everything works with Torch 2.2.0+cu121 ===\")\n",
        "!python test_deps.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z-O4Xdz04kRX",
      "metadata": {
        "id": "z-O4Xdz04kRX"
      },
      "source": [
        "---\n",
        "## 6) **Flexible Attention**\n",
        "\n",
        "Here’s a snippet that builds various attention masks (causal, sliding, etc.) and uses a chunked approach, plus `torch.compile` if you like. This demonstration shows different mask types in one place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LMGxXZCW4kRX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMGxXZCW4kRX",
        "outputId": "339d3c23-a355-40f8-edcd-f95311af2464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping torch.compile (Python 3.11+ not yet supported).\n",
            "\n",
            "===> Testing mask_type = causal\n",
            "seq_len=128, out.shape=torch.Size([2, 128, 64]), mask_type=causal\n",
            "seq_len=256, out.shape=torch.Size([2, 256, 64]), mask_type=causal\n",
            "seq_len=300, out.shape=torch.Size([2, 300, 64]), mask_type=causal\n",
            "seq_len=512, out.shape=torch.Size([2, 512, 64]), mask_type=causal\n",
            "\n",
            "===> Testing mask_type = sliding\n",
            "seq_len=128, out.shape=torch.Size([2, 128, 64]), mask_type=sliding\n",
            "seq_len=256, out.shape=torch.Size([2, 256, 64]), mask_type=sliding\n",
            "seq_len=300, out.shape=torch.Size([2, 300, 64]), mask_type=sliding\n",
            "seq_len=512, out.shape=torch.Size([2, 512, 64]), mask_type=sliding\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import math\n",
        "import torch\n",
        "\n",
        "def build_attention_mask(seq_len, mask_type=\"causal\", window_size=64, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Creates an attention mask:\n",
        "      - \"causal\": blocks j > i (standard auto-regressive mask).\n",
        "      - \"sliding\": local window = ±window_size around each token.\n",
        "    \"\"\"\n",
        "    mask = torch.zeros(seq_len, seq_len, device=device)\n",
        "    if mask_type == \"causal\":\n",
        "        # Triangular upper matrix => block j>i\n",
        "        casual_mat = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1)\n",
        "        mask[casual_mat.bool()] = float(\"-1e9\")\n",
        "    elif mask_type == \"sliding\":\n",
        "        # For each position i, block everything outside [i - window_size, i + window_size]\n",
        "        for i in range(seq_len):\n",
        "            left = max(0, i - window_size)\n",
        "            right = min(seq_len, i + window_size + 1)\n",
        "            mask[i, :left] = float(\"-1e9\")\n",
        "            mask[i, right:] = float(\"-1e9\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown mask_type={mask_type}\")\n",
        "    return mask\n",
        "\n",
        "def flex_attention(q, k, v, attn_mask):\n",
        "    \"\"\"\n",
        "    Simple scaled dot-product attention:\n",
        "      q, k, v: shape [batch, seq_len, d_model]\n",
        "      attn_mask: shape [seq_len, seq_len], large negative => blocked\n",
        "    \"\"\"\n",
        "    d_model = q.shape[-1]\n",
        "    # (batch, seq_len, d_model) @ (batch, d_model, seq_len) => (batch, seq_len, seq_len)\n",
        "    attn_scores = torch.bmm(q, k.transpose(1, 2)) / math.sqrt(d_model)\n",
        "\n",
        "    # Apply the mask (broadcast => (batch, seq_len, seq_len))\n",
        "    attn_scores = attn_scores + attn_mask.unsqueeze(0)\n",
        "\n",
        "    # Softmax and multiply by v\n",
        "    attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "    out = torch.bmm(attn_probs, v)\n",
        "    return out\n",
        "\n",
        "# Fallback approach for Python 3.11:\n",
        "# - If Python < 3.11 => we compile\n",
        "# - If Python >= 3.11 => skip compile to avoid runtime error\n",
        "if sys.version_info < (3, 11):\n",
        "    compiled_flex_attention = torch.compile(flex_attention, mode=\"default\")\n",
        "    print(\"Using torch.compile on Python < 3.11.\")\n",
        "else:\n",
        "    compiled_flex_attention = flex_attention\n",
        "    print(\"Skipping torch.compile (Python 3.11+ not yet supported).\")\n",
        "\n",
        "def run_flex_attention_demo():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    batch_size = 2\n",
        "    d_model = 64\n",
        "\n",
        "    for mask_type in [\"causal\", \"sliding\"]:\n",
        "        print(f\"\\n===> Testing mask_type = {mask_type}\")\n",
        "        for seq_len in [128, 256, 300, 512]:\n",
        "            q = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "            k = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "            v = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "\n",
        "            base_mask = build_attention_mask(seq_len, mask_type=mask_type, device=device)\n",
        "            out = compiled_flex_attention(q, k, v, base_mask)\n",
        "            print(f\"seq_len={seq_len}, out.shape={out.shape}, mask_type={mask_type}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_flex_attention_demo()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YJpiQQgp4kRY",
      "metadata": {
        "id": "YJpiQQgp4kRY"
      },
      "source": [
        "---\n",
        "## 7) **Sequence Classification Patch** (LoRA + `AutoModelForSequenceClassification`)\n",
        "\n",
        "We patch `AutoModelForSequenceClassification` by injecting LoRA modules into every `nn.Linear` in the model, then fine-tune only the LoRA parameters on a toy dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TtzfPXUE4kRY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtzfPXUE4kRY",
        "outputId": "27ae039b-bf50-450e-def4-0c2f0ecf0c71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Checking GPU / driver info ===\n",
            "Thu Feb 20 18:43:30 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "=== 1) Uninstall conflicting packages (torch, transformers, peft, xformers, etc.) ===\n",
            "Found existing installation: torch 2.1.0+cu121\n",
            "Uninstalling torch-2.1.0+cu121:\n",
            "  Successfully uninstalled torch-2.1.0+cu121\n",
            "Found existing installation: transformers 4.31.0\n",
            "Uninstalling transformers-4.31.0:\n",
            "  Successfully uninstalled transformers-4.31.0\n",
            "Found existing installation: peft 0.14.0\n",
            "Uninstalling peft-0.14.0:\n",
            "  Successfully uninstalled peft-0.14.0\n",
            "Found existing installation: xformers 0.0.24\n",
            "Uninstalling xformers-0.0.24:\n",
            "  Successfully uninstalled xformers-0.0.24\n",
            "Found existing installation: tokenizers 0.13.3\n",
            "Uninstalling tokenizers-0.13.3:\n",
            "  Successfully uninstalled tokenizers-0.13.3\n",
            "Found existing installation: bitsandbytes 0.45.2\n",
            "Uninstalling bitsandbytes-0.45.2:\n",
            "  Successfully uninstalled bitsandbytes-0.45.2\n",
            "\n",
            "=== 2) Install Torch 2.1.0+cu121 & Transformers==4.31.0 ===\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.1.0+cu121\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torch-2.1.0%2Bcu121-cp311-cp311-linux_x86_64.whl (2200.6 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (2024.10.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0+cu121) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0+cu121) (1.3.0)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "torchvision 0.20.0.dev20241112+cu121 requires torch==2.6.0.dev20241112, but you have torch 2.1.0+cu121 which is incompatible.\n",
            "torchaudio 2.5.0.dev20241112+cu121 requires torch==2.6.0.dev20241112, but you have torch 2.1.0+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.1.0+cu121\n",
            "Collecting transformers==4.31.0\n",
            "  Using cached transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n",
            "  Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2025.1.31)\n",
            "Using cached transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.31.0\n",
            "\n",
            "=== 3) Running your LoRA BERT classification code ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3, avg_loss=0.8157\n",
            "Epoch 2/3, avg_loss=0.7862\n",
            "Epoch 3/3, avg_loss=0.7097\n",
            "\n",
            "Inference Test:\n",
            "Input: ['I dislike the taste, not recommended.']\n",
            "Logits: [[-0.36591572  0.18960014]]\n",
            "Predicted label: 1 (0=Neg,1=Pos)\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "# SINGLE-CELL COLAB SCRIPT:\n",
        "# LoRA BERT classification w/ Torch 2.1.0+cu121 & Transformers 4.31.0\n",
        "# Removing peft & older libraries => fix the 'adapter_kwargs' error.\n",
        "################################################################################\n",
        "\n",
        "print(\"=== Checking GPU / driver info ===\")\n",
        "!nvidia-smi\n",
        "\n",
        "print(\"\\n=== 1) Uninstall conflicting packages (torch, transformers, peft, xformers, etc.) ===\")\n",
        "!pip uninstall -y torch transformers peft xformers tokenizers bitsandbytes\n",
        "\n",
        "print(\"\\n=== 2) Install Torch 2.1.0+cu121 & Transformers==4.31.0 ===\")\n",
        "!pip install torch==2.1.0+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers==4.31.0\n",
        "\n",
        "print(\"\\n=== 3) Running your LoRA BERT classification code ===\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        ")\n",
        "\n",
        "class ToyClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=32):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class LoRALinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, lora_rank=4, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.lora_down = nn.Linear(in_features, lora_rank, bias=False)\n",
        "        self.lora_up   = nn.Linear(lora_rank, out_features, bias=False)\n",
        "        nn.init.zeros_(self.lora_down.weight)\n",
        "        nn.init.zeros_(self.lora_up.weight)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.alpha * self.lora_up(self.lora_down(x))\n",
        "\n",
        "def patch_model_for_sequence_classification(model, lora_rank=4, alpha=1.0):\n",
        "    modules_to_patch = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            modules_to_patch.append((name, module))\n",
        "\n",
        "    for full_name, module in modules_to_patch:\n",
        "        safe_name = full_name.replace(\".\", \"_\")\n",
        "        lora_mod = LoRALinear(\n",
        "            module.in_features,\n",
        "            module.out_features,\n",
        "            lora_rank=lora_rank,\n",
        "            alpha=alpha\n",
        "        ).to(module.weight.device, module.weight.dtype)\n",
        "\n",
        "        # Register it\n",
        "        model.add_module(f\"lora_{safe_name}\", lora_mod)\n",
        "\n",
        "        # Patch forward\n",
        "        orig_forward = module.forward\n",
        "        def custom_forward(m_self, x, orig_forward=orig_forward, lora_layer=lora_mod):\n",
        "            base_out = orig_forward(x)\n",
        "            lora_out = lora_layer(x)\n",
        "            return base_out + lora_out\n",
        "\n",
        "        module.forward = custom_forward.__get__(module, module.__class__)\n",
        "\n",
        "    return model\n",
        "\n",
        "def finetune_sequence_classification():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model_name = \"bert-base-uncased\"\n",
        "    num_labels = 2\n",
        "\n",
        "    config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "    model.to(device)\n",
        "\n",
        "    # Inject LoRA\n",
        "    patch_model_for_sequence_classification(model, lora_rank=4, alpha=1.0)\n",
        "\n",
        "    texts = [\n",
        "        \"I love this product, it is amazing!\",\n",
        "        \"This is the worst experience of my life.\",\n",
        "        \"The movie was quite entertaining.\",\n",
        "        \"Horrible service, will not come back!\"\n",
        "    ]\n",
        "    labels = [1, 0, 1, 0]\n",
        "    dataset = ToyClassificationDataset(texts, labels, tokenizer, max_length=16)\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    # Only train LoRA params\n",
        "    lora_params = []\n",
        "    for param_name, param in model.named_parameters():\n",
        "        if \"lora_\" in param_name:\n",
        "            param.requires_grad = True\n",
        "            lora_params.append(param)\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    optimizer = optim.AdamW(lora_params, lr=1e-4)\n",
        "    model.train()\n",
        "    epochs = 3\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, avg_loss={avg_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    sample_text = [\"I dislike the taste, not recommended.\"]\n",
        "    enc = tokenizer(sample_text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**enc).logits\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "    print(\"\\nInference Test:\")\n",
        "    print(f\"Input: {sample_text}\")\n",
        "    print(f\"Logits: {logits.cpu().numpy()}\")\n",
        "    print(f\"Predicted label: {preds.item()} (0=Neg,1=Pos)\")\n",
        "\n",
        "finetune_sequence_classification()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MwGqZ7NL4kRY",
      "metadata": {
        "id": "MwGqZ7NL4kRY"
      },
      "source": [
        "---\n",
        "## 8) **Refactored Attention**\n",
        "\n",
        "Merging `xformers`, PyTorch’s SDPA, `flash_attn`, and a fallback “flex” approach in a single function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BpUWkXr44kRY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpUWkXr44kRY",
        "outputId": "a465ec8f-49cb-4d81-c850-dcc8a064930e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fallback => torch.Size([2, 4, 16, 64])\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "\n",
        "try:\n",
        "    import xformers.ops as xops\n",
        "    XFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XFORMERS_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import flash_attn\n",
        "    FLASH_ATTN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    FLASH_ATTN_AVAILABLE = False\n",
        "\n",
        "SDPA_AVAILABLE = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n",
        "\n",
        "def flex_custom_attention(q, k, v, attn_mask=None):\n",
        "    d_k = q.shape[-1]\n",
        "    scores = torch.matmul(q, k.transpose(-1, -2)) / (d_k ** 0.5)\n",
        "    if attn_mask is not None:\n",
        "        scores = scores + attn_mask\n",
        "    weights = torch.softmax(scores, dim=-1)\n",
        "    weights = weights.to(v.dtype)\n",
        "    out = torch.matmul(weights, v)\n",
        "    return out\n",
        "\n",
        "def xformers_attention(q, k, v, attn_mask=None):\n",
        "    B, H, L, D = q.shape\n",
        "    q_ = q.reshape(B*H, L, D)\n",
        "    k_ = k.reshape(B*H, L, D)\n",
        "    v_ = v.reshape(B*H, L, D)\n",
        "\n",
        "    bool_mask = None\n",
        "    if attn_mask is not None:\n",
        "        expanded = attn_mask.expand(B, H, L, L).reshape(B*H, L, L)\n",
        "        bool_mask = (expanded < -1e4)\n",
        "    out = xops.memory_efficient_attention(\n",
        "        q_, k_, v_,\n",
        "        attn_mask=bool_mask,\n",
        "        p=0.0\n",
        "    )\n",
        "    return out.reshape(B, H, L, D)\n",
        "\n",
        "def flash_attention(q, k, v, attn_mask=None):\n",
        "    import flash_attn\n",
        "    B, H, L, D = q.shape\n",
        "    q_ = q.reshape(B*H, L, D)\n",
        "    k_ = k.reshape(B*H, L, D)\n",
        "    v_ = v.reshape(B*H, L, D)\n",
        "    out = flash_attn.flash_attn_func(\n",
        "        q_, k_, v_,\n",
        "        dropout_p=0.0,\n",
        "        softmax_scale=None,\n",
        "        causal=False\n",
        "    )\n",
        "    return out.reshape(B, H, L, D)\n",
        "\n",
        "def sdpa_attention(q, k, v, attn_mask=None):\n",
        "    from torch.nn.functional import scaled_dot_product_attention as sdpa\n",
        "    B, H, L, D = q.shape\n",
        "    q_ = q.permute(2, 0, 1, 3).reshape(L, B*H, D)\n",
        "    k_ = k.permute(2, 0, 1, 3).reshape(L, B*H, D)\n",
        "    v_ = v.permute(2, 0, 1, 3).reshape(L, B*H, D)\n",
        "\n",
        "    am = None\n",
        "    if attn_mask is not None:\n",
        "        am = attn_mask.expand(B, H, L, L).reshape(B*H, L, L)\n",
        "    out_ = sdpa(q_, k_, v_, attn_mask=am, dropout_p=0.0, is_causal=False)\n",
        "    out = out_.reshape(L, B, H, D).permute(1, 2, 0, 3)\n",
        "    return out\n",
        "\n",
        "def unified_attention(q, k, v, attn_mask=None, backend=\"auto\"):\n",
        "    if backend == \"auto\":\n",
        "        if XFORMERS_AVAILABLE:\n",
        "            backend = \"xformers\"\n",
        "        elif FLASH_ATTN_AVAILABLE:\n",
        "            backend = \"flash\"\n",
        "        elif SDPA_AVAILABLE:\n",
        "            backend = \"sdpa\"\n",
        "        else:\n",
        "            backend = \"flex\"\n",
        "\n",
        "    if backend == \"xformers\":\n",
        "        if not XFORMERS_AVAILABLE:\n",
        "            raise RuntimeError(\"xformers not installed!\")\n",
        "        return xformers_attention(q, k, v, attn_mask)\n",
        "    elif backend == \"flash\":\n",
        "        if not FLASH_ATTN_AVAILABLE:\n",
        "            raise RuntimeError(\"flash_attn not installed!\")\n",
        "        return flash_attention(q, k, v, attn_mask)\n",
        "    elif backend == \"sdpa\":\n",
        "        if not SDPA_AVAILABLE:\n",
        "            raise RuntimeError(\"PyTorch >=2.0 needed for SDPA!\")\n",
        "        return sdpa_attention(q, k, v, attn_mask)\n",
        "    else:\n",
        "        return flex_custom_attention(q, k, v, attn_mask)\n",
        "\n",
        "# Demo usage\n",
        "def example_unified_attention():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    B, H, L, D = 2, 4, 16, 64\n",
        "    q = torch.randn(B, H, L, D, device=device, dtype=torch.float16)\n",
        "    k = torch.randn(B, H, L, D, device=device, dtype=torch.float16)\n",
        "    v = torch.randn(B, H, L, D, device=device, dtype=torch.float16)\n",
        "    attn_mask = torch.zeros((B, 1, L, L), device=device, dtype=torch.float32)\n",
        "    blocked = torch.rand((B, 1, L, L), device=device) < 0.2\n",
        "    attn_mask[blocked] = float(\"-inf\")\n",
        "    out_flex = unified_attention(q, k, v, attn_mask, backend=\"flex\")\n",
        "    print(\"fallback =>\", out_flex.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    example_unified_attention()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fgcv54J4kRY",
      "metadata": {
        "id": "6fgcv54J4kRY"
      },
      "source": [
        "---\n",
        "## Final Notes\n",
        "\n",
        "- This notebookincludes separate code snippets for each task.\n",
        "- Some cells (like the nF4 → Triton example) are skeletons or placeholders to illustrate core ideas.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01cb4a121efe4a22ab7c73ad32dec2d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "038ca7f144d4440c9e8491aecad624ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e825c79d0f6e44ebad83a6f771be1e1a",
            "placeholder": "​",
            "style": "IPY_MODEL_cce49134c0c9410981740d858c0443db",
            "value": "model.safetensors: 100%"
          }
        },
        "15759ebbfcc04d9c9f2f7b4fd4309b0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "181373080b8d496589f6fe3cb3073875": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa1fbe80d09649d58e67f0340155092c",
              "IPY_MODEL_8913aab41e5d4cf2a95d6140f946d8a9",
              "IPY_MODEL_ccb0c8b8ba4b480fbd7bb052519d8df5"
            ],
            "layout": "IPY_MODEL_9108f014eaa945b6902bd16eebf3ba46"
          }
        },
        "1ca0dd3965de4e0db99dd15859e11c94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fdd31c751e14bdfa97c033aa1b1ee42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51cee95ea779434f95d936fc91ea68ec",
              "IPY_MODEL_57218f4090c94b4caff3c11666127659",
              "IPY_MODEL_4bb026a75d084ad4bc7ea2d6dc025d7f"
            ],
            "layout": "IPY_MODEL_de9ade1df4d3488b9e0e8b7eb26a5bb8"
          }
        },
        "23f5ab8bb9224fcca40f3502841539f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78443b8aa21643b8a43fdef3482b8335",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccb13d4a89374c97bbddf15bb70a03d9",
            "value": 440449768
          }
        },
        "259873c2475441c1906585786a12aafe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "333953a8b4514d69b879051f4f604035": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a9cebd59a4f4cb6993fd0f6c8ba2190",
            "placeholder": "​",
            "style": "IPY_MODEL_545f4c90bd3249079ee8b543844c59fc",
            "value": " 440M/440M [00:02&lt;00:00, 220MB/s]"
          }
        },
        "3d9f035606e64e61b8c52c49d17bfa6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e314711bf3541b090080509efeea854": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f01843639b19476eba0fdeefc13f4064",
            "placeholder": "​",
            "style": "IPY_MODEL_b0733ffb1dc8406094cb82e87f8b6e43",
            "value": "config.json: 100%"
          }
        },
        "4bb026a75d084ad4bc7ea2d6dc025d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58ace24fdc684a03a8b6fd67baf06e29",
            "placeholder": "​",
            "style": "IPY_MODEL_01cb4a121efe4a22ab7c73ad32dec2d9",
            "value": " 466k/466k [00:00&lt;00:00, 5.28MB/s]"
          }
        },
        "4bc5dd30f02e44208ac465b787df0f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4db6473706974738a9e00a3ca226a2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51977d81eb4c427882df1e6ae3ac8987": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb1da27cfce7434998ad155ebe5a8976",
            "placeholder": "​",
            "style": "IPY_MODEL_15759ebbfcc04d9c9f2f7b4fd4309b0e",
            "value": " 570/570 [00:00&lt;00:00, 73.5kB/s]"
          }
        },
        "519ffd095d384b5285cd5cd145e5c924": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51cee95ea779434f95d936fc91ea68ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdee380a2bb6487d984672bfd5bf82eb",
            "placeholder": "​",
            "style": "IPY_MODEL_519ffd095d384b5285cd5cd145e5c924",
            "value": "tokenizer.json: 100%"
          }
        },
        "545f4c90bd3249079ee8b543844c59fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5496f47078f3431ca85e5777a4289e67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55f9698ffbf24fe2adcaae2502447133": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57218f4090c94b4caff3c11666127659": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a3c546a486e4fc99c18147b843cc440",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5496f47078f3431ca85e5777a4289e67",
            "value": 466062
          }
        },
        "58ace24fdc684a03a8b6fd67baf06e29": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a9cebd59a4f4cb6993fd0f6c8ba2190": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5baf1ab73a034ec69e747c11755d8991": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc42c294506d42c6ae4c78c537efe17c",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e03d1c6a1a6b48ada55d5c299f0b80e9",
            "value": 570
          }
        },
        "5ca428b9ae1643f6b86f6f24783f2f69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_259873c2475441c1906585786a12aafe",
            "placeholder": "​",
            "style": "IPY_MODEL_6b975e8c791b4cb7858ad4cbbd281418",
            "value": " 48.0/48.0 [00:00&lt;00:00, 5.81kB/s]"
          }
        },
        "68ceb509b6a44fceb719d5c39f269db9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a3c546a486e4fc99c18147b843cc440": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b975e8c791b4cb7858ad4cbbd281418": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70851ba666664e47bf2156ba5ad32847": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e314711bf3541b090080509efeea854",
              "IPY_MODEL_5baf1ab73a034ec69e747c11755d8991",
              "IPY_MODEL_51977d81eb4c427882df1e6ae3ac8987"
            ],
            "layout": "IPY_MODEL_1ca0dd3965de4e0db99dd15859e11c94"
          }
        },
        "78443b8aa21643b8a43fdef3482b8335": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a625d82abb84f66bb69d13ae6b27521": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8913aab41e5d4cf2a95d6140f946d8a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68ceb509b6a44fceb719d5c39f269db9",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4db6473706974738a9e00a3ca226a2fa",
            "value": 231508
          }
        },
        "9108f014eaa945b6902bd16eebf3ba46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9190644ea3b142238bd6a92f5dc06b5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dd7876d99334fd9aabdacfcc9c2b261": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa1fbe80d09649d58e67f0340155092c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a625d82abb84f66bb69d13ae6b27521",
            "placeholder": "​",
            "style": "IPY_MODEL_3d9f035606e64e61b8c52c49d17bfa6d",
            "value": "vocab.txt: 100%"
          }
        },
        "b0733ffb1dc8406094cb82e87f8b6e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6413960dedf43069f4ac84eac3d05c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc0d335e69d04bcab58cde25af6255ac",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4bc5dd30f02e44208ac465b787df0f16",
            "value": 48
          }
        },
        "b934c51ac0a84f7daeb80a0cfc7bc944": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef3ab1bbe1024a53b006af49acbf5892",
              "IPY_MODEL_b6413960dedf43069f4ac84eac3d05c3",
              "IPY_MODEL_5ca428b9ae1643f6b86f6f24783f2f69"
            ],
            "layout": "IPY_MODEL_d2387e6f99cb4d3da83a1904598da5cf"
          }
        },
        "bc0cdc1deeca4454bdd468b0a50b7035": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdee380a2bb6487d984672bfd5bf82eb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccb0c8b8ba4b480fbd7bb052519d8df5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9190644ea3b142238bd6a92f5dc06b5b",
            "placeholder": "​",
            "style": "IPY_MODEL_55f9698ffbf24fe2adcaae2502447133",
            "value": " 232k/232k [00:00&lt;00:00, 1.45MB/s]"
          }
        },
        "ccb13d4a89374c97bbddf15bb70a03d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cce49134c0c9410981740d858c0443db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1cec5d326d24e5f845cecf452937351": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2387e6f99cb4d3da83a1904598da5cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de9ade1df4d3488b9e0e8b7eb26a5bb8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e03d1c6a1a6b48ada55d5c299f0b80e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e825c79d0f6e44ebad83a6f771be1e1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9ea43f386464dcdadfd9d417bb34be3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_038ca7f144d4440c9e8491aecad624ee",
              "IPY_MODEL_23f5ab8bb9224fcca40f3502841539f5",
              "IPY_MODEL_333953a8b4514d69b879051f4f604035"
            ],
            "layout": "IPY_MODEL_9dd7876d99334fd9aabdacfcc9c2b261"
          }
        },
        "ef3ab1bbe1024a53b006af49acbf5892": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1cec5d326d24e5f845cecf452937351",
            "placeholder": "​",
            "style": "IPY_MODEL_bc0cdc1deeca4454bdd468b0a50b7035",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "f01843639b19476eba0fdeefc13f4064": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb1da27cfce7434998ad155ebe5a8976": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc0d335e69d04bcab58cde25af6255ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc42c294506d42c6ae4c78c537efe17c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
