{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMGhfxxG6mzLRGsOz+ZwWGl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csalnav2/QdotCS/blob/master/nf4Triton1.02x.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 1) Full environment reset\n",
        "# =============================\n",
        "!pip uninstall -y torch torchvision torchaudio triton xformers bitsandbytes unsloth unsloth_zoo fastai cut_cross_entropy\n",
        "\n",
        "# =============================\n",
        "# 2) Install nightly PyTorch (>=2.6.0 dev) + matching Triton\n",
        "# =============================\n",
        "!pip install --no-cache-dir --pre --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n",
        "!pip install --no-cache-dir triton>=3.1.0\n",
        "\n",
        "# =============================\n",
        "# 3) Install extras\n",
        "# =============================\n",
        "!pip install --no-deps xformers==0.0.29 bitsandbytes accelerate peft trl\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth_zoo\n",
        "!pip install --no-deps unsloth\n",
        "!pip install tyro\n",
        "\n",
        "import torch\n",
        "print(\"Torch:\", torch.__version__)\n",
        "\n",
        "import triton\n",
        "print(\"Triton:\", triton.__version__)\n",
        "\n",
        "# If Colab complains \"You must restart runtime,\" do it and re-run.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9KY6Yww-4VR",
        "outputId": "1a19fb4e-dd08-415b-db22-315386183fbf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.5.1+cu124\n",
            "Uninstalling torch-2.5.1+cu124:\n",
            "  Successfully uninstalled torch-2.5.1+cu124\n",
            "Found existing installation: torchvision 0.20.1+cu124\n",
            "Uninstalling torchvision-0.20.1+cu124:\n",
            "  Successfully uninstalled torchvision-0.20.1+cu124\n",
            "Found existing installation: torchaudio 2.5.1+cu124\n",
            "Uninstalling torchaudio-2.5.1+cu124:\n",
            "  Successfully uninstalled torchaudio-2.5.1+cu124\n",
            "Found existing installation: triton 3.1.0\n",
            "Uninstalling triton-3.1.0:\n",
            "  Successfully uninstalled triton-3.1.0\n",
            "\u001b[33mWARNING: Skipping xformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping unsloth as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping unsloth_zoo as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: fastai 2.7.18\n",
            "Uninstalling fastai-2.7.18:\n",
            "  Successfully uninstalled fastai-2.7.18\n",
            "\u001b[33mWARNING: Skipping cut_cross_entropy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/nightly/cu121\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/torch-2.6.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (768.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.0/768.0 MB\u001b[0m \u001b[31m332.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/torchvision-0.20.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m343.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/torchaudio-2.5.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m354.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m342.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m380.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m331.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m264.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m348.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m154.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m376.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m351.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m352.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m323.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m317.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-triton==3.1.0+cf34004b8a (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.1.0%2Bcf34004b8a-cp311-cp311-linux_x86_64.whl (239.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.7/239.7 MB\u001b[0m \u001b[31m334.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, pytorch-triton, nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-cusparselt-cu12-0.6.2 nvidia-nvtx-cu12-12.1.105 pytorch-triton-3.1.0+cf34004b8a torch-2.6.0.dev20241112+cu121 torchaudio-2.5.0.dev20241112+cu121 torchvision-0.20.0.dev20241112+cu121\n",
            "Collecting xformers==0.0.29\n",
            "  Downloading xformers-0.0.29-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Collecting trl\n",
            "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
            "Downloading xformers-0.0.29-cp311-cp311-manylinux_2_28_x86_64.whl (15.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers, trl, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.3 trl-0.15.2 xformers-0.0.29\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (4.25.6)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Collecting hf_transfer\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, hf_transfer, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 hf_transfer-0.1.9 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting unsloth_zoo\n",
            "  Downloading unsloth_zoo-2025.2.7-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading unsloth_zoo-2025.2.7-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.1/107.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unsloth_zoo\n",
            "Successfully installed unsloth_zoo-2025.2.7\n",
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.2.15-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth-2025.2.15-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.8/188.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unsloth\n",
            "Successfully installed unsloth-2025.2.15\n",
            "Collecting tyro\n",
            "  Downloading tyro-0.9.16-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro) (4.4.2)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from tyro) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro) (0.1.2)\n",
            "Downloading tyro-0.9.16-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: shtab, tyro\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.2.7 requires cut_cross_entropy, which is not installed.\n",
            "unsloth 2025.2.15 requires protobuf<4.0.0, but you have protobuf 4.25.6 which is incompatible.\n",
            "unsloth-zoo 2025.2.7 requires protobuf<4.0.0, but you have protobuf 4.25.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed shtab-1.7.1 tyro-0.9.16\n",
            "Torch: 2.6.0.dev20241112+cu121\n",
            "Triton: 3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import inspect\n",
        "from transformers import set_seed\n",
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "\n",
        "# For unsloth-based NF4 decode\n",
        "from unsloth.kernels.utils import fast_dequantize\n",
        "\n",
        "# For PEFT-based NF4 decode\n",
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on device:\", device)\n",
        "\n",
        "def assert_same(x, y, dt):\n",
        "    if x.dtype != dt:\n",
        "        raise RuntimeError(f\"dtype mismatch: got {x.dtype}, expected {dt}\")\n",
        "    torch.testing.assert_close(x, y, check_stride=True)\n",
        "\n",
        "def bnb_Linear4bit(hd, m, dt=torch.float16):\n",
        "    return Linear4bit(hd, m, bias=None, compute_dtype=dt, compress_statistics=True, quant_type=\"nf4\")\n",
        "\n",
        "def assert_bnb_state(w, dt):\n",
        "    assert w.weight.dtype == torch.uint8\n",
        "    s = w.weight.quant_state\n",
        "    assert s.dtype == dt\n",
        "    assert s.absmax.dtype == torch.uint8\n",
        "    assert s.code.dtype == torch.float32\n",
        "    assert s.offset.dtype == torch.float32\n",
        "    assert s.blocksize == 64\n",
        "    assert s.state2.absmax.dtype == torch.float32\n",
        "    assert s.state2.code.dtype == torch.float32\n",
        "    assert s.state2.blocksize == 256\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd=4096, m=14336, dt=torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dt).to(device)\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dt).to(device)\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dt).to(device)\n",
        "        # Force correct quant_state dtype\n",
        "        self.gate_proj.weight.quant_state.dtype = dt\n",
        "        self.up_proj.weight.quant_state.dtype   = dt\n",
        "        self.down_proj.weight.quant_state.dtype = dt\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    up   = X @ fx(mlp.up_proj).t()\n",
        "    gate = X @ fx(mlp.gate_proj).t()\n",
        "    return mlp.act_fn(gate) * up @ fx(mlp.down_proj).t()\n",
        "\n",
        "def mlp_dequantize(X, mlp, fx):\n",
        "    a = fx(mlp.up_proj).t();   torch.cuda.synchronize()\n",
        "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
        "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
        "    return a,b,c\n",
        "\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n",
        "def test_dequantize(dequant_fx):\n",
        "    e = 0\n",
        "    # Some test configs\n",
        "    configs = [\n",
        "      (2, 3333, 2048, 8192, 3407, torch.float16),\n",
        "      (5,  777, 1024, 4096, 3409, torch.bfloat16),\n",
        "      (3, 2048, 4096, 14336,3408, torch.bfloat16),\n",
        "    ]\n",
        "    for (bsz, ql, hd, m, seed, dt) in configs:\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "        mlp = MLP(hd, m, dt)\n",
        "        X   = torch.randn((bsz, ql, hd), device=device, dtype=dt)\n",
        "        torch.cuda.synchronize()\n",
        "        # Warmup checks\n",
        "        for _ in range(2):\n",
        "            out_manual = mlp_forward(X, mlp, dequant_fx)\n",
        "            out_model  = mlp(X)\n",
        "            assert_same(out_manual, out_model, dt)\n",
        "            assert_bnb_state(mlp.up_proj, dt)\n",
        "            assert_bnb_state(mlp.gate_proj, dt)\n",
        "            assert_bnb_state(mlp.down_proj, dt)\n",
        "            a,b,c = mlp_dequantize(X, mlp, dequant_fx)\n",
        "            A,B,C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
        "            assert_same(a,A, dt)\n",
        "            assert_same(b,B, dt)\n",
        "            assert_same(c,C, dt)\n",
        "        # Benchmark\n",
        "        torch.cuda.synchronize()\n",
        "        t0 = time.time()\n",
        "        for _ in range(1000):\n",
        "            mlp_dequantize(X, mlp, dequant_fx)\n",
        "        e += (time.time() - t0)\n",
        "    return e\n",
        "\n",
        "# Test unsloth + peft\n",
        "if device == \"cuda\":\n",
        "    e_unsloth = test_dequantize(unsloth_dequantize)\n",
        "    print(f\"[INFO] unsloth_dequantize total time: {e_unsloth:.4f}s\")\n",
        "    e_peft = test_dequantize(peft_dequantize)\n",
        "    print(f\"[INFO] peft_dequantize total time: {e_peft:.4f}s\")\n",
        "else:\n",
        "    print(\"[INFO] CPU environment, skipping NF4 bitsandbytes tests.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_99Stplc_5i0",
        "outputId": "e96ae20d-4706-4a01-d135-ecc27ce9abea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.6.0.dev20241112+cu121)\n",
            "    Python  3.11.11 (you have 3.11.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "Running on device: cuda\n",
            "[INFO] unsloth_dequantize total time: 5.2716s\n",
            "[INFO] peft_dequantize total time: 5.4960s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch triton bitsandbytes unsloth\n",
        "!pip install --pre --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n",
        "!pip install --no-cache-dir triton>=3.1.0\n",
        "!pip install bitsandbytes\n",
        "!pip install --no-deps unsloth\n",
        "!pip install --no-deps peft\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjrhqfFq_OO7",
        "outputId": "080c8a84-c137-4114-e319-a4a0ae0e6012"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0.dev20241112+cu121\n",
            "Uninstalling torch-2.6.0.dev20241112+cu121:\n",
            "  Successfully uninstalled torch-2.6.0.dev20241112+cu121\n",
            "Found existing installation: triton 3.2.0\n",
            "Uninstalling triton-3.2.0:\n",
            "  Successfully uninstalled triton-3.2.0\n",
            "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping unsloth as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/nightly/cu121\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu121/torch-2.6.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (768.0 MB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.0.dev20241112+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.0.dev20241112+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: pytorch-triton==3.1.0+cf34004b8a in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0+cf34004b8a)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.2.7 requires cut_cross_entropy, which is not installed.\n",
            "unsloth-zoo 2025.2.7 requires triton; platform_system == \"Linux\", which is not installed.\n",
            "xformers 0.0.29 requires torch==2.5.1, but you have torch 2.6.0.dev20241112+cu121 which is incompatible.\n",
            "unsloth-zoo 2025.2.7 requires protobuf<4.0.0, but you have protobuf 4.25.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.6.0.dev20241112+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.2.7 requires cut_cross_entropy, which is not installed.\n",
            "unsloth-zoo 2025.2.7 requires protobuf<4.0.0, but you have protobuf 4.25.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting bitsandbytes\n",
            "  Using cached bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0.dev20241112+cu121)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: pytorch-triton==3.1.0+cf34004b8a in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0+cf34004b8a)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.0->bitsandbytes) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Using cached bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.3\n",
            "Collecting unsloth\n",
            "  Using cached unsloth-2025.2.15-py3-none-any.whl.metadata (57 kB)\n",
            "Using cached unsloth-2025.2.15-py3-none-any.whl (188 kB)\n",
            "Installing collected packages: unsloth\n",
            "Successfully installed unsloth-2025.2.15\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================ Cell start ================================\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from triton import jit\n",
        "\n",
        "# 1) Minimal kernel with *no* parameters or shape arguments.\n",
        "#    We literally do arange(0, 512) => a compile-time constant: 512\n",
        "@jit\n",
        "def kernel_512(X_ptr, Y_ptr):\n",
        "    # Hard-coded literal => 512 => must be recognized as a compile-time constant\n",
        "    idx = tl.arange(0, 512)\n",
        "    x_val = tl.load(X_ptr + idx)\n",
        "    out   = x_val + 1.0\n",
        "    tl.store(Y_ptr + idx, out)\n",
        "\n",
        "# 2) Attempt to run\n",
        "try:\n",
        "    x = torch.randn(512, device=\"cuda\")\n",
        "    y = torch.empty_like(x)\n",
        "    # Launch with grid=(1,) => single block\n",
        "    kernel_512[(1,)](x, y)\n",
        "    print(\"[INFO] Triton kernel_512 succeeded!\")\n",
        "    print(\"[INFO] y[:10] =\", y[:10])\n",
        "except Exception as e:\n",
        "    print(\"[ERROR] kernel_512 failed =>\", e)\n",
        "# ================================ Cell end =================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JJVG9zqAIuX",
        "outputId": "c06db1c4-16c7-427b-ca33-bf8654f80e4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Triton kernel_512 succeeded!\n",
            "[INFO] y[:10] = tensor([-0.3428,  1.0706,  0.2601,  0.5595,  1.5209, -0.2726,  1.5390,  1.3119,\n",
            "         1.6588,  1.2323], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrIIc5h8HhFS",
        "outputId": "f2263719-6e84-4316-f3e4-ad6759cf4fba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0.dev20241112+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: pytorch-triton==3.1.0+cf34004b8a in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0+cf34004b8a)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Ensure we have a GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"[INFO] device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6K0_iWUHnc-",
        "outputId": "6a9a56b2-7404-4a4a-b27a-0d341437ba56"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "\n",
        "################################################################################\n",
        "# 1) MLP using bitsandbytes 4-bit layers\n",
        "################################################################################\n",
        "def bnb_Linear4bit(in_features, out_features, dtype=torch.float16):\n",
        "    \"\"\"\n",
        "    We'll forcibly interpret arguments as (in_features, out_features)\n",
        "    though bitsandbytes might flatten them.\n",
        "    \"\"\"\n",
        "    return Linear4bit(\n",
        "        in_features, out_features,\n",
        "        bias=None,\n",
        "        compute_dtype=dtype,\n",
        "        compress_statistics=True,\n",
        "        quant_type=\"nf4\",\n",
        "    )\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd=2048, m=8192, dtype=torch.float16):\n",
        "        super().__init__()\n",
        "        # Gate & Up => (hd->m), down => (m->hd)\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dtype=dtype).cuda()\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dtype=dtype).cuda()\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dtype=dtype).cuda()\n",
        "\n",
        "        for layer in [self.gate_proj, self.up_proj, self.down_proj]:\n",
        "            layer.weight.quant_state.dtype = dtype\n",
        "\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "\n",
        "        print(\"\\n=== MLP LAYER SHAPES (bitsandbytes param) ===\")\n",
        "        print(\"gate_proj =>\", self.gate_proj.weight.shape)\n",
        "        print(\"up_proj   =>\", self.up_proj.weight.shape)\n",
        "        print(\"down_proj =>\", self.down_proj.weight.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(\n",
        "            self.act_fn(self.gate_proj(x)) * self.up_proj(x)\n",
        "        )\n",
        "\n",
        "################################################################################\n",
        "# 2) A decode function that re-shapes (8388608,1)->(8192,1024)->(8192,2048)\n",
        "################################################################################\n",
        "def nf4_decode(layer):\n",
        "    \"\"\"\n",
        "    If layer.weight.shape => (8388608,1), we interpret out_features=8192,\n",
        "    half_in=1024 => final => (8192,2048) after decoding to float16.\n",
        "\n",
        "    If shape is different, handle accordingly.\n",
        "    \"\"\"\n",
        "    w_packed = layer.weight.data  # nibble-packed\n",
        "    shape_packed = w_packed.shape  # e.g. (8388608,1)\n",
        "\n",
        "    if shape_packed[0] == 8388608 and shape_packed[1] == 1:\n",
        "        # we interpret out_features=8192, half_in=1024 => so final => (8192, 2048)\n",
        "        out_features, half_in = 8192, 1024\n",
        "        # create a random float16 matrix => (8192, 2048) as a placeholder decode\n",
        "        decoded = torch.randn(out_features, half_in*2, device=w_packed.device, dtype=torch.float16)\n",
        "        return decoded\n",
        "    else:\n",
        "        # If bitsandbytes stored it differently, adapt logic\n",
        "        # For example, if shape=(m, hd//2), we do final => (m, hd)\n",
        "        m, half_hd = shape_packed\n",
        "        # Maybe it's already (8192, 1024)? Then we do => (8192, 2048)\n",
        "        decoded = torch.randn(m, half_hd*2, device=w_packed.device, dtype=torch.float16)\n",
        "        return decoded\n",
        "\n",
        "################################################################################\n",
        "# 3) A small forward that decodes \"gate_proj\" => matmul => see if it works\n",
        "################################################################################\n",
        "def debug_forward(X, mlp):\n",
        "    \"\"\"\n",
        "    We'll decode the gate_proj weight => shape (m, hd),\n",
        "    then matmul with X => shape(bsz, qlen, hd).\n",
        "    \"\"\"\n",
        "    w_dec = nf4_decode(mlp.gate_proj)  # (m, hd) => e.g. (8192, 2048)\n",
        "    print(\"[debug_forward] w_dec.shape =>\", w_dec.shape)\n",
        "\n",
        "    bsz, qlen, hd = X.shape\n",
        "    # (bsz*qlen, hd) x (hd, m)\n",
        "    X_2d= X.reshape(bsz*qlen, hd)\n",
        "    out_2d= X_2d @ w_dec.t()  # => (bsz*qlen, m)\n",
        "    return out_2d.reshape(bsz, qlen, -1)\n",
        "\n",
        "################################################################################\n",
        "# 4) Running the test\n",
        "################################################################################\n",
        "def run_test():\n",
        "    # typical => hd=2048 => m=8192 => in_features=2048 => out_features=8192\n",
        "    mlp = MLP(2048, 8192, dtype=torch.float16)\n",
        "    # X => (2, 3333, 2048)\n",
        "    X= torch.randn((2, 3333, 2048), device=\"cuda\", dtype=torch.float16)\n",
        "    print(\"X.shape =>\", X.shape)\n",
        "\n",
        "    out= debug_forward(X, mlp)\n",
        "    print(\"[INFO] final out.shape =>\", out.shape)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    run_test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlUv0LP3fbyT",
        "outputId": "1186111d-c21e-45be-b396-3fb2ac5b2105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== MLP LAYER SHAPES (bitsandbytes param) ===\n",
            "gate_proj => torch.Size([8388608, 1])\n",
            "up_proj   => torch.Size([8388608, 1])\n",
            "down_proj => torch.Size([8388608, 1])\n",
            "X.shape => torch.Size([2, 3333, 2048])\n",
            "[debug_forward] w_dec.shape => torch.Size([8192, 2048])\n",
            "[INFO] final out.shape => torch.Size([2, 3333, 8192])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# 0) Install / Imports\n",
        "################################################################################\n",
        "try:\n",
        "    import triton\n",
        "except ImportError:\n",
        "    !pip install -U triton\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from triton import jit\n",
        "\n",
        "# Attempt bitsandbytes/transformers\n",
        "try:\n",
        "    from bitsandbytes.nn import Linear4bit\n",
        "    from transformers.activations import ACT2FN\n",
        "except ImportError:\n",
        "    print(\"[WARNING] bitsandbytes or transformers not installed; MLP test might not work fully.\")\n",
        "\n",
        "# For older GPUs, fallback BF16->FP16 if SM80+ is not available\n",
        "major_cc, minor_cc = torch.cuda.get_device_capability()\n",
        "BF16_AVAILABLE = (major_cc >= 8)\n",
        "if not BF16_AVAILABLE:\n",
        "    print(\"[INFO] This GPU does not support SM80+. We fallback from BF16 to FP16 where needed.\")\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# 1) Triton Kernel => Multi-Block NF4 Dequant\n",
        "################################################################################\n",
        "\n",
        "@triton.jit\n",
        "def _nf4_dequantize_kernel(\n",
        "    QWEIGHT, ABSMAX, CODE, OFFSET, S2_ABSMAX, S2_CODE, OUT,\n",
        "    BLOCK_SIZE: tl.constexpr, LENGTH: tl.constexpr,\n",
        "    USE_128: tl.int32, SIGNED_NIB: tl.int32\n",
        "):\n",
        "    \"\"\"\n",
        "    Multi-block NF4 kernel => each block processes up to BLOCK_SIZE elements.\n",
        "    \"\"\"\n",
        "    pid = tl.program_id(0)\n",
        "    idx = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = idx < LENGTH\n",
        "\n",
        "    byte_idx = idx // 2\n",
        "    nib_side = idx & 1\n",
        "\n",
        "    # Load as uint8 for bitwise\n",
        "    byte_v = tl.load(QWEIGHT + byte_idx, mask=mask, other=0).to(tl.uint8)\n",
        "    nib_i32 = (byte_v >> (nib_side * 4)) & 0xF\n",
        "\n",
        "    # Signed nib?\n",
        "    if SIGNED_NIB != 0:\n",
        "        is_neg = nib_i32 >= 8\n",
        "        nib_signed = nib_i32 - 16\n",
        "        nib_signed = tl.where(is_neg, nib_signed, nib_i32)\n",
        "        code_idx   = nib_signed + 8\n",
        "        code_idx   = tl.where(code_idx < 0, 0, code_idx)\n",
        "        code_idx   = tl.where(code_idx > 15, 15, code_idx)\n",
        "    else:\n",
        "        code_idx = nib_i32\n",
        "        code_idx = tl.where(code_idx > 15, 15, code_idx)\n",
        "\n",
        "    # Load LUT => code, offset\n",
        "    code_val = tl.load(CODE + code_idx,  mask=mask, other=0)\n",
        "    off_val  = tl.load(OFFSET + nib_i32, mask=mask, other=0)\n",
        "    val_f32  = code_val + off_val\n",
        "\n",
        "    # Scale factors\n",
        "    block_id = idx // 64\n",
        "    am_u8    = tl.load(ABSMAX  + block_id, mask=mask, other=0)\n",
        "    s2_am_u8 = tl.load(S2_ABSMAX+ block_id, mask=mask, other=127)\n",
        "\n",
        "    scale = tl.where(USE_128 != 0, am_u8 / 128.0, am_u8 / 127.0)\n",
        "    scale = tl.where(scale == 0, 1.0, scale)\n",
        "    s2_am = tl.where(USE_128 != 0, s2_am_u8 / 128.0, s2_am_u8 / 127.0)\n",
        "\n",
        "    s2_cd = tl.load(S2_CODE + nib_i32, mask=mask, other=0)\n",
        "    val_f32 = val_f32 * s2_cd * scale * s2_am\n",
        "\n",
        "    out_val = val_f32.to(tl.float16)\n",
        "    tl.store(OUT + idx, out_val, mask=mask)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# 2) your_dequantize_nf4 => checks for 4-bit vs. already-full\n",
        "################################################################################\n",
        "\n",
        "def your_dequantize_nf4(layer):\n",
        "    \"\"\"\n",
        "    Checks the shape of layer.weight => if numel == rows*cols, skip decode.\n",
        "    If numel*2 == rows*cols, do 4-bit decode with the Triton kernel.\n",
        "    If offset / state2 is missing, use dummy placeholders.\n",
        "    \"\"\"\n",
        "    w = layer.weight\n",
        "    if not hasattr(w, 'quant_state'):\n",
        "        # fallback => no decode\n",
        "        return w\n",
        "\n",
        "    qs = w.quant_state\n",
        "    rows, cols = w.shape\n",
        "    final_numel = rows * cols  # how many FP16 elements we need for the final shape\n",
        "    actual_numel = w.numel()   # how many elements bitsandbytes allocated\n",
        "\n",
        "    # CASE 1: If bitsandbytes is storing it at \"full\" size => skip decode\n",
        "    if actual_numel == final_numel:\n",
        "        return w\n",
        "\n",
        "    # CASE 2: If half size => do the NF4 decode\n",
        "    if actual_numel * 2 != final_numel:\n",
        "        # mismatch => raise an error or skip\n",
        "        msg = (f\"[ERROR] Expected w.numel()*2 == {final_numel}, but got w.numel()={actual_numel}. \"\n",
        "               f\"Cannot decode properly. shape={w.shape}, final_numel={final_numel}\")\n",
        "        raise RuntimeError(msg)\n",
        "\n",
        "    # Now we do the normal NF4 decode\n",
        "    length_nibbles = final_numel  # = w.numel() * 2\n",
        "    BLOCK_SIZE = 1024\n",
        "    grid = ((length_nibbles + BLOCK_SIZE - 1) // BLOCK_SIZE,)\n",
        "\n",
        "    out_1d = torch.empty(length_nibbles, dtype=torch.float16, device=w.device)\n",
        "\n",
        "    # Check qs.offset\n",
        "    offset_tensor = (\n",
        "        qs.offset.to(w.device)\n",
        "        if getattr(qs, 'offset', None) is not None\n",
        "        else torch.zeros((16,), dtype=w.dtype, device=w.device)\n",
        "    )\n",
        "\n",
        "    # If qs.state2 is None or missing => dummy placeholders\n",
        "    if getattr(qs, 'state2', None) is None:\n",
        "        qs.state2 = type('', (), {})()\n",
        "    if getattr(qs.state2, 'absmax', None) is None:\n",
        "        qs.state2.absmax = torch.zeros_like(qs.absmax)\n",
        "    if getattr(qs.state2, 'code', None) is None:\n",
        "        qs.state2.code = torch.zeros_like(qs.code)\n",
        "\n",
        "    absmax_s2 = qs.state2.absmax.to(w.device)\n",
        "    code_s2   = qs.state2.code.to(w.device)\n",
        "\n",
        "    # Launch kernel\n",
        "    _nf4_dequantize_kernel[grid](\n",
        "        w,\n",
        "        qs.absmax.to(w.device),\n",
        "        qs.code.to(w.device),\n",
        "        offset_tensor,\n",
        "        absmax_s2,\n",
        "        code_s2,\n",
        "        out_1d,\n",
        "        BLOCK_SIZE=BLOCK_SIZE,\n",
        "        LENGTH=length_nibbles,\n",
        "        USE_128=0,\n",
        "        SIGNED_NIB=0\n",
        "    )\n",
        "\n",
        "    return out_1d.reshape(rows, cols)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# 3) MLP => 3 x 4-bit Layers\n",
        "################################################################################\n",
        "\n",
        "try:\n",
        "    from bitsandbytes.nn import Linear4bit\n",
        "    from transformers.activations import ACT2FN\n",
        "except ImportError:\n",
        "    class Linear4bit(nn.Linear):\n",
        "        pass\n",
        "    def ACT2FN(x): return torch.nn.functional.silu\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd=4096, m=14336, dtype=torch.float16):\n",
        "        super().__init__()\n",
        "        # Example bitsandbytes usage => 4-bit NF4\n",
        "        # Setting compress_statistics=False to avoid dynamic stats changes\n",
        "        self.gate_proj = Linear4bit(hd, m, bias=None, compute_dtype=dtype,\n",
        "                                    compress_statistics=False, quant_type=\"nf4\").cuda()\n",
        "        self.up_proj   = Linear4bit(hd, m, bias=None, compute_dtype=dtype,\n",
        "                                    compress_statistics=False, quant_type=\"nf4\").cuda()\n",
        "        self.down_proj = Linear4bit(m, hd, bias=None, compute_dtype=dtype,\n",
        "                                    compress_statistics=False, quant_type=\"nf4\").cuda()\n",
        "\n",
        "        # If bitsandbytes => set quant_state dtype\n",
        "        for layer in (self.gate_proj, self.up_proj, self.down_proj):\n",
        "            if hasattr(layer.weight, 'quant_state'):\n",
        "                layer.weight.quant_state.dtype = dtype\n",
        "\n",
        "        self.act_fn = torch.nn.functional.silu\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(\n",
        "            self.act_fn(self.gate_proj(x)) * self.up_proj(x)\n",
        "        )\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# 4) Helper => decode each layer weight, then forward\n",
        "################################################################################\n",
        "\n",
        "def mlp_forward(X, mlp, decode_fx):\n",
        "    \"\"\"\n",
        "    Calls decode_fx on each 4-bit layer, does forward with them, then reverts.\n",
        "    \"\"\"\n",
        "    gw_decoded = decode_fx(mlp.gate_proj)\n",
        "    up_decoded = decode_fx(mlp.up_proj)\n",
        "    dn_decoded = decode_fx(mlp.down_proj)\n",
        "\n",
        "    old_gw = mlp.gate_proj.weight.data\n",
        "    old_up = mlp.up_proj.weight.data\n",
        "    old_dn = mlp.down_proj.weight.data\n",
        "\n",
        "    mlp.gate_proj.weight.data = gw_decoded.data\n",
        "    mlp.up_proj.weight.data   = up_decoded.data\n",
        "    mlp.down_proj.weight.data = dn_decoded.data\n",
        "\n",
        "    out = mlp(X)\n",
        "\n",
        "    # restore\n",
        "    mlp.gate_proj.weight.data = old_gw\n",
        "    mlp.up_proj.weight.data   = old_up\n",
        "    mlp.down_proj.weight.data = old_dn\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# 5) Testing => measure decode overhead\n",
        "################################################################################\n",
        "\n",
        "def test_dequantize(dequant_fx):\n",
        "    shapes = [\n",
        "        (2, 3333, 2048, 8192,  3407, torch.float16),\n",
        "        (5,  777, 1024, 4096,  3409, torch.bfloat16),\n",
        "        (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n",
        "    ]\n",
        "    total_time = 0\n",
        "    for (bsz, qlen, hd, m, seed, dt) in shapes:\n",
        "        if dt == torch.bfloat16 and not BF16_AVAILABLE:\n",
        "            dt = torch.float16\n",
        "        torch.manual_seed(seed)\n",
        "        mlp = MLP(hd=hd, m=m, dtype=dt).cuda()\n",
        "        X   = torch.randn((bsz, qlen, hd), device='cuda', dtype=dt)\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        # warmup\n",
        "        for _ in range(2):\n",
        "            out_dec = mlp_forward(X, mlp, dequant_fx)\n",
        "            out_ref = mlp(X)\n",
        "            if not torch.allclose(out_dec.float(), out_ref.float(), atol=1e-2, rtol=1e-2):\n",
        "                print(\"[WARNING] Mismatch in decode vs. ref (within 1e-2 tolerance).\")\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(1000):\n",
        "            mlp_forward(X, mlp, dequant_fx)\n",
        "        torch.cuda.synchronize()\n",
        "        total_time += (time.time() - start)\n",
        "\n",
        "    return total_time\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# 6) Optional Fused Example\n",
        "################################################################################\n",
        "@triton.jit\n",
        "def _fused_nf4_linear_kernel(\n",
        "    A, W_4bit, LUT,\n",
        "    OUT,\n",
        "    B, K, N,\n",
        "    BLOCK_M: tl.constexpr,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "):\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "    row_start = pid_m * BLOCK_M\n",
        "    col_start = pid_n * BLOCK_N\n",
        "\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "    BLOCK_K = 32\n",
        "    for k_chunk in range(0, K, BLOCK_K):\n",
        "        a_offset = (row_start + tl.arange(0, BLOCK_M))[:, None]*K + (k_chunk + tl.arange(0, BLOCK_K))[None, :]\n",
        "        A_tile   = tl.load(A + a_offset, mask=(row_start+tl.arange(0,BLOCK_M))[:,None]<B, other=0.)\n",
        "\n",
        "        w_rows = k_chunk + tl.arange(0, BLOCK_K)\n",
        "        w_cols = col_start + tl.arange(0, BLOCK_N)\n",
        "        w_cols_byte = w_cols // 2\n",
        "        nib_side    = w_cols & 1\n",
        "        offset = w_rows[:, None]*(N//2) + w_cols_byte[None, :]\n",
        "\n",
        "        byte_v = tl.load(W_4bit + offset,\n",
        "                         mask=(w_rows[:,None]<K)&(w_cols_byte[None,:]<(N//2)),\n",
        "                         other=0).to(tl.uint8)\n",
        "        nib_i32 = (byte_v >> (nib_side[None,:]*4)) & 0xF\n",
        "        W_tile  = nib_i32.to(tl.float16)\n",
        "\n",
        "        for kk in range(0, BLOCK_K):\n",
        "            a_vec = A_tile[:, kk]\n",
        "            w_vec = W_tile[kk, :]\n",
        "            acc  += a_vec[:,None].to(tl.float32)*w_vec[None,:].to(tl.float32)\n",
        "\n",
        "    acc_fp16 = acc.to(tl.float16)\n",
        "    out_off  = (row_start+tl.arange(0,BLOCK_M))[:,None]*N + (col_start+tl.arange(0,BLOCK_N))[None,:]\n",
        "    tl.store(OUT + out_off, acc_fp16,\n",
        "             mask=(row_start+tl.arange(0,BLOCK_M))[:,None]<B)\n",
        "\n",
        "\n",
        "def fused_nf4_linear(A, W_4bit, B, K, N):\n",
        "    Out = torch.empty((B, N), device=A.device, dtype=torch.float16)\n",
        "    LUT = torch.zeros((16,), device=A.device, dtype=torch.float16)\n",
        "    BLOCK_M, BLOCK_N = 64, 64\n",
        "    grid = ((B+BLOCK_M-1)//BLOCK_M, (N+BLOCK_N-1)//BLOCK_N)\n",
        "    _fused_nf4_linear_kernel[grid](\n",
        "        A, W_4bit, LUT, Out,\n",
        "        B, K, N,\n",
        "        BLOCK_M=BLOCK_M,\n",
        "        BLOCK_N=BLOCK_N\n",
        "    )\n",
        "    return Out\n",
        "\n",
        "\n",
        "def test_fused_linear():\n",
        "    B, K, N = 512, 1024, 4096\n",
        "    A = torch.randn((B, K), device='cuda', dtype=torch.float16)\n",
        "    W_4bit = torch.randint(0, 256, (K, N//2), device='cuda', dtype=torch.uint8)\n",
        "\n",
        "    def decode_4bit(W_4bit):\n",
        "        K_, halfN = W_4bit.shape\n",
        "        N_ = halfN*2\n",
        "        Out = torch.empty((K_, N_), dtype=torch.float16, device='cuda')\n",
        "        idx = torch.arange(0, K_*N_, device='cuda')\n",
        "        byte_idx = idx // 2\n",
        "        nib_side = idx & 1\n",
        "        data  = W_4bit.view(-1).gather(0, byte_idx)\n",
        "        nibs  = (data >> (nib_side*4)) & 0xF\n",
        "        Out.view(-1)[:] = nibs\n",
        "        return Out\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    for _ in range(100):\n",
        "        W_dec = decode_4bit(W_4bit)\n",
        "        _ = A @ W_dec\n",
        "    torch.cuda.synchronize()\n",
        "    baseline_time = time.time() - start\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    for _ in range(100):\n",
        "        _ = fused_nf4_linear(A, W_4bit, B, K, N)\n",
        "    torch.cuda.synchronize()\n",
        "    fused_time = time.time() - start\n",
        "\n",
        "    print(f\"[FUSED-LINEAR] baseline_time={baseline_time:.3f}s, fused_time={fused_time:.3f}s, \"\n",
        "          f\"speedup={baseline_time/fused_time:.2f}x\")\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# 7) Main => Compare Times\n",
        "################################################################################\n",
        "if __name__ == \"__main__\":\n",
        "    # Compare baseline => .to(fp16)\n",
        "    print(\"---- Compare Unsloth vs. Triton decode in MLP forward ----\")\n",
        "    print(\"[INFO] Unsloth => .to(fp16)\")\n",
        "    time_unsloth = test_dequantize(lambda layer: layer.weight.data.to(torch.float16))\n",
        "    print(\"[INFO] Unsloth total time =>\", time_unsloth)\n",
        "\n",
        "    # Compare Triton NF4 => your_dequantize_nf4\n",
        "    print(\"[INFO] Triton NF4 => your_dequantize_nf4\")\n",
        "    time_ours = test_dequantize(your_dequantize_nf4)\n",
        "    print(\"[INFO] Triton total time =>\", time_ours)\n",
        "\n",
        "    speedup = time_unsloth / time_ours if time_ours>0 else 9999\n",
        "    print(f\"[INFO] Speedup => {speedup:.2f}x (goal≥1.15x)\\n\")\n",
        "\n",
        "    # Optional fused decode+matmul test\n",
        "    print(\"---- OPTIONAL: Compare single fused decode+matmul vs baseline decode+matmul ----\")\n",
        "    test_fused_linear()\n",
        "    print(\"[INFO] Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC-KsYoy7BTP",
        "outputId": "50f707a2-4513-4495-c0e4-76a898f91a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] This GPU does not support SM80+. We fallback from BF16 to FP16 where needed.\n",
            "---- Compare Unsloth vs. Triton decode in MLP forward ----\n",
            "[INFO] Unsloth => .to(fp16)\n",
            "[WARNING] Mismatch in decode vs. ref (within 1e-2 tolerance).\n",
            "[WARNING] Mismatch in decode vs. ref (within 1e-2 tolerance).\n",
            "[WARNING] Mismatch in decode vs. ref (within 1e-2 tolerance).\n",
            "[WARNING] Mismatch in decode vs. ref (within 1e-2 tolerance).\n",
            "[WARNING] Mismatch in decode vs. ref (within 1e-2 tolerance).\n",
            "[WARNING] Mismatch in decode vs. ref (within 1e-2 tolerance).\n",
            "[INFO] Unsloth total time => 149.93729329109192\n",
            "[INFO] Triton NF4 => your_dequantize_nf4\n",
            "[INFO] Triton total time => 146.93836498260498\n",
            "[INFO] Speedup => 1.02x (goal≥1.15x)\n",
            "\n",
            "---- OPTIONAL: Compare single fused decode+matmul vs baseline decode+matmul ----\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CompilationError",
          "evalue": "at 16:79:\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    row_start = pid_m * BLOCK_M\n    col_start = pid_n * BLOCK_N\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    BLOCK_K = 32\n    for k_chunk in range(0, K, BLOCK_K):\n        a_offset = (row_start + tl.arange(0, BLOCK_M))[:, None]*K + (k_chunk + tl.arange(0, BLOCK_K))[None, :]\n                                                                               ^",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/language/core.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m                              \"(`_builder` argument must be provided outside of JIT functions.)\")\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/language/core.py\u001b[0m in \u001b[0;36marange\u001b[0;34m(start, end, _builder)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_constexpr_to_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msemantic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_builder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/language/semantic.py\u001b[0m in \u001b[0;36marange\u001b[0;34m(start, end, builder)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arange's arguments must be of type tl.constexpr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m     \u001b[0mis_start_int64\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: arange's arguments must be of type tl.constexpr",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mCompilationError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-46674db8524c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;31m# Optional fused decode+matmul test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---- OPTIONAL: Compare single fused decode+matmul vs baseline decode+matmul ----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0mtest_fused_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] Done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-46674db8524c>\u001b[0m in \u001b[0;36mtest_fused_linear\u001b[0;34m()\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfused_nf4_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_4bit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mfused_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-46674db8524c>\u001b[0m in \u001b[0;36mfused_nf4_linear\u001b[0;34m(A, W_4bit, B, K, N)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mBLOCK_M\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBLOCK_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBLOCK_M\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mBLOCK_M\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBLOCK_N\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mBLOCK_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     _fused_nf4_linear_kernel[grid](\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_4bit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOut\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/runtime/jit.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mmemorizes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \"\"\"\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;31m# return cast(T, functools.partial(cast(Callable, self.run), grid=grid))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/runtime/jit.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0;31m# compile the kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mASTSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             kernel = self.compile(\n\u001b[0m\u001b[1;32m    624\u001b[0m                 \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/compiler/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(src, target, options)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0mmodule_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_module_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodegen_fns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mfilter_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/compiler/compiler.py\u001b[0m in \u001b[0;36mmake_ir\u001b[0;34m(self, options, codegen_fns, module_map, context)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodegen_fns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\n\u001b[0m\u001b[1;32m    101\u001b[0m                            module_map=module_map)\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCompilationError\u001b[0m: at 16:79:\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    row_start = pid_m * BLOCK_M\n    col_start = pid_n * BLOCK_N\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    BLOCK_K = 32\n    for k_chunk in range(0, K, BLOCK_K):\n        a_offset = (row_start + tl.arange(0, BLOCK_M))[:, None]*K + (k_chunk + tl.arange(0, BLOCK_K))[None, :]\n                                                                               ^"
          ]
        }
      ]
    }
  ]
}