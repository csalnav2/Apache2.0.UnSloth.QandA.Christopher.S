{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOStDJkj1sJLTU8mC/PSdgD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csalnav2/QdotCS/blob/master/nf4toTriton1.0Xrev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 1) Full environment reset\n",
        "# =============================\n",
        "!pip uninstall -y torch torchvision torchaudio triton xformers bitsandbytes unsloth unsloth_zoo fastai cut_cross_entropy\n",
        "\n",
        "# =============================\n",
        "# 2) Install nightly PyTorch (>=2.6.0 dev) + matching Triton\n",
        "# =============================\n",
        "!pip install --no-cache-dir --pre --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n",
        "!pip install --no-cache-dir triton>=3.1.0\n",
        "\n",
        "# =============================\n",
        "# 3) Install extras\n",
        "# =============================\n",
        "!pip install --no-deps xformers==0.0.29 bitsandbytes accelerate peft trl\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth_zoo\n",
        "!pip install --no-deps unsloth\n",
        "!pip install tyro\n",
        "\n",
        "import torch\n",
        "print(\"Torch:\", torch.__version__)\n",
        "\n",
        "import triton\n",
        "print(\"Triton:\", triton.__version__)\n",
        "\n",
        "# If Colab complains \"You must restart runtime,\" do it and re-run.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9KY6Yww-4VR",
        "outputId": "b3233b59-1cff-4375-9401-59e5a4dca89b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.5.1+cu124\n",
            "Uninstalling torch-2.5.1+cu124:\n",
            "  Successfully uninstalled torch-2.5.1+cu124\n",
            "Found existing installation: torchvision 0.20.1+cu124\n",
            "Uninstalling torchvision-0.20.1+cu124:\n",
            "  Successfully uninstalled torchvision-0.20.1+cu124\n",
            "Found existing installation: torchaudio 2.5.1+cu124\n",
            "Uninstalling torchaudio-2.5.1+cu124:\n",
            "  Successfully uninstalled torchaudio-2.5.1+cu124\n",
            "Found existing installation: triton 3.1.0\n",
            "Uninstalling triton-3.1.0:\n",
            "  Successfully uninstalled triton-3.1.0\n",
            "\u001b[33mWARNING: Skipping xformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping unsloth as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping unsloth_zoo as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: fastai 2.7.18\n",
            "Uninstalling fastai-2.7.18:\n",
            "  Successfully uninstalled fastai-2.7.18\n",
            "\u001b[33mWARNING: Skipping cut_cross_entropy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/nightly/cu121\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/torch-2.6.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (768.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.0/768.0 MB\u001b[0m \u001b[31m327.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/torchvision-0.20.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m354.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/torchaudio-2.5.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m355.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m379.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m387.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m364.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m296.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m271.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m308.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m300.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m292.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m361.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m283.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m285.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-triton==3.1.0+cf34004b8a (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.1.0%2Bcf34004b8a-cp311-cp311-linux_x86_64.whl (239.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.7/239.7 MB\u001b[0m \u001b[31m318.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, pytorch-triton, nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-cusparselt-cu12-0.6.2 nvidia-nvtx-cu12-12.1.105 pytorch-triton-3.1.0+cf34004b8a torch-2.6.0.dev20241112+cu121 torchaudio-2.5.0.dev20241112+cu121 torchvision-0.20.0.dev20241112+cu121\n",
            "Collecting xformers==0.0.29\n",
            "  Downloading xformers-0.0.29-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Collecting trl\n",
            "  Downloading trl-0.15.1-py3-none-any.whl.metadata (11 kB)\n",
            "Downloading xformers-0.0.29-cp311-cp311-manylinux_2_28_x86_64.whl (15.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.15.1-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers, trl, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.2 trl-0.15.1 xformers-0.0.29\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (4.25.6)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Collecting hf_transfer\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, hf_transfer, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 hf_transfer-0.1.9 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting unsloth_zoo\n",
            "  Downloading unsloth_zoo-2025.2.7-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading unsloth_zoo-2025.2.7-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.1/107.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unsloth_zoo\n",
            "Successfully installed unsloth_zoo-2025.2.7\n",
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.2.15-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth-2025.2.15-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.8/188.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unsloth\n",
            "Successfully installed unsloth-2025.2.15\n",
            "Collecting tyro\n",
            "  Downloading tyro-0.9.16-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro) (4.4.2)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from tyro) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro) (0.1.2)\n",
            "Downloading tyro-0.9.16-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: shtab, tyro\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.2.7 requires cut_cross_entropy, which is not installed.\n",
            "unsloth 2025.2.15 requires protobuf<4.0.0, but you have protobuf 4.25.6 which is incompatible.\n",
            "unsloth-zoo 2025.2.7 requires protobuf<4.0.0, but you have protobuf 4.25.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed shtab-1.7.1 tyro-0.9.16\n",
            "Torch: 2.6.0.dev20241112+cu121\n",
            "Triton: 3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import inspect\n",
        "from transformers import set_seed\n",
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "\n",
        "# For unsloth-based NF4 decode\n",
        "from unsloth.kernels.utils import fast_dequantize\n",
        "\n",
        "# For PEFT-based NF4 decode\n",
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on device:\", device)\n",
        "\n",
        "def assert_same(x, y, dt):\n",
        "    if x.dtype != dt:\n",
        "        raise RuntimeError(f\"dtype mismatch: got {x.dtype}, expected {dt}\")\n",
        "    torch.testing.assert_close(x, y, check_stride=True)\n",
        "\n",
        "def bnb_Linear4bit(hd, m, dt=torch.float16):\n",
        "    return Linear4bit(hd, m, bias=None, compute_dtype=dt, compress_statistics=True, quant_type=\"nf4\")\n",
        "\n",
        "def assert_bnb_state(w, dt):\n",
        "    assert w.weight.dtype == torch.uint8\n",
        "    s = w.weight.quant_state\n",
        "    assert s.dtype == dt\n",
        "    assert s.absmax.dtype == torch.uint8\n",
        "    assert s.code.dtype == torch.float32\n",
        "    assert s.offset.dtype == torch.float32\n",
        "    assert s.blocksize == 64\n",
        "    assert s.state2.absmax.dtype == torch.float32\n",
        "    assert s.state2.code.dtype == torch.float32\n",
        "    assert s.state2.blocksize == 256\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd=4096, m=14336, dt=torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dt).to(device)\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dt).to(device)\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dt).to(device)\n",
        "        # Force correct quant_state dtype\n",
        "        self.gate_proj.weight.quant_state.dtype = dt\n",
        "        self.up_proj.weight.quant_state.dtype   = dt\n",
        "        self.down_proj.weight.quant_state.dtype = dt\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    up   = X @ fx(mlp.up_proj).t()\n",
        "    gate = X @ fx(mlp.gate_proj).t()\n",
        "    return mlp.act_fn(gate) * up @ fx(mlp.down_proj).t()\n",
        "\n",
        "def mlp_dequantize(X, mlp, fx):\n",
        "    a = fx(mlp.up_proj).t();   torch.cuda.synchronize()\n",
        "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
        "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
        "    return a,b,c\n",
        "\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n",
        "def test_dequantize(dequant_fx):\n",
        "    e = 0\n",
        "    # Some test configs\n",
        "    configs = [\n",
        "      (2, 3333, 2048, 8192, 3407, torch.float16),\n",
        "      (5,  777, 1024, 4096, 3409, torch.bfloat16),\n",
        "      (3, 2048, 4096, 14336,3408, torch.bfloat16),\n",
        "    ]\n",
        "    for (bsz, ql, hd, m, seed, dt) in configs:\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "        mlp = MLP(hd, m, dt)\n",
        "        X   = torch.randn((bsz, ql, hd), device=device, dtype=dt)\n",
        "        torch.cuda.synchronize()\n",
        "        # Warmup checks\n",
        "        for _ in range(2):\n",
        "            out_manual = mlp_forward(X, mlp, dequant_fx)\n",
        "            out_model  = mlp(X)\n",
        "            assert_same(out_manual, out_model, dt)\n",
        "            assert_bnb_state(mlp.up_proj, dt)\n",
        "            assert_bnb_state(mlp.gate_proj, dt)\n",
        "            assert_bnb_state(mlp.down_proj, dt)\n",
        "            a,b,c = mlp_dequantize(X, mlp, dequant_fx)\n",
        "            A,B,C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
        "            assert_same(a,A, dt)\n",
        "            assert_same(b,B, dt)\n",
        "            assert_same(c,C, dt)\n",
        "        # Benchmark\n",
        "        torch.cuda.synchronize()\n",
        "        t0 = time.time()\n",
        "        for _ in range(1000):\n",
        "            mlp_dequantize(X, mlp, dequant_fx)\n",
        "        e += (time.time() - t0)\n",
        "    return e\n",
        "\n",
        "# Test unsloth + peft\n",
        "if device == \"cuda\":\n",
        "    e_unsloth = test_dequantize(unsloth_dequantize)\n",
        "    print(f\"[INFO] unsloth_dequantize total time: {e_unsloth:.4f}s\")\n",
        "    e_peft = test_dequantize(peft_dequantize)\n",
        "    print(f\"[INFO] peft_dequantize total time: {e_peft:.4f}s\")\n",
        "else:\n",
        "    print(\"[INFO] CPU environment, skipping NF4 bitsandbytes tests.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_99Stplc_5i0",
        "outputId": "8f77292d-ce76-4ba4-a5a9-2c99b096b6f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.6.0.dev20241112+cu121)\n",
            "    Python  3.11.11 (you have 3.11.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "Running on device: cuda\n",
            "[INFO] unsloth_dequantize total time: 5.2649s\n",
            "[INFO] peft_dequantize total time: 5.4897s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch triton bitsandbytes unsloth\n",
        "!pip install --pre --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n",
        "!pip install --no-cache-dir triton>=3.1.0\n",
        "!pip install bitsandbytes\n",
        "!pip install --no-deps unsloth\n",
        "!pip install --no-deps peft\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjrhqfFq_OO7",
        "outputId": "19988aa1-5e39-4342-d3e4-4379ab395f27"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0.dev20241112+cu121\n",
            "Uninstalling torch-2.6.0.dev20241112+cu121:\n",
            "  Successfully uninstalled torch-2.6.0.dev20241112+cu121\n",
            "\u001b[33mWARNING: Skipping triton as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping unsloth as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/nightly/cu121\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu121/torch-2.6.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (768.0 MB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.0.dev20241112+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.0.dev20241112+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: pytorch-triton==3.1.0+cf34004b8a in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0+cf34004b8a)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.2.7 requires cut_cross_entropy, which is not installed.\n",
            "unsloth-zoo 2025.2.7 requires triton; platform_system == \"Linux\", which is not installed.\n",
            "xformers 0.0.29 requires torch==2.5.1, but you have torch 2.6.0.dev20241112+cu121 which is incompatible.\n",
            "unsloth-zoo 2025.2.7 requires protobuf<4.0.0, but you have protobuf 4.25.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.6.0.dev20241112+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.2.7 requires cut_cross_entropy, which is not installed.\n",
            "unsloth-zoo 2025.2.7 requires protobuf<4.0.0, but you have protobuf 4.25.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting bitsandbytes\n",
            "  Using cached bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0.dev20241112+cu121)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: pytorch-triton==3.1.0+cf34004b8a in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0+cf34004b8a)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.0->bitsandbytes) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Using cached bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.2\n",
            "Collecting unsloth\n",
            "  Using cached unsloth-2025.2.15-py3-none-any.whl.metadata (57 kB)\n",
            "Using cached unsloth-2025.2.15-py3-none-any.whl (188 kB)\n",
            "Installing collected packages: unsloth\n",
            "Successfully installed unsloth-2025.2.15\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================ Cell start ================================\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from triton import jit\n",
        "\n",
        "# 1) Minimal kernel with *no* parameters or shape arguments.\n",
        "#    We literally do arange(0, 512) => a compile-time constant: 512\n",
        "@jit\n",
        "def kernel_512(X_ptr, Y_ptr):\n",
        "    # Hard-coded literal => 512 => must be recognized as a compile-time constant\n",
        "    idx = tl.arange(0, 512)\n",
        "    x_val = tl.load(X_ptr + idx)\n",
        "    out   = x_val + 1.0\n",
        "    tl.store(Y_ptr + idx, out)\n",
        "\n",
        "# 2) Attempt to run\n",
        "try:\n",
        "    x = torch.randn(512, device=\"cuda\")\n",
        "    y = torch.empty_like(x)\n",
        "    # Launch with grid=(1,) => single block\n",
        "    kernel_512[(1,)](x, y)\n",
        "    print(\"[INFO] Triton kernel_512 succeeded!\")\n",
        "    print(\"[INFO] y[:10] =\", y[:10])\n",
        "except Exception as e:\n",
        "    print(\"[ERROR] kernel_512 failed =>\", e)\n",
        "# ================================ Cell end =================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JJVG9zqAIuX",
        "outputId": "e6985b55-9814-48aa-9a03-67df8a3ef35e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Triton kernel_512 succeeded!\n",
            "[INFO] y[:10] = tensor([ 1.1689,  0.7234,  1.3856,  0.2225,  1.0638,  0.6520,  0.5889,  2.3875,\n",
            "        -0.0040,  0.6079], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrIIc5h8HhFS",
        "outputId": "bdd2f3d6-4b06-4572-dc53-ab6ea2f78338"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0.dev20241112+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: pytorch-triton==3.1.0+cf34004b8a in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0+cf34004b8a)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Ensure we have a GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"[INFO] device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6K0_iWUHnc-",
        "outputId": "d3b7adc2-4855-431f-fb10-37c298b25628"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "\n",
        "################################################################################\n",
        "# 1) MLP using bitsandbytes 4-bit layers\n",
        "################################################################################\n",
        "def bnb_Linear4bit(in_features, out_features, dtype=torch.float16):\n",
        "    \"\"\"\n",
        "    We'll forcibly interpret arguments as (in_features, out_features)\n",
        "    though bitsandbytes might flatten them.\n",
        "    \"\"\"\n",
        "    return Linear4bit(\n",
        "        in_features, out_features,\n",
        "        bias=None,\n",
        "        compute_dtype=dtype,\n",
        "        compress_statistics=True,\n",
        "        quant_type=\"nf4\",\n",
        "    )\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd=2048, m=8192, dtype=torch.float16):\n",
        "        super().__init__()\n",
        "        # Gate & Up => (hd->m), down => (m->hd)\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dtype=dtype).cuda()\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dtype=dtype).cuda()\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dtype=dtype).cuda()\n",
        "\n",
        "        for layer in [self.gate_proj, self.up_proj, self.down_proj]:\n",
        "            layer.weight.quant_state.dtype = dtype\n",
        "\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "\n",
        "        print(\"\\n=== MLP LAYER SHAPES (bitsandbytes param) ===\")\n",
        "        print(\"gate_proj =>\", self.gate_proj.weight.shape)\n",
        "        print(\"up_proj   =>\", self.up_proj.weight.shape)\n",
        "        print(\"down_proj =>\", self.down_proj.weight.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(\n",
        "            self.act_fn(self.gate_proj(x)) * self.up_proj(x)\n",
        "        )\n",
        "\n",
        "################################################################################\n",
        "# 2) A decode function that re-shapes (8388608,1)->(8192,1024)->(8192,2048)\n",
        "################################################################################\n",
        "def nf4_decode(layer):\n",
        "    \"\"\"\n",
        "    If layer.weight.shape => (8388608,1), we interpret out_features=8192,\n",
        "    half_in=1024 => final => (8192,2048) after decoding to float16.\n",
        "\n",
        "    If shape is different, handle accordingly.\n",
        "    \"\"\"\n",
        "    w_packed = layer.weight.data  # nibble-packed\n",
        "    shape_packed = w_packed.shape  # e.g. (8388608,1)\n",
        "\n",
        "    if shape_packed[0] == 8388608 and shape_packed[1] == 1:\n",
        "        # we interpret out_features=8192, half_in=1024 => so final => (8192, 2048)\n",
        "        out_features, half_in = 8192, 1024\n",
        "        # create a random float16 matrix => (8192, 2048) as a placeholder decode\n",
        "        decoded = torch.randn(out_features, half_in*2, device=w_packed.device, dtype=torch.float16)\n",
        "        return decoded\n",
        "    else:\n",
        "        # If bitsandbytes stored it differently, adapt logic\n",
        "        # For example, if shape=(m, hd//2), we do final => (m, hd)\n",
        "        m, half_hd = shape_packed\n",
        "        # Maybe it's already (8192, 1024)? Then we do => (8192, 2048)\n",
        "        decoded = torch.randn(m, half_hd*2, device=w_packed.device, dtype=torch.float16)\n",
        "        return decoded\n",
        "\n",
        "################################################################################\n",
        "# 3) A small forward that decodes \"gate_proj\" => matmul => see if it works\n",
        "################################################################################\n",
        "def debug_forward(X, mlp):\n",
        "    \"\"\"\n",
        "    We'll decode the gate_proj weight => shape (m, hd),\n",
        "    then matmul with X => shape(bsz, qlen, hd).\n",
        "    \"\"\"\n",
        "    w_dec = nf4_decode(mlp.gate_proj)  # (m, hd) => e.g. (8192, 2048)\n",
        "    print(\"[debug_forward] w_dec.shape =>\", w_dec.shape)\n",
        "\n",
        "    bsz, qlen, hd = X.shape\n",
        "    # (bsz*qlen, hd) x (hd, m)\n",
        "    X_2d= X.reshape(bsz*qlen, hd)\n",
        "    out_2d= X_2d @ w_dec.t()  # => (bsz*qlen, m)\n",
        "    return out_2d.reshape(bsz, qlen, -1)\n",
        "\n",
        "################################################################################\n",
        "# 4) Running the test\n",
        "################################################################################\n",
        "def run_test():\n",
        "    # typical => hd=2048 => m=8192 => in_features=2048 => out_features=8192\n",
        "    mlp = MLP(2048, 8192, dtype=torch.float16)\n",
        "    # X => (2, 3333, 2048)\n",
        "    X= torch.randn((2, 3333, 2048), device=\"cuda\", dtype=torch.float16)\n",
        "    print(\"X.shape =>\", X.shape)\n",
        "\n",
        "    out= debug_forward(X, mlp)\n",
        "    print(\"[INFO] final out.shape =>\", out.shape)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    run_test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlUv0LP3fbyT",
        "outputId": "2ec5c505-8d40-43d6-ad3c-7aac725349af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== MLP LAYER SHAPES (bitsandbytes param) ===\n",
            "gate_proj => torch.Size([8388608, 1])\n",
            "up_proj   => torch.Size([8388608, 1])\n",
            "down_proj => torch.Size([8388608, 1])\n",
            "X.shape => torch.Size([2, 3333, 2048])\n",
            "[debug_forward] w_dec.shape => torch.Size([8192, 2048])\n",
            "[INFO] final out.shape => torch.Size([2, 3333, 8192])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 0) Install / Imports (Colab-friendly, no %%capture) ====================\n",
        "\n",
        "try:\n",
        "    import triton\n",
        "except ImportError:\n",
        "    !pip install -U triton\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from triton import jit\n",
        "\n",
        "# If these are not available, comment them out or install them\n",
        "try:\n",
        "    from bitsandbytes.nn import Linear4bit\n",
        "    from transformers.activations import ACT2FN\n",
        "except ImportError:\n",
        "    print(\"[WARNING] bitsandbytes or transformers not installed; the MLP test might not work fully.\")\n",
        "    # you can do: !pip install bitsandbytes transformers\n",
        "\n",
        "# ==================== Step 1: Triton Kernel ====================\n",
        "\n",
        "@jit\n",
        "def _nf4_dequantize_kernel(\n",
        "    QWEIGHT, ABSMAX, CODE, OFFSET, S2_ABSMAX, S2_CODE, OUT,\n",
        "    BLOCK_SIZE: tl.constexpr, LENGTH: tl.constexpr,\n",
        "    USE_128: tl.int32, SIGNED_NIB: tl.int32\n",
        "):\n",
        "    \"\"\"\n",
        "    Triton kernel for NF4 dequantization in one pass.\n",
        "    \"\"\"\n",
        "    pid = tl.program_id(0)\n",
        "    idx = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = idx < LENGTH\n",
        "\n",
        "    byte_idx = idx // 2\n",
        "    nib_side = idx & 1\n",
        "\n",
        "    byte_v = tl.load(QWEIGHT + byte_idx, mask=mask, other=0)\n",
        "    nib = (byte_v >> (nib_side * 4)) & 0xF\n",
        "\n",
        "    # Signed nibble logic\n",
        "    if SIGNED_NIB:\n",
        "        is_neg = nib >= 8\n",
        "        nib_signed = nib - 16\n",
        "        nib_signed = tl.where(is_neg, nib_signed, nib)\n",
        "        code_idx = nib_signed + 8\n",
        "        code_idx = tl.where(code_idx < 0, 0, code_idx)\n",
        "        code_idx = tl.where(code_idx > 15, 15, code_idx)\n",
        "    else:\n",
        "        code_idx = nib\n",
        "        code_idx = tl.where(code_idx > 15, 15, code_idx)\n",
        "\n",
        "    # Load values from lookup tables\n",
        "    code_val = tl.load(CODE + code_idx)\n",
        "    off_val = tl.load(OFFSET + nib)\n",
        "\n",
        "    val_f32 = code_val + off_val\n",
        "\n",
        "    block_id = idx // 64\n",
        "    am_u8 = tl.load(ABSMAX + block_id, mask=mask, other=0)\n",
        "    s2_am_u8 = tl.load(S2_ABSMAX + block_id, mask=mask, other=127)\n",
        "\n",
        "    scale = tl.where(USE_128, am_u8 / 128.0, am_u8 / 127.0)\n",
        "    scale = tl.where(scale == 0, 1.0, scale)\n",
        "\n",
        "    s2_am = tl.where(USE_128, s2_am_u8 / 128.0, s2_am_u8 / 127.0)\n",
        "\n",
        "    s2_cd = tl.load(S2_CODE + nib, mask=mask, other=0)\n",
        "    val_f32 = val_f32 * s2_cd * scale * s2_am\n",
        "\n",
        "    out_val = val_f32.to(tl.float16)\n",
        "    tl.store(OUT + idx, out_val, mask=mask)\n",
        "\n",
        "\n",
        "def your_dequantize_nf4(layer):\n",
        "    \"\"\"\n",
        "    Calls the Triton kernel to dequantize NF4 weights.\n",
        "    \"\"\"\n",
        "    qweight = layer.weight\n",
        "    qs = layer.weight.quant_state\n",
        "\n",
        "    length_nibbles = qweight.numel() * 2\n",
        "    device = qweight.device\n",
        "    BLOCK_SIZE = 65536  # big block => single-block approach\n",
        "\n",
        "    out = torch.empty(length_nibbles, dtype=torch.float16, device=device)\n",
        "\n",
        "    _nf4_dequantize_kernel[(1,)](\n",
        "        qweight,\n",
        "        qs.absmax.to(device),\n",
        "        qs.code.to(device),\n",
        "        qs.offset.to(device),\n",
        "        qs.state2.absmax.to(device),\n",
        "        qs.state2.code.to(device),\n",
        "        out,\n",
        "        BLOCK_SIZE=BLOCK_SIZE,\n",
        "        LENGTH=length_nibbles,\n",
        "        USE_128=0,\n",
        "        SIGNED_NIB=0\n",
        "    )\n",
        "\n",
        "    return out.reshape(qweight.shape[0], qweight.shape[1] * 2)\n",
        "\n",
        "\n",
        "# ==================== Step 2: MLP Class ====================\n",
        "\n",
        "try:\n",
        "    from bitsandbytes.nn import Linear4bit\n",
        "    from transformers.activations import ACT2FN\n",
        "except ImportError:\n",
        "    # If bitsandbytes or transformers is missing, define stubs or skip\n",
        "    class Linear4bit(nn.Linear):\n",
        "        pass\n",
        "    def ACT2FN(x): return torch.nn.functional.relu\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd=4096, m=14336, dtype=torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = Linear4bit(hd, m, bias=None, compute_dtype=dtype, compress_statistics=True, quant_type=\"nf4\").cuda()\n",
        "        self.up_proj = Linear4bit(hd, m, bias=None, compute_dtype=dtype, compress_statistics=True, quant_type=\"nf4\").cuda()\n",
        "        self.down_proj = Linear4bit(m, hd, bias=None, compute_dtype=dtype, compress_statistics=True, quant_type=\"nf4\").cuda()\n",
        "\n",
        "        self.gate_proj.weight.quant_state.dtype = dtype\n",
        "        self.up_proj.weight.quant_state.dtype = dtype\n",
        "        self.down_proj.weight.quant_state.dtype = dtype\n",
        "\n",
        "        # default => silu if ACT2FN available\n",
        "        if isinstance(ACT2FN, dict):\n",
        "            self.act_fn = torch.nn.functional.silu\n",
        "        else:\n",
        "            # fallback\n",
        "            self.act_fn = lambda x: x * torch.sigmoid(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    return mlp.down_proj(mlp.act_fn(mlp.gate_proj(X)) * mlp.up_proj(X))\n",
        "\n",
        "\n",
        "# ==================== Step 3: Testing Function ====================\n",
        "\n",
        "def test_dequantize(dequantize_fx):\n",
        "    \"\"\"\n",
        "    We'll replicate your original approach:\n",
        "      => 3 test shapes\n",
        "      => build MLP\n",
        "      => decode -> warmup -> decode repeated -> measure time\n",
        "    \"\"\"\n",
        "    shapes = [\n",
        "        (2, 3333, 2048, 8192, 3407, torch.float16),\n",
        "        (5, 777, 1024, 4096, 3409, torch.bfloat16),\n",
        "        (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n",
        "    ]\n",
        "\n",
        "    total_time = 0\n",
        "    for (bsz, qlen, hd, m, seed, dt) in shapes:\n",
        "        torch.manual_seed(seed)\n",
        "        mlp = MLP(hd=hd, m=m, dtype=dt)\n",
        "        X = torch.randn((bsz, qlen, hd), device=\"cuda\", dtype=dt)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Warmup\n",
        "        for _ in range(2):\n",
        "            out_decoded = mlp_forward(X, mlp, dequantize_fx)\n",
        "            out_ref = mlp(X)\n",
        "            # we can do an assert or not\n",
        "            assert torch.allclose(out_decoded, out_ref, atol=1e-3), \"Mismatch in decode vs. ref\"\n",
        "\n",
        "        # Benchmarking => 1000 calls\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(1000):\n",
        "            mlp_forward(X, mlp, dequantize_fx)\n",
        "        torch.cuda.synchronize()\n",
        "        elapsed = time.time() - start\n",
        "        total_time += elapsed\n",
        "\n",
        "    return total_time\n",
        "\n",
        "\n",
        "# ==================== Step 4: Running the Tests ====================\n",
        "\n",
        "print(\"[INFO] Running Unsloth Baseline decode => .to(fp16)...\")\n",
        "time_unsloth = test_dequantize(lambda layer: layer.weight.data.to(torch.float16))\n",
        "print(\"[INFO] Unsloth total time:\", time_unsloth)\n",
        "\n",
        "print(\"[INFO] Running Triton NF4 Kernel => your_dequantize_nf4 ...\")\n",
        "time_ours = test_dequantize(your_dequantize_nf4)\n",
        "print(\"[INFO] Triton total time:\", time_ours)\n",
        "\n",
        "speedup = time_unsloth / time_ours if time_ours>0 else 9999\n",
        "print(f\"[INFO] Speedup => {speedup:.2f}x (goal≥1.15x)\")\n",
        "\n",
        "print(\"[INFO] Done! You should see the final speedup printed above.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC-KsYoy7BTP",
        "outputId": "166bd12a-4231-4065-8c91-7d8c7bd83827"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Running Unsloth Baseline decode => .to(fp16)...\n",
            "[INFO] Unsloth total time: 1007.4150297641754\n",
            "[INFO] Running Triton NF4 Kernel => your_dequantize_nf4 ...\n",
            "[INFO] Triton total time: 1004.682094335556\n",
            "[INFO] Speedup => 1.00x (goal≥1.15x)\n",
            "[INFO] Done! You should see the final speedup printed above.\n"
          ]
        }
      ]
    }
  ]
}